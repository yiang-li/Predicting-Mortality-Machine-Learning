{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href= \"https://hrs.isr.umich.edu/sites/default/files/meta/2002/core/codebook/h02_00.html?_ga=2.14020593.714056361.1676427746-1610833755.1676427746\">codebook</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Background:\n",
    "\n",
    "The current academic literature in predicting mortality has extensively focused on disease and frailty, although social, behavioral, and psychological statuses may herald broad physiological decline. I tested the effectiveness of the machine learning algorithms on the NSHAP sample in Project 1 and learnt the important features predicting mortality. This project will extend the analysis by 1) using a different set of predictors 2) applying some new algorithms in addition to tree-based algorithm 3) using a different dataset.\n",
    "\n",
    "#### Context:\n",
    "\n",
    "Household Retire Survey (HRS) is a nationally representative survey of the older US adults (aged 50-61) collected every two years from 1992 to 2018. I use 2002 characteristics as baseline and 2016 disposition status (reported in the 2018 wave) as target. \n",
    "\n",
    "#### Goal\n",
    "The goal of this project is to predict mortality in the next 14 years using the baseline characteristics in 2002. I will also discussed the social and demographic characteristics of the cases whose disposition statuses were either wrongly predicted as death or alive by the algorithms. The findings will serve important purposes for public health practitioners in understanding the risk and protective factors of mortality in the aging process.\n",
    "\n",
    "#### The task: \n",
    "The task is to train a binary classification model to predict mortality in the next 14 years using the baseline characteristics in 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>PN</th>\n",
       "      <th>deceased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10475</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10592</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11219</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11575</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11626</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10027</th>\n",
       "      <td>213379</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10028</th>\n",
       "      <td>213410</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10029</th>\n",
       "      <td>213418</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10030</th>\n",
       "      <td>213428</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10031</th>\n",
       "      <td>213434</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10032 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HHID  PN  deceased\n",
       "0       10475  10         1\n",
       "1       10592  10         1\n",
       "2       11219  10         1\n",
       "3       11575  20         1\n",
       "4       11626  10         1\n",
       "...       ...  ..       ...\n",
       "10027  213379  10         1\n",
       "10028  213410  10         1\n",
       "10029  213418  10         1\n",
       "10030  213428  10         1\n",
       "10031  213434  10         1\n",
       "\n",
       "[10032 rows x 3 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing all disposition files of every wave\n",
    "all_dispositon_files = glob.glob('hrs_data/disposition/*.csv')\n",
    "# Create an empty dataframe to store all the dataframes\n",
    "dispositon_data_full = pd.DataFrame()\n",
    "# Loop through all the files and read them into a list of dataframes\n",
    "for filename in all_dispositon_files:\n",
    "    df_disposition = pd.read_csv(filename, index_col=None, header=0)\n",
    "    dispositon_data_full = pd.concat([dispositon_data_full, df_disposition], axis=0, ignore_index=True)\n",
    "# Concatenate all the dataframes into one dataframe\n",
    "dispositon_data = dispositon_data_full.iloc[:, :2].copy()\n",
    "# As these are all the disposition files, we can set deceased to 1\n",
    "dispositon_data['deceased'] = 1\n",
    "dispositon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>PN</th>\n",
       "      <th>HC139</th>\n",
       "      <th>HC005</th>\n",
       "      <th>HC010</th>\n",
       "      <th>HC001</th>\n",
       "      <th>HC070</th>\n",
       "      <th>HC117</th>\n",
       "      <th>HC128</th>\n",
       "      <th>HX067_R</th>\n",
       "      <th>...</th>\n",
       "      <th>HMARITAL</th>\n",
       "      <th>HX060_R</th>\n",
       "      <th>HB031A</th>\n",
       "      <th>HB053</th>\n",
       "      <th>HE012</th>\n",
       "      <th>HE046</th>\n",
       "      <th>HF174</th>\n",
       "      <th>HG092</th>\n",
       "      <th>HF176</th>\n",
       "      <th>HG001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>170.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>179.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1938.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>10</td>\n",
       "      <td>180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9998.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>30</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>10</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    HHID  PN  HC139  HC005  HC010  HC001  HC070  HC117  HC128  HX067_R  ...  \\\n",
       "0      3  10  170.0    5.0    5.0    3.0    5.0    5.0    1.0   1936.0  ...   \n",
       "1      3  20  179.0    5.0    5.0    3.0    1.0    5.0    5.0   1938.0  ...   \n",
       "2  10001  10  180.0    5.0    5.0    2.0    5.0    5.0    5.0   1939.0  ...   \n",
       "3  10003  30  120.0    1.0    5.0    1.0    5.0    5.0    5.0   1956.0  ...   \n",
       "4  10004  10  230.0    1.0    5.0    2.0    1.0    5.0    1.0   1939.0  ...   \n",
       "\n",
       "   HMARITAL  HX060_R  HB031A  HB053  HE012  HE046  HF174   HG092  HF176  HG001  \n",
       "0       1.0      1.0     1.0      1    5.0   12.0    5.0    20.0    1.0    5.0  \n",
       "1       1.0      2.0     1.0      1    5.0   12.0    5.0   100.0    1.0    5.0  \n",
       "2       6.0      1.0     1.0      3    NaN    NaN    5.0  9998.0    3.0    5.0  \n",
       "3       5.0      2.0     1.0      1    NaN    NaN    5.0     0.0    0.0    5.0  \n",
       "4       1.0      1.0     1.0      1    1.0   95.0    1.0   200.0    1.0    5.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, import all the core files in the baseline wave 2002\n",
    "all_core_files = glob.glob('hrs_data/*.csv')\n",
    "# Create an empty dataframe to concatenate with all the dataframes\n",
    "core_data_raw = pd.DataFrame()\n",
    "# Loop through all the files and read them into a dataframe\n",
    "for filename in all_core_files:\n",
    "    dfcore = pd.read_csv(filename, index_col=None, header=0)\n",
    "    col_to_merge = dfcore.columns.difference(core_data_raw.columns)\n",
    "    if filename.endswith('R.csv'):\n",
    "        core_data_raw = pd.concat([core_data_raw, dfcore[col_to_merge]], axis=1)\n",
    "    elif filename.endswith('H.csv'):\n",
    "        dfmerge = pd.concat([dfcore[col_to_merge], dfcore['HHID']], axis=1)\n",
    "        core_data_raw = core_data_raw.merge(dfmerge,\n",
    "                                            on=[\"HHID\"], how=\"inner\")\n",
    "# Select the columns we need\n",
    "core_data = core_data_raw[['HHID', 'PN',\n",
    "                           'HC139', 'HC005', 'HC010', 'HC001', 'HC070',\n",
    "                           'HC117', 'HC128', 'HX067_R', 'HB014A', 'HC134', 'HQ331', 'HQ376', 'HMARITAL', 'HX060_R',\n",
    "                           'HB031A',\n",
    "                           'HB053', 'HE012', 'HE046', 'HF174', 'HG092', 'HF176',\n",
    "                           'HG001'\n",
    "                           ]]\n",
    "# Show the first 5 rows of the dataframe\n",
    "core_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    10973\n",
       "1.0     9245\n",
       "Name: deceased, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the core data with the disposition data\n",
    "df02_d = core_data.merge(dispositon_data, on=[\"HHID\", \"PN\"], how=\"left\")\n",
    "# Fill the respondents not appearing in the disposition data as not deceased\n",
    "df02_d['deceased'] = df02_d['deceased'].fillna(0)\n",
    "# Show the breakdown of deceased and not deceased\n",
    "df02_d['deceased'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 9043 respondents' missing ordinal variables by mean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weight (Pounds)</th>\n",
       "      <th>Self-Rated Health</th>\n",
       "      <th>Education Yrs</th>\n",
       "      <th>Religion Importance</th>\n",
       "      <th>Number of Grandchildren</th>\n",
       "      <th>Hours of Volunteering</th>\n",
       "      <th>Number of Times Socializing</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20218.000000</td>\n",
       "      <td>20218.000000</td>\n",
       "      <td>20218.000000</td>\n",
       "      <td>20218.000000</td>\n",
       "      <td>20218.000000</td>\n",
       "      <td>20218.000000</td>\n",
       "      <td>20218.000000</td>\n",
       "      <td>20218.000000</td>\n",
       "      <td>20218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>171.296493</td>\n",
       "      <td>2.868444</td>\n",
       "      <td>12.100723</td>\n",
       "      <td>1.905974</td>\n",
       "      <td>6.657524</td>\n",
       "      <td>45.703950</td>\n",
       "      <td>1.406329</td>\n",
       "      <td>2.767061</td>\n",
       "      <td>68.788793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>38.282986</td>\n",
       "      <td>1.112213</td>\n",
       "      <td>3.363827</td>\n",
       "      <td>1.359676</td>\n",
       "      <td>5.093500</td>\n",
       "      <td>165.001137</td>\n",
       "      <td>5.934359</td>\n",
       "      <td>12.135002</td>\n",
       "      <td>9.954431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>145.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>170.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.657524</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.788793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>192.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>45.703950</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>9000.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Weight (Pounds)  Self-Rated Health  Education Yrs  Religion Importance  \\\n",
       "count     20218.000000       20218.000000   20218.000000         20218.000000   \n",
       "mean        171.296493           2.868444      12.100723             1.905974   \n",
       "std          38.282986           1.112213       3.363827             1.359676   \n",
       "min          65.000000           1.000000       0.000000             1.000000   \n",
       "25%         145.000000           2.000000      11.000000             1.000000   \n",
       "50%         170.000000           3.000000      12.000000             1.000000   \n",
       "75%         192.000000           4.000000      14.000000             3.000000   \n",
       "max         400.000000           5.000000      17.000000             5.000000   \n",
       "\n",
       "       Number of Grandchildren  Hours of Volunteering  \\\n",
       "count             20218.000000           20218.000000   \n",
       "mean                  6.657524              45.703950   \n",
       "std                   5.093500             165.001137   \n",
       "min                   0.000000               0.000000   \n",
       "25%                   3.000000               0.000000   \n",
       "50%                   6.657524              10.000000   \n",
       "75%                   7.000000              45.703950   \n",
       "max                  77.000000            9000.000000   \n",
       "\n",
       "       Number of Times Socializing        Wealth           Age  \n",
       "count                 20218.000000  20218.000000  20218.000000  \n",
       "mean                      1.406329      2.767061     68.788793  \n",
       "std                       5.934359     12.135002      9.954431  \n",
       "min                       0.000000      0.000000     26.000000  \n",
       "25%                       1.000000      0.000000     62.000000  \n",
       "50%                       1.000000      0.000000     68.788793  \n",
       "75%                       2.000000      0.000000     75.000000  \n",
       "max                     365.000000    100.000000    110.000000  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the continuous/ordinal variables\n",
    "con_col = ['HC139', 'HC001', 'HB014A', 'HB053', 'HE046', 'HG092', 'HF176',\n",
    "           'HX067_R', 'HC134', 'HQ331', 'HQ376']\n",
    "# Create a new dataframe with only continuous/ordinal variables\n",
    "df02_d_ord = df02_d[con_col].copy()\n",
    "# Construct the household asset variable\n",
    "wealth_col = ['HC134', 'HQ331', 'HQ376']\n",
    "for col in wealth_col:\n",
    "    # Set values of no wealth to 0\n",
    "    df02_d_ord[col] = df02_d_ord[col].replace(np.nan, 0)\n",
    "df02_d_ord['wealth_amt'] = df02_d_ord['HC134'] + df02_d_ord['HQ331'] + df02_d_ord['HQ376']\n",
    "# Scale the wealth variable to 0-100\n",
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "df02_d_ord['wealth_amt'] = scaler.fit_transform(df02_d_ord['wealth_amt'].values.reshape(-1, 1))\n",
    "df02_d_ord.drop(['HC134', 'HQ331', 'HQ376'], axis=1, inplace=True)\n",
    "# Construct the age variable\n",
    "df02_d_ord['age'] = 2002 - df02_d_ord['HX067_R']\n",
    "df02_d_ord.drop(['HX067_R'], axis=1, inplace=True)\n",
    "# Set values of missing ordinal variables originally coded as negative to nan\n",
    "df02_d_ord[df02_d_ord < 0] = np.nan\n",
    "for i in df02_d_ord.columns:\n",
    "    if max(df02_d_ord[i]) == 9999:\n",
    "        df02_d_ord[i] = df02_d_ord[i].replace([9999, 9998], np.nan)\n",
    "    elif max(df02_d_ord[i]) == 999:\n",
    "        df02_d_ord[i] = df02_d_ord[i].replace([999, 998], np.nan)\n",
    "    elif max(df02_d_ord[i]) == 99:\n",
    "        df02_d_ord[i] = df02_d_ord[i].replace([99, 98, 97, 95], np.nan)\n",
    "    elif max(df02_d_ord[i]) == 9:\n",
    "        df02_d_ord[i] = df02_d_ord[i].replace([9, 8], np.nan)\n",
    "# Check the number of missing values in original ordinal variables\n",
    "df02_d_ord_nona = df02_d_ord.dropna()\n",
    "print(\"Filled {} respondents' missing ordinal variables by mean\".format(len(df02_d_ord) - len(df02_d_ord_nona)))\n",
    "# Fill the missing values with the mean of the variable\n",
    "df02_d_ord.fillna(df02_d_ord.mean(), inplace=True)\n",
    "# Name the columns\n",
    "df02_d_ord.columns = ['Weight (Pounds)', 'Self-Rated Health', 'Education Yrs',\n",
    "                      'Religion Importance', 'Number of Grandchildren',\n",
    "                      'Hours of Volunteering', 'Number of Times Socializing',\n",
    "                      'Wealth', 'Age'\n",
    "                      ]\n",
    "# Get the summary statistics of the ordinal variables\n",
    "df02_d_ord.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the columns \n",
      " HC005        663\n",
      "HC010        663\n",
      "HC070        663\n",
      "HC117        786\n",
      "HC128        663\n",
      "HMARITAL       1\n",
      "HE012       2107\n",
      "HF174        669\n",
      "HG001        671\n",
      "HX060_R     2051\n",
      "HB031A        41\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Arthritis</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Drunk Alcohol</th>\n",
       "      <th>Married</th>\n",
       "      <th>Children Nearby</th>\n",
       "      <th>Relatives Nearby</th>\n",
       "      <th>Functional Limitations</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20213</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20214</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20215</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>non-White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20216</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20217</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20218 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hypertension  Diabetes  Arthritis  Smoking  Drunk Alcohol  Married  \\\n",
       "0             False     False      False    False           True     True   \n",
       "1             False     False       True    False          False     True   \n",
       "2             False     False      False    False          False    False   \n",
       "3              True     False      False    False          False    False   \n",
       "4              True     False       True    False           True     True   \n",
       "...             ...       ...        ...      ...            ...      ...   \n",
       "20213          True     False       True    False          False    False   \n",
       "20214          True     False       True    False          False    False   \n",
       "20215          True     False       True    False          False    False   \n",
       "20216          True     False       True    False          False     True   \n",
       "20217          True     False       True    False          False     True   \n",
       "\n",
       "       Children Nearby  Relatives Nearby  Functional Limitations     Sex  \\\n",
       "0                False             False                   False    Male   \n",
       "1                False             False                   False  Female   \n",
       "2                 True             False                   False    Male   \n",
       "3                 True             False                   False  Female   \n",
       "4                 True              True                   False    Male   \n",
       "...                ...               ...                     ...     ...   \n",
       "20213             True             False                   False  Female   \n",
       "20214             True             False                   False  Female   \n",
       "20215            False             False                   False  Female   \n",
       "20216             True             False                   False  Female   \n",
       "20217             True             False                   False  Female   \n",
       "\n",
       "            Race  \n",
       "0          White  \n",
       "1          White  \n",
       "2          White  \n",
       "3          White  \n",
       "4          White  \n",
       "...          ...  \n",
       "20213      White  \n",
       "20214      White  \n",
       "20215  non-White  \n",
       "20216      White  \n",
       "20217      White  \n",
       "\n",
       "[20218 rows x 11 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the binary/categorical variables\n",
    "cat_col = ['HC005', 'HC010', 'HC070', 'HC117', 'HC128', 'HMARITAL', 'HE012',\n",
    "           'HF174', 'HG001',\n",
    "           'HX060_R', 'HB031A']\n",
    "df02_d_cat = df02_d[cat_col].copy()\n",
    "# Check the number of missing values in original categorical variables\n",
    "print(\"Missing values in the columns\", \"\\n\", df02_d_cat.isnull().sum())\n",
    "# Fill the missing values with the mode of the variable\n",
    "df02_d_cat.fillna(df02_d_cat.mode().iloc[0], inplace=True)\n",
    "for i in cat_col[:3]:\n",
    "    df02_d_cat[i] = df02_d_cat[i].isin([1, 3])\n",
    "for i in cat_col[3:9]:\n",
    "    df02_d_cat[i] = df02_d_cat[i].isin([1])\n",
    "df02_d_cat['HX060_R'] = np.where(df02_d_cat['HX060_R'] == 1, \"Male\", \"Female\")\n",
    "df02_d_cat['HB031A'] = np.where(df02_d_cat['HB031A'] == 1, \"White\", \"non-White\")\n",
    "df02_d_cat.columns = ['Hypertension', 'Diabetes', 'Arthritis', 'Smoking',\n",
    "                      'Drunk Alcohol', 'Married', 'Children Nearby',\n",
    "                      'Relatives Nearby', 'Functional Limitations',\n",
    "                      'Sex', 'Race']\n",
    "df02_d_cat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the earlier descriptive statistics in cleaning continuous variables, we see that the variable <code>age</code> variable includes the respondents aged below 51. This is because HRS sample collection is based on the households and include the spouse or partner of the main household respondent whose age may below 51. We will remove the respondents aged below 51 in the data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceased</th>\n",
       "      <th>Weight (Pounds)</th>\n",
       "      <th>Self-Rated Health</th>\n",
       "      <th>Education Yrs</th>\n",
       "      <th>Religion Importance</th>\n",
       "      <th>Number of Grandchildren</th>\n",
       "      <th>Hours of Volunteering</th>\n",
       "      <th>Number of Times Socializing</th>\n",
       "      <th>Wealth</th>\n",
       "      <th>Age</th>\n",
       "      <th>...</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Arthritis</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Drunk Alcohol</th>\n",
       "      <th>Married</th>\n",
       "      <th>Children Nearby</th>\n",
       "      <th>Relatives Nearby</th>\n",
       "      <th>Functional Limitations</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.657524</td>\n",
       "      <td>45.70395</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.657524</td>\n",
       "      <td>200.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.657524</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20213</th>\n",
       "      <td>0.0</td>\n",
       "      <td>171.296493</td>\n",
       "      <td>2.868444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>45.70395</td>\n",
       "      <td>1.406329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.788793</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20214</th>\n",
       "      <td>0.0</td>\n",
       "      <td>171.296493</td>\n",
       "      <td>2.868444</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>45.70395</td>\n",
       "      <td>1.406329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.788793</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20215</th>\n",
       "      <td>1.0</td>\n",
       "      <td>171.296493</td>\n",
       "      <td>2.868444</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>45.70395</td>\n",
       "      <td>1.406329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.788793</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>non-White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20216</th>\n",
       "      <td>1.0</td>\n",
       "      <td>171.296493</td>\n",
       "      <td>2.868444</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>45.70395</td>\n",
       "      <td>1.406329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.788793</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20217</th>\n",
       "      <td>0.0</td>\n",
       "      <td>171.296493</td>\n",
       "      <td>2.868444</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>45.70395</td>\n",
       "      <td>1.406329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.788793</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19762 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       deceased  Weight (Pounds)  Self-Rated Health  Education Yrs  \\\n",
       "0           1.0       170.000000           3.000000           12.0   \n",
       "1           1.0       179.000000           3.000000           16.0   \n",
       "2           0.0       180.000000           2.000000           12.0   \n",
       "4           1.0       230.000000           2.000000           16.0   \n",
       "5           0.0       155.000000           1.000000           12.0   \n",
       "...         ...              ...                ...            ...   \n",
       "20213       0.0       171.296493           2.868444            0.0   \n",
       "20214       0.0       171.296493           2.868444           14.0   \n",
       "20215       1.0       171.296493           2.868444           12.0   \n",
       "20216       1.0       171.296493           2.868444           15.0   \n",
       "20217       0.0       171.296493           2.868444           15.0   \n",
       "\n",
       "       Religion Importance  Number of Grandchildren  Hours of Volunteering  \\\n",
       "0                      1.0                12.000000               20.00000   \n",
       "1                      1.0                12.000000              100.00000   \n",
       "2                      3.0                 6.657524               45.70395   \n",
       "4                      1.0                 6.657524              200.00000   \n",
       "5                      1.0                 6.657524               40.00000   \n",
       "...                    ...                      ...                    ...   \n",
       "20213                  1.0                20.000000               45.70395   \n",
       "20214                  1.0                 2.000000               45.70395   \n",
       "20215                  3.0                 8.000000               45.70395   \n",
       "20216                  5.0                 2.000000               45.70395   \n",
       "20217                  5.0                 2.000000               45.70395   \n",
       "\n",
       "       Number of Times Socializing  Wealth        Age  ...  Diabetes  \\\n",
       "0                         1.000000     0.0  66.000000  ...     False   \n",
       "1                         1.000000     0.0  64.000000  ...     False   \n",
       "2                         3.000000     0.0  63.000000  ...     False   \n",
       "4                         1.000000     0.0  63.000000  ...     False   \n",
       "5                         1.000000     0.0  56.000000  ...     False   \n",
       "...                            ...     ...        ...  ...       ...   \n",
       "20213                     1.406329     0.0  68.788793  ...     False   \n",
       "20214                     1.406329     0.0  68.788793  ...     False   \n",
       "20215                     1.406329     0.0  68.788793  ...     False   \n",
       "20216                     1.406329     0.0  68.788793  ...     False   \n",
       "20217                     1.406329     0.0  68.788793  ...     False   \n",
       "\n",
       "       Arthritis  Smoking  Drunk Alcohol  Married  Children Nearby  \\\n",
       "0          False    False           True     True            False   \n",
       "1           True    False          False     True            False   \n",
       "2          False    False          False    False             True   \n",
       "4           True    False           True     True             True   \n",
       "5          False    False           True     True             True   \n",
       "...          ...      ...            ...      ...              ...   \n",
       "20213       True    False          False    False             True   \n",
       "20214       True    False          False    False             True   \n",
       "20215       True    False          False    False            False   \n",
       "20216       True    False          False     True             True   \n",
       "20217       True    False          False     True             True   \n",
       "\n",
       "       Relatives Nearby  Functional Limitations     Sex       Race  \n",
       "0                 False                   False    Male      White  \n",
       "1                 False                   False  Female      White  \n",
       "2                 False                   False    Male      White  \n",
       "4                  True                   False    Male      White  \n",
       "5                  True                   False  Female      White  \n",
       "...                 ...                     ...     ...        ...  \n",
       "20213             False                   False  Female      White  \n",
       "20214             False                   False  Female      White  \n",
       "20215             False                   False  Female  non-White  \n",
       "20216             False                   False  Female      White  \n",
       "20217             False                   False  Female      White  \n",
       "\n",
       "[19762 rows x 21 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrs_clean_full = pd.concat([df02_d['deceased'], df02_d_ord, df02_d_cat], axis=1)\n",
    "hrs_clean = hrs_clean_full[hrs_clean_full['Age'] >= 51]\n",
    "hrs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hrs_clean.to_csv('hrs_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#describe data after train-test split: class distribution, sizes -> balanced and proportion in the training and testing sets\n",
    "#describe parameter tuning grid search CV before doing LR\n",
    "#get the best parameter -> show the best f1 score\n",
    "\n",
    "##grid search for LR, Naiv Bayes\n",
    "# show parameter tuning grid search CV results in markdown\n",
    "# show classification report, plot ROC, and show confusion matrix\n",
    "# qualitative measures of errors: top positive features, error analysis\n",
    "\n",
    "#interpret model performance by harmonizing three types of evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x7fece1aabe50>"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "def is_male(s):\n",
    "    return s == 'Male'\n",
    "\n",
    "\n",
    "def is_white(s):\n",
    "    return s == 'White'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "hrs_df = pd.read_csv('hrs_clean.csv')\n",
    "hrs_df['Sex'] = hrs_df['Sex'].apply(is_male)\n",
    "hrs_df['Race'] = hrs_df['Race'].apply(is_white)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "y_df = hrs_df[['deceased']]\n",
    "X_df = hrs_df.drop(columns=['deceased'], inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyu/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6215532506956741"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l = LogisticRegression(random_state=seed, solver='liblinear').fit(X_train, y_train)\n",
    "clf_l.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.63475801e-03  1.08143377e-02 -8.82312462e-02  1.96453135e-02\n",
      "   2.35904361e-02 -2.23301806e-04 -2.52580913e-03 -9.41758976e-04\n",
      "   3.01565548e-02  1.46469339e-01  6.84343432e-03  1.59596792e-01\n",
      "  -3.38827108e-01 -1.75716291e-01 -4.70444079e-01  1.42722059e-01\n",
      "   3.75167560e-02  1.29907791e-01 -1.72838928e-01  2.51075920e-01]]\n",
      "[-0.66096528]\n"
     ]
    }
   ],
   "source": [
    "print(clf_l.coef_)\n",
    "print(clf_l.intercept_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performence on train set 0.6302738946169903\n",
      "Performence on test set 0.6215532506956741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.72      0.67      2115\n",
      "         1.0       0.61      0.51      0.56      1838\n",
      "\n",
      "    accuracy                           0.62      3953\n",
      "   macro avg       0.62      0.61      0.61      3953\n",
      "weighted avg       0.62      0.62      0.62      3953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_l.predict(X_test)\n",
    "y_prob = clf_l.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Performence on train set\", accuracy_score(clf_l.predict(X_train), y_train))\n",
    "print(\"Performence on test set\", accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6648195309425138\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(roc_auc_score(y_test, y_prob))\n",
    "cur = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.plot(cur[0], cur[1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0.6035922084492791\n",
      "0.0001 0.618770554009613\n",
      "0.001 0.6202883885656464\n",
      "0.01 0.6182646091576018\n",
      "0.1 0.618770554009613\n",
      "1.0 0.6215532506956741\n",
      "2 0.6213002782696686\n",
      "3 0.6218062231216798\n",
      "4 0.6215532506956741\n",
      "5 0.6215532506956741\n",
      "6 0.6218062231216798\n",
      "7 0.6210473058436631\n",
      "8 0.6215532506956741\n"
     ]
    }
   ],
   "source": [
    "for i_c in [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 2, 3, 4, 5, 6, 7, 8]:\n",
    "    clf_l_l2 = LogisticRegression(penalty='l2', C=i_c, random_state=seed, solver='liblinear').fit(X_train,\n",
    "                                                                                                  np.ravel(y_train))\n",
    "    print(i_c, clf_l_l2.score(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 0.5350366810017708\n",
      "0.0001 0.5691879585125221\n",
      "0.001 0.6162408297495573\n",
      "0.01 0.6180116367315962\n",
      "0.1 0.6162408297495573\n",
      "1.0 0.6207943334176574\n",
      "2 0.6213002782696686\n",
      "3 0.6215532506956741\n",
      "4 0.6220591955476853\n",
      "5 0.6220591955476853\n",
      "6 0.6220591955476853\n",
      "7 0.6218062231216798\n",
      "8 0.6218062231216798\n"
     ]
    }
   ],
   "source": [
    "for i_c in [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 2, 3, 4, 5, 6, 7, 8]:\n",
    "    clf_l_l1 = LogisticRegression(penalty='l1', C=i_c, random_state=seed, solver='liblinear').fit(X_train,\n",
    "                                                                                                  np.ravel(y_train))\n",
    "    print(i_c, clf_l_l1.score(X_test, y_test))\n",
    "i_c = 4.5\n",
    "clf_l_l1 = LogisticRegression(penalty='l1', C=i_c, random_state=seed, solver='liblinear').fit(X_train,\n",
    "                                                                                              np.ravel(y_train))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-4.62327463e-03,  1.10483990e-02, -8.81345866e-02,\n         1.96338774e-02,  2.36272858e-02, -2.23097869e-04,\n        -2.52117512e-03, -9.42945780e-04,  3.01988700e-02,\n         1.46400461e-01,  5.98437654e-03,  1.59528476e-01,\n        -3.39367208e-01, -1.75401204e-01, -4.70842635e-01,\n         1.42931330e-01,  3.74044549e-02,  1.29704866e-01,\n        -1.72576332e-01,  2.51737647e-01]])"
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_l_l1.coef_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, dims, act=False):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.act = act\n",
    "        self.linears = torch.nn.ModuleList()\n",
    "        for idx in range(len(dims) - 1):\n",
    "            self.linears.append(torch.nn.Linear(dims[idx], dims[idx + 1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x\n",
    "        for ly in self.linears:\n",
    "            if self.act:\n",
    "                output = torch.relu(output)\n",
    "            output = ly(output)\n",
    "        output = torch.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def predict(self, x, thresh=0.5):\n",
    "        output = x\n",
    "        for ly in self.linears:\n",
    "            if self.act:\n",
    "                output = torch.relu(output)\n",
    "            output = ly(output)\n",
    "        output = torch.sigmoid(output)\n",
    "        output = [xi >= thresh for xi in torch.squeeze(output)]\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def train_nn(nn_k, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000):\n",
    "    losses = []\n",
    "    losses_test = []\n",
    "    Iterations = []\n",
    "    iter = -1\n",
    "    for epoch in range(epochs):\n",
    "        x = X_train_ts\n",
    "        labels = y_train_ts\n",
    "        optimizer.zero_grad()  # Setting our stored gradients equal to zero\n",
    "        outputs = nn_k(x)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()  # Computes the gradient of the given tensor w.r.t. the weights/bias\n",
    "\n",
    "        optimizer.step()  # Updates weights and biases with the optimizer (SGD)\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                # Calculating the loss and accuracy for the test dataset\n",
    "                correct_test = 0\n",
    "                total_test = 0\n",
    "                outputs_test = nn_k(X_val_ts)\n",
    "                loss_test = criterion(outputs_test, y_val_ts)\n",
    "\n",
    "                predicted_test = outputs_test.round().detach().numpy()\n",
    "                total_test += y_val_ts.size(0)\n",
    "                correct_test += np.sum(predicted_test == y_val_ts.detach().numpy())\n",
    "                accuracy_test = 100 * correct_test / total_test\n",
    "                losses_test.append(loss_test.item())\n",
    "\n",
    "                # Calculating the loss and accuracy for the train dataset\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                total += y_train_ts.size(0)\n",
    "                correct += np.sum(torch.squeeze(outputs).round().detach().numpy() == y_train_ts.detach().numpy())\n",
    "                accuracy = 100 * correct / total\n",
    "                losses.append(loss.item())\n",
    "                Iterations.append(iter)\n",
    "\n",
    "                print(f\"Iteration: {iter}. \\nTest - Loss: {loss_test.item()}. Accuracy: {accuracy_test}\")\n",
    "                print(f\"Train -  Loss: {loss.item()}. Accuracy: {accuracy}\\n\")\n",
    "\n",
    "    return nn_l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "X_train_np = np.array(X_train).astype(float)\n",
    "y_train_np = np.array(y_train).astype(float)\n",
    "X_test_np = torch.Tensor(np.array(X_test).astype(float))\n",
    "y_test_np = torch.Tensor(np.array(y_test).astype(float))\n",
    "\n",
    "X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(X_train_np, y_train_np,\n",
    "                                                              test_size=0.25,\n",
    "                                                              random_state=seed)\n",
    "\n",
    "X_train_ts, X_val_ts, X_test_ts = torch.Tensor(X_train_np), torch.Tensor(X_val_np), torch.Tensor(X_test_np)\n",
    "y_train_ts, y_val_ts, y_test_ts = torch.Tensor(y_train_np), torch.Tensor(y_val_np), torch.Tensor(y_test_np)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. \n",
      "Test - Loss: 2.6070327758789062. Accuracy: 54.13609916519099\n",
      "Train -  Loss: 2.925916910171509. Accuracy: 563958.0971659919\n",
      "\n",
      "Iteration: 20. \n",
      "Test - Loss: 1.2053086757659912. Accuracy: 56.43814824184164\n",
      "Train -  Loss: 1.1912716627120972. Accuracy: 595029.9595141701\n",
      "\n",
      "Iteration: 40. \n",
      "Test - Loss: 1.1166075468063354. Accuracy: 56.99468757905388\n",
      "Train -  Loss: 1.0918006896972656. Accuracy: 594374.0890688259\n",
      "\n",
      "Iteration: 60. \n",
      "Test - Loss: 1.0502108335494995. Accuracy: 56.79230963824943\n",
      "Train -  Loss: 1.0293036699295044. Accuracy: 594628.2388663967\n",
      "\n",
      "Iteration: 80. \n",
      "Test - Loss: 0.9911094903945923. Accuracy: 57.01998482165444\n",
      "Train -  Loss: 0.9737199544906616. Accuracy: 595177.5303643724\n",
      "\n",
      "Iteration: 100. \n",
      "Test - Loss: 0.9395629167556763. Accuracy: 57.171768277257776\n",
      "Train -  Loss: 0.9253186583518982. Accuracy: 595571.052631579\n",
      "\n",
      "Iteration: 120. \n",
      "Test - Loss: 0.8955408930778503. Accuracy: 57.27295724766\n",
      "Train -  Loss: 0.8840181827545166. Accuracy: 596423.6842105263\n",
      "\n",
      "Iteration: 140. \n",
      "Test - Loss: 0.8586907386779785. Accuracy: 57.52592967366557\n",
      "Train -  Loss: 0.8494446277618408. Accuracy: 596948.3805668016\n",
      "\n",
      "Iteration: 160. \n",
      "Test - Loss: 0.8283395767211914. Accuracy: 57.70301037186947\n",
      "Train -  Loss: 0.8209985494613647. Accuracy: 597776.4170040486\n",
      "\n",
      "Iteration: 180. \n",
      "Test - Loss: 0.8036435842514038. Accuracy: 57.90538831267392\n",
      "Train -  Loss: 0.7978186011314392. Accuracy: 598481.4777327935\n",
      "\n",
      "Iteration: 200. \n",
      "Test - Loss: 0.7836408019065857. Accuracy: 57.728307614470026\n",
      "Train -  Loss: 0.7790398001670837. Accuracy: 599252.1255060729\n",
      "\n",
      "Iteration: 220. \n",
      "Test - Loss: 0.7674523591995239. Accuracy: 57.75360485707058\n",
      "Train -  Loss: 0.7638410329818726. Accuracy: 599817.8137651822\n",
      "\n",
      "Iteration: 240. \n",
      "Test - Loss: 0.7542746067047119. Accuracy: 57.778902099671136\n",
      "Train -  Loss: 0.7514467835426331. Accuracy: 600457.2874493927\n",
      "\n",
      "Iteration: 260. \n",
      "Test - Loss: 0.7434912323951721. Accuracy: 57.88009107007336\n",
      "Train -  Loss: 0.74129718542099. Accuracy: 600998.3805668016\n",
      "\n",
      "Iteration: 280. \n",
      "Test - Loss: 0.7345956563949585. Accuracy: 57.95598279787503\n",
      "Train -  Loss: 0.7329543232917786. Accuracy: 601424.6963562754\n",
      "\n",
      "Iteration: 300. \n",
      "Test - Loss: 0.7272454500198364. Accuracy: 57.6271186440678\n",
      "Train -  Loss: 0.7260783910751343. Accuracy: 601744.4331983805\n",
      "\n",
      "Iteration: 320. \n",
      "Test - Loss: 0.721167802810669. Accuracy: 57.75360485707058\n",
      "Train -  Loss: 0.7203983664512634. Accuracy: 601941.1943319838\n",
      "\n",
      "Iteration: 340. \n",
      "Test - Loss: 0.7161519527435303. Accuracy: 57.804199342271694\n",
      "Train -  Loss: 0.7157634496688843. Accuracy: 602195.3441295547\n",
      "\n",
      "Iteration: 360. \n",
      "Test - Loss: 0.7120069265365601. Accuracy: 57.804199342271694\n",
      "Train -  Loss: 0.7119836807250977. Accuracy: 602383.9068825911\n",
      "\n",
      "Iteration: 380. \n",
      "Test - Loss: 0.7086296081542969. Accuracy: 57.98128004047559\n",
      "Train -  Loss: 0.7089232802391052. Accuracy: 602474.0890688259\n",
      "\n",
      "Iteration: 400. \n",
      "Test - Loss: 0.7058561444282532. Accuracy: 58.05717176827726\n",
      "Train -  Loss: 0.7064570784568787. Accuracy: 602556.0728744939\n",
      "\n",
      "Iteration: 420. \n",
      "Test - Loss: 0.7035976052284241. Accuracy: 58.05717176827726\n",
      "Train -  Loss: 0.7044585347175598. Accuracy: 602679.048582996\n",
      "\n",
      "Iteration: 440. \n",
      "Test - Loss: 0.7017595767974854. Accuracy: 58.05717176827726\n",
      "Train -  Loss: 0.7028706669807434. Accuracy: 602793.8259109312\n",
      "\n",
      "Iteration: 460. \n",
      "Test - Loss: 0.7002611756324768. Accuracy: 58.05717176827726\n",
      "Train -  Loss: 0.7015959024429321. Accuracy: 602867.6113360324\n",
      "\n",
      "Iteration: 480. \n",
      "Test - Loss: 0.6990347504615784. Accuracy: 58.082469010877816\n",
      "Train -  Loss: 0.7005544304847717. Accuracy: 603039.7773279352\n",
      "\n",
      "Iteration: 500. \n",
      "Test - Loss: 0.6980246901512146. Accuracy: 58.107766253478374\n",
      "Train -  Loss: 0.6997204422950745. Accuracy: 603080.7692307692\n",
      "\n",
      "Iteration: 520. \n",
      "Test - Loss: 0.697173535823822. Accuracy: 58.107766253478374\n",
      "Train -  Loss: 0.699024498462677. Accuracy: 603129.9595141701\n",
      "\n",
      "Iteration: 540. \n",
      "Test - Loss: 0.6964706182479858. Accuracy: 58.082469010877816\n",
      "Train -  Loss: 0.6984586715698242. Accuracy: 603113.5627530364\n",
      "\n",
      "Iteration: 560. \n",
      "Test - Loss: 0.695863664150238. Accuracy: 58.05717176827726\n",
      "Train -  Loss: 0.6979714632034302. Accuracy: 603236.5384615385\n",
      "\n",
      "Iteration: 580. \n",
      "Test - Loss: 0.6953425407409668. Accuracy: 58.0318745256767\n",
      "Train -  Loss: 0.6975654363632202. Accuracy: 603187.3481781377\n",
      "\n",
      "Iteration: 600. \n",
      "Test - Loss: 0.69488924741745. Accuracy: 58.133063496078925\n",
      "Train -  Loss: 0.6972045302391052. Accuracy: 603211.943319838\n",
      "\n",
      "Iteration: 620. \n",
      "Test - Loss: 0.6944901347160339. Accuracy: 58.23425246648115\n",
      "Train -  Loss: 0.6968868970870972. Accuracy: 603302.1255060729\n",
      "\n",
      "Iteration: 640. \n",
      "Test - Loss: 0.6941267251968384. Accuracy: 58.107766253478374\n",
      "Train -  Loss: 0.6966110467910767. Accuracy: 603326.7206477732\n",
      "\n",
      "Iteration: 660. \n",
      "Test - Loss: 0.6938064694404602. Accuracy: 58.05717176827726\n",
      "Train -  Loss: 0.6963537335395813. Accuracy: 603318.5222672065\n",
      "\n",
      "Iteration: 680. \n",
      "Test - Loss: 0.6935077905654907. Accuracy: 57.98128004047559\n",
      "Train -  Loss: 0.6961184144020081. Accuracy: 603334.91902834\n",
      "\n",
      "Iteration: 700. \n",
      "Test - Loss: 0.6932332515716553. Accuracy: 58.05717176827726\n",
      "Train -  Loss: 0.695898711681366. Accuracy: 603310.3238866397\n",
      "\n",
      "Iteration: 720. \n",
      "Test - Loss: 0.6929728388786316. Accuracy: 58.082469010877816\n",
      "Train -  Loss: 0.6956950426101685. Accuracy: 603343.1174089069\n",
      "\n",
      "Iteration: 740. \n",
      "Test - Loss: 0.6927348971366882. Accuracy: 58.082469010877816\n",
      "Train -  Loss: 0.6955010294914246. Accuracy: 603351.3157894737\n",
      "\n",
      "Iteration: 760. \n",
      "Test - Loss: 0.6925061345100403. Accuracy: 58.082469010877816\n",
      "Train -  Loss: 0.6953176259994507. Accuracy: 603359.5141700405\n",
      "\n",
      "Iteration: 780. \n",
      "Test - Loss: 0.6922853589057922. Accuracy: 58.082469010877816\n",
      "Train -  Loss: 0.6951363682746887. Accuracy: 603375.9109311741\n",
      "\n",
      "Iteration: 800. \n",
      "Test - Loss: 0.692080020904541. Accuracy: 58.18365798128004\n",
      "Train -  Loss: 0.6949672102928162. Accuracy: 603375.9109311741\n",
      "\n",
      "Iteration: 820. \n",
      "Test - Loss: 0.691876232624054. Accuracy: 58.158360738679484\n",
      "Train -  Loss: 0.6947999000549316. Accuracy: 603408.7044534413\n",
      "\n",
      "Iteration: 840. \n",
      "Test - Loss: 0.6916850805282593. Accuracy: 58.158360738679484\n",
      "Train -  Loss: 0.6946380734443665. Accuracy: 603408.7044534413\n",
      "\n",
      "Iteration: 860. \n",
      "Test - Loss: 0.6914982795715332. Accuracy: 58.158360738679484\n",
      "Train -  Loss: 0.6944843530654907. Accuracy: 603392.3076923077\n",
      "\n",
      "Iteration: 880. \n",
      "Test - Loss: 0.6913187503814697. Accuracy: 58.158360738679484\n",
      "Train -  Loss: 0.6943314671516418. Accuracy: 603408.7044534413\n",
      "\n",
      "Iteration: 900. \n",
      "Test - Loss: 0.691142737865448. Accuracy: 58.18365798128004\n",
      "Train -  Loss: 0.6941827535629272. Accuracy: 603425.1012145749\n",
      "\n",
      "Iteration: 920. \n",
      "Test - Loss: 0.6909703016281128. Accuracy: 58.208955223880594\n",
      "Train -  Loss: 0.6940346360206604. Accuracy: 603433.2995951417\n",
      "\n",
      "Iteration: 940. \n",
      "Test - Loss: 0.6908038854598999. Accuracy: 58.18365798128004\n",
      "Train -  Loss: 0.6938929557800293. Accuracy: 603416.9028340081\n",
      "\n",
      "Iteration: 960. \n",
      "Test - Loss: 0.6906428337097168. Accuracy: 58.23425246648115\n",
      "Train -  Loss: 0.6937517523765564. Accuracy: 603441.4979757085\n",
      "\n",
      "Iteration: 980. \n",
      "Test - Loss: 0.6904845237731934. Accuracy: 58.23425246648115\n",
      "Train -  Loss: 0.6936137080192566. Accuracy: 603449.6963562754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_l = LinearClassifier([len(X_train_np[0]), 1])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(nn_l.parameters(), lr=2e-4)\n",
    "\n",
    "nn_l = train_nn(nn_l, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. \n",
      "Test - Loss: 3.37021541595459. Accuracy: 50.771565899316975\n",
      "Train -  Loss: 4.189990520477295. Accuracy: 635300.4048582996\n",
      "\n",
      "Iteration: 20. \n",
      "Test - Loss: 1.4316766262054443. Accuracy: 52.314697697950926\n",
      "Train -  Loss: 1.551377296447754. Accuracy: 635677.5303643724\n",
      "\n",
      "Iteration: 40. \n",
      "Test - Loss: 0.911888837814331. Accuracy: 53.73134328358209\n",
      "Train -  Loss: 0.9098217487335205. Accuracy: 618985.6275303643\n",
      "\n",
      "Iteration: 60. \n",
      "Test - Loss: 0.7461252808570862. Accuracy: 54.41436883379711\n",
      "Train -  Loss: 0.7562097907066345. Accuracy: 597981.3765182187\n",
      "\n",
      "Iteration: 80. \n",
      "Test - Loss: 0.6974267959594727. Accuracy: 55.55274475082216\n",
      "Train -  Loss: 0.6968273520469666. Accuracy: 606729.048582996\n",
      "\n",
      "Iteration: 100. \n",
      "Test - Loss: 0.6927720308303833. Accuracy: 56.64052618264609\n",
      "Train -  Loss: 0.689319908618927. Accuracy: 602679.048582996\n",
      "\n",
      "Iteration: 120. \n",
      "Test - Loss: 0.6877872943878174. Accuracy: 56.99468757905388\n",
      "Train -  Loss: 0.6844087839126587. Accuracy: 602851.2145748988\n",
      "\n",
      "Iteration: 140. \n",
      "Test - Loss: 0.6845842003822327. Accuracy: 57.65241588666835\n",
      "Train -  Loss: 0.6808664798736572. Accuracy: 603482.4898785425\n",
      "\n",
      "Iteration: 160. \n",
      "Test - Loss: 0.6821696162223816. Accuracy: 57.829496584872246\n",
      "Train -  Loss: 0.6781027317047119. Accuracy: 603531.6801619433\n",
      "\n",
      "Iteration: 180. \n",
      "Test - Loss: 0.6801284551620483. Accuracy: 58.18365798128004\n",
      "Train -  Loss: 0.6758460998535156. Accuracy: 603835.020242915\n",
      "\n",
      "Iteration: 200. \n",
      "Test - Loss: 0.6784226298332214. Accuracy: 58.28484695168227\n",
      "Train -  Loss: 0.6739612221717834. Accuracy: 603949.7975708502\n",
      "\n",
      "Iteration: 220. \n",
      "Test - Loss: 0.6769624352455139. Accuracy: 58.66430559069062\n",
      "Train -  Loss: 0.6723540425300598. Accuracy: 604072.7732793522\n",
      "\n",
      "Iteration: 240. \n",
      "Test - Loss: 0.6756885051727295. Accuracy: 58.81608904629395\n",
      "Train -  Loss: 0.6709592938423157. Accuracy: 604220.3441295547\n",
      "\n",
      "Iteration: 260. \n",
      "Test - Loss: 0.6745643019676208. Accuracy: 58.99316974449785\n",
      "Train -  Loss: 0.669730007648468. Accuracy: 604187.5506072874\n",
      "\n",
      "Iteration: 280. \n",
      "Test - Loss: 0.6735594868659973. Accuracy: 59.043764229698965\n",
      "Train -  Loss: 0.6686314940452576. Accuracy: 604253.1376518218\n",
      "\n",
      "Iteration: 300. \n",
      "Test - Loss: 0.6726517081260681. Accuracy: 59.11965595750063\n",
      "Train -  Loss: 0.6676378846168518. Accuracy: 604367.914979757\n",
      "\n",
      "Iteration: 320. \n",
      "Test - Loss: 0.6718239784240723. Accuracy: 59.32203389830509\n",
      "Train -  Loss: 0.6667296290397644. Accuracy: 604466.2955465587\n",
      "\n",
      "Iteration: 340. \n",
      "Test - Loss: 0.6710630655288696. Accuracy: 59.372628383506196\n",
      "Train -  Loss: 0.6658915877342224. Accuracy: 604449.8987854251\n",
      "\n",
      "Iteration: 360. \n",
      "Test - Loss: 0.6703587770462036. Accuracy: 59.60030356691121\n",
      "Train -  Loss: 0.6651120185852051. Accuracy: 604507.2874493927\n",
      "\n",
      "Iteration: 380. \n",
      "Test - Loss: 0.669702410697937. Accuracy: 59.54970908171009\n",
      "Train -  Loss: 0.6643816828727722. Accuracy: 604613.8663967612\n",
      "\n",
      "Iteration: 400. \n",
      "Test - Loss: 0.669087827205658. Accuracy: 59.7773842651151\n",
      "Train -  Loss: 0.6636935472488403. Accuracy: 604728.6437246964\n",
      "\n",
      "Iteration: 420. \n",
      "Test - Loss: 0.6685093641281128. Accuracy: 59.726789779913986\n",
      "Train -  Loss: 0.6630417108535767. Accuracy: 604712.2469635628\n",
      "\n",
      "Iteration: 440. \n",
      "Test - Loss: 0.6679624915122986. Accuracy: 59.82797875031621\n",
      "Train -  Loss: 0.6624215841293335. Accuracy: 604753.2388663967\n",
      "\n",
      "Iteration: 460. \n",
      "Test - Loss: 0.6674439311027527. Accuracy: 59.954464963319\n",
      "Train -  Loss: 0.6618295907974243. Accuracy: 604753.2388663967\n",
      "\n",
      "Iteration: 480. \n",
      "Test - Loss: 0.6669504642486572. Accuracy: 59.954464963319\n",
      "Train -  Loss: 0.661262571811676. Accuracy: 604786.032388664\n",
      "\n",
      "Iteration: 500. \n",
      "Test - Loss: 0.666479766368866. Accuracy: 60.08095117632178\n",
      "Train -  Loss: 0.6607182621955872. Accuracy: 604794.2307692308\n",
      "\n",
      "Iteration: 520. \n",
      "Test - Loss: 0.6660296320915222. Accuracy: 60.15684290412345\n",
      "Train -  Loss: 0.6601945757865906. Accuracy: 604851.6194331984\n",
      "\n",
      "Iteration: 540. \n",
      "Test - Loss: 0.6655985116958618. Accuracy: 60.25803187452568\n",
      "Train -  Loss: 0.6596897840499878. Accuracy: 604818.8259109312\n",
      "\n",
      "Iteration: 560. \n",
      "Test - Loss: 0.665184736251831. Accuracy: 60.18214014672401\n",
      "Train -  Loss: 0.6592025756835938. Accuracy: 604892.6113360324\n",
      "\n",
      "Iteration: 580. \n",
      "Test - Loss: 0.6647871732711792. Accuracy: 60.25803187452568\n",
      "Train -  Loss: 0.6587318778038025. Accuracy: 604868.016194332\n",
      "\n",
      "Iteration: 600. \n",
      "Test - Loss: 0.6644047498703003. Accuracy: 60.25803187452568\n",
      "Train -  Loss: 0.6582766175270081. Accuracy: 604884.4129554656\n",
      "\n",
      "Iteration: 620. \n",
      "Test - Loss: 0.6640365719795227. Accuracy: 60.359220844927904\n",
      "Train -  Loss: 0.6578360199928284. Accuracy: 604868.016194332\n",
      "\n",
      "Iteration: 640. \n",
      "Test - Loss: 0.6636818051338196. Accuracy: 60.333923602327346\n",
      "Train -  Loss: 0.6574094891548157. Accuracy: 604876.2145748988\n",
      "\n",
      "Iteration: 660. \n",
      "Test - Loss: 0.6633398532867432. Accuracy: 60.409815330129014\n",
      "Train -  Loss: 0.6569963693618774. Accuracy: 604958.1983805668\n",
      "\n",
      "Iteration: 680. \n",
      "Test - Loss: 0.6630101203918457. Accuracy: 60.5363015431318\n",
      "Train -  Loss: 0.6565962433815002. Accuracy: 605015.5870445344\n",
      "\n",
      "Iteration: 700. \n",
      "Test - Loss: 0.6626922488212585. Accuracy: 60.5363015431318\n",
      "Train -  Loss: 0.6562087535858154. Accuracy: 605031.983805668\n",
      "\n",
      "Iteration: 720. \n",
      "Test - Loss: 0.6623860597610474. Accuracy: 60.688084998735135\n",
      "Train -  Loss: 0.655833899974823. Accuracy: 605048.3805668016\n",
      "\n",
      "Iteration: 740. \n",
      "Test - Loss: 0.6620912551879883. Accuracy: 60.61219327093347\n",
      "Train -  Loss: 0.6554715037345886. Accuracy: 605081.1740890688\n",
      "\n",
      "Iteration: 760. \n",
      "Test - Loss: 0.6618079543113708. Accuracy: 60.713382241335694\n",
      "Train -  Loss: 0.6551217436790466. Accuracy: 605072.975708502\n",
      "\n",
      "Iteration: 780. \n",
      "Test - Loss: 0.6615363955497742. Accuracy: 60.73867948393625\n",
      "Train -  Loss: 0.6547850370407104. Accuracy: 605031.983805668\n",
      "\n",
      "Iteration: 800. \n",
      "Test - Loss: 0.6612768769264221. Accuracy: 60.86516569693903\n",
      "Train -  Loss: 0.6544618010520935. Accuracy: 605089.3724696357\n",
      "\n",
      "Iteration: 820. \n",
      "Test - Loss: 0.6610300540924072. Accuracy: 60.86516569693903\n",
      "Train -  Loss: 0.6541527509689331. Accuracy: 605113.967611336\n",
      "\n",
      "Iteration: 840. \n",
      "Test - Loss: 0.6607964634895325. Accuracy: 60.89046293953959\n",
      "Train -  Loss: 0.6538588404655457. Accuracy: 605154.9595141701\n",
      "\n",
      "Iteration: 860. \n",
      "Test - Loss: 0.6605767011642456. Accuracy: 60.86516569693903\n",
      "Train -  Loss: 0.6535804867744446. Accuracy: 605269.7368421053\n",
      "\n",
      "Iteration: 880. \n",
      "Test - Loss: 0.6603711247444153. Accuracy: 60.83986845433848\n",
      "Train -  Loss: 0.6533184051513672. Accuracy: 605269.7368421053\n",
      "\n",
      "Iteration: 900. \n",
      "Test - Loss: 0.6601797342300415. Accuracy: 60.86516569693903\n",
      "Train -  Loss: 0.6530723571777344. Accuracy: 605277.9352226721\n",
      "\n",
      "Iteration: 920. \n",
      "Test - Loss: 0.6600019931793213. Accuracy: 60.83986845433848\n",
      "Train -  Loss: 0.6528418064117432. Accuracy: 605294.3319838056\n",
      "\n",
      "Iteration: 940. \n",
      "Test - Loss: 0.6598368883132935. Accuracy: 60.81457121173792\n",
      "Train -  Loss: 0.6526253819465637. Accuracy: 605343.5222672065\n",
      "\n",
      "Iteration: 960. \n",
      "Test - Loss: 0.6596831679344177. Accuracy: 60.81457121173792\n",
      "Train -  Loss: 0.6524216532707214. Accuracy: 605318.9271255061\n",
      "\n",
      "Iteration: 980. \n",
      "Test - Loss: 0.6595392823219299. Accuracy: 60.78927396913736\n",
      "Train -  Loss: 0.6522285342216492. Accuracy: 605318.9271255061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_l_s = LinearClassifier([len(X_train_np[0]), 1])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(nn_l_s.parameters(), momentum=0.9, lr=2e-4)\n",
    "\n",
    "nn_l_s = train_nn(nn_l_s, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. \n",
      "Test - Loss: 18.18303680419922. Accuracy: 53.5542625853782\n",
      "Train -  Loss: 18.205808639526367. Accuracy: 640727.7327935223\n",
      "\n",
      "Iteration: 20. \n",
      "Test - Loss: 15.577570915222168. Accuracy: 53.2001011889704\n",
      "Train -  Loss: 15.548083305358887. Accuracy: 639989.8785425101\n",
      "\n",
      "Iteration: 40. \n",
      "Test - Loss: 12.787200927734375. Accuracy: 52.82064254996205\n",
      "Train -  Loss: 12.617386817932129. Accuracy: 638653.5425101215\n",
      "\n",
      "Iteration: 60. \n",
      "Test - Loss: 9.583593368530273. Accuracy: 51.8593473311409\n",
      "Train -  Loss: 9.513964653015137. Accuracy: 636103.8461538461\n",
      "\n",
      "Iteration: 80. \n",
      "Test - Loss: 7.467722415924072. Accuracy: 50.31621553250696\n",
      "Train -  Loss: 7.15646505355835. Accuracy: 629463.1578947369\n",
      "\n",
      "Iteration: 100. \n",
      "Test - Loss: 6.604668617248535. Accuracy: 48.570705793068555\n",
      "Train -  Loss: 6.024292945861816. Accuracy: 611254.5546558704\n",
      "\n",
      "Iteration: 120. \n",
      "Test - Loss: 6.225675582885742. Accuracy: 47.12876296483683\n",
      "Train -  Loss: 5.612283706665039. Accuracy: 596440.08097166\n",
      "\n",
      "Iteration: 140. \n",
      "Test - Loss: 5.481131553649902. Accuracy: 47.83708575765242\n",
      "Train -  Loss: 4.942904472351074. Accuracy: 594431.4777327935\n",
      "\n",
      "Iteration: 160. \n",
      "Test - Loss: 4.099774360656738. Accuracy: 47.7864912724513\n",
      "Train -  Loss: 3.8647425174713135. Accuracy: 597645.2429149798\n",
      "\n",
      "Iteration: 180. \n",
      "Test - Loss: 3.0834271907806396. Accuracy: 48.545408550468\n",
      "Train -  Loss: 2.985626697540283. Accuracy: 597989.5748987854\n",
      "\n",
      "Iteration: 200. \n",
      "Test - Loss: 2.0520288944244385. Accuracy: 48.77308373387301\n",
      "Train -  Loss: 1.9837580919265747. Accuracy: 598186.3360323886\n",
      "\n",
      "Iteration: 220. \n",
      "Test - Loss: 1.1169933080673218. Accuracy: 50.06324310650139\n",
      "Train -  Loss: 1.1301294565200806. Accuracy: 600580.2631578947\n",
      "\n",
      "Iteration: 240. \n",
      "Test - Loss: 0.9732797741889954. Accuracy: 50.771565899316975\n",
      "Train -  Loss: 0.9880412220954895. Accuracy: 591603.036437247\n",
      "\n",
      "Iteration: 260. \n",
      "Test - Loss: 0.9158159494400024. Accuracy: 50.67037692891475\n",
      "Train -  Loss: 0.9201699495315552. Accuracy: 596349.8987854251\n",
      "\n",
      "Iteration: 280. \n",
      "Test - Loss: 0.8833165168762207. Accuracy: 50.59448520111308\n",
      "Train -  Loss: 0.8884294629096985. Accuracy: 594234.7165991903\n",
      "\n",
      "Iteration: 300. \n",
      "Test - Loss: 0.8601794242858887. Accuracy: 51.0498355679231\n",
      "Train -  Loss: 0.8653359413146973. Accuracy: 595292.3076923077\n",
      "\n",
      "Iteration: 320. \n",
      "Test - Loss: 0.8409480452537537. Accuracy: 50.822160384518085\n",
      "Train -  Loss: 0.8393393158912659. Accuracy: 594997.1659919028\n",
      "\n",
      "Iteration: 340. \n",
      "Test - Loss: 0.8235394954681396. Accuracy: 51.22691626612699\n",
      "Train -  Loss: 0.8143425583839417. Accuracy: 595243.1174089069\n",
      "\n",
      "Iteration: 360. \n",
      "Test - Loss: 0.8075931072235107. Accuracy: 51.30280799392866\n",
      "Train -  Loss: 0.797374963760376. Accuracy: 595439.8785425101\n",
      "\n",
      "Iteration: 380. \n",
      "Test - Loss: 0.7934720516204834. Accuracy: 51.53048317733367\n",
      "Train -  Loss: 0.7825260758399963. Accuracy: 595923.5829959514\n",
      "\n",
      "Iteration: 400. \n",
      "Test - Loss: 0.7806721329689026. Accuracy: 51.8593473311409\n",
      "Train -  Loss: 0.7698944211006165. Accuracy: 596235.1214574899\n",
      "\n",
      "Iteration: 420. \n",
      "Test - Loss: 0.7688664793968201. Accuracy: 52.061725271945356\n",
      "Train -  Loss: 0.7585505843162537. Accuracy: 596677.8340080972\n",
      "\n",
      "Iteration: 440. \n",
      "Test - Loss: 0.7580471634864807. Accuracy: 52.11231975714647\n",
      "Train -  Loss: 0.7481947541236877. Accuracy: 597161.5384615385\n",
      "\n",
      "Iteration: 460. \n",
      "Test - Loss: 0.7481991052627563. Accuracy: 52.23880597014925\n",
      "Train -  Loss: 0.7387690544128418. Accuracy: 597743.6234817813\n",
      "\n",
      "Iteration: 480. \n",
      "Test - Loss: 0.7392706871032715. Accuracy: 52.441183910953704\n",
      "Train -  Loss: 0.730216383934021. Accuracy: 598342.1052631579\n",
      "\n",
      "Iteration: 500. \n",
      "Test - Loss: 0.7311944961547852. Accuracy: 52.694156336959274\n",
      "Train -  Loss: 0.7224703431129456. Accuracy: 598842.2064777327\n",
      "\n",
      "Iteration: 520. \n",
      "Test - Loss: 0.7238967418670654. Accuracy: 53.023020490766505\n",
      "Train -  Loss: 0.7154607176780701. Accuracy: 599580.060728745\n",
      "\n",
      "Iteration: 540. \n",
      "Test - Loss: 0.7173024415969849. Accuracy: 53.45307361497597\n",
      "Train -  Loss: 0.7091163396835327. Accuracy: 600317.914979757\n",
      "\n",
      "Iteration: 560. \n",
      "Test - Loss: 0.7113397717475891. Accuracy: 53.73134328358209\n",
      "Train -  Loss: 0.7033684849739075. Accuracy: 600973.7854251012\n",
      "\n",
      "Iteration: 580. \n",
      "Test - Loss: 0.705940842628479. Accuracy: 54.11080192259044\n",
      "Train -  Loss: 0.6981533169746399. Accuracy: 601703.4412955466\n",
      "\n",
      "Iteration: 600. \n",
      "Test - Loss: 0.7010443806648254. Accuracy: 54.51555780419934\n",
      "Train -  Loss: 0.6934122443199158. Accuracy: 602416.7004048583\n",
      "\n",
      "Iteration: 620. \n",
      "Test - Loss: 0.6965949535369873. Accuracy: 55.04679989881103\n",
      "Train -  Loss: 0.6890933513641357. Accuracy: 603236.5384615385\n",
      "\n",
      "Iteration: 640. \n",
      "Test - Loss: 0.6925442814826965. Accuracy: 55.32506956741715\n",
      "Train -  Loss: 0.6851510405540466. Accuracy: 603884.2105263158\n",
      "\n",
      "Iteration: 660. \n",
      "Test - Loss: 0.6888497471809387. Accuracy: 55.881608904629395\n",
      "Train -  Loss: 0.6815457940101624. Accuracy: 604548.2793522268\n",
      "\n",
      "Iteration: 680. \n",
      "Test - Loss: 0.6854751706123352. Accuracy: 56.05868960283329\n",
      "Train -  Loss: 0.6782434582710266. Accuracy: 605081.1740890688\n",
      "\n",
      "Iteration: 700. \n",
      "Test - Loss: 0.6823890209197998. Accuracy: 56.38755375664053\n",
      "Train -  Loss: 0.6752147674560547. Accuracy: 605359.91902834\n",
      "\n",
      "Iteration: 720. \n",
      "Test - Loss: 0.6795640587806702. Accuracy: 56.79230963824943\n",
      "Train -  Loss: 0.672434389591217. Accuracy: 605753.4412955466\n",
      "\n",
      "Iteration: 740. \n",
      "Test - Loss: 0.6769765615463257. Accuracy: 57.475335188464456\n",
      "Train -  Loss: 0.6698802709579468. Accuracy: 606114.1700404858\n",
      "\n",
      "Iteration: 760. \n",
      "Test - Loss: 0.674606204032898. Accuracy: 57.90538831267392\n",
      "Train -  Loss: 0.6675333976745605. Accuracy: 606351.9230769231\n",
      "\n",
      "Iteration: 780. \n",
      "Test - Loss: 0.6724344491958618. Accuracy: 58.082469010877816\n",
      "Train -  Loss: 0.6653766632080078. Accuracy: 606614.2712550607\n",
      "\n",
      "Iteration: 800. \n",
      "Test - Loss: 0.6704455018043518. Accuracy: 58.28484695168227\n",
      "Train -  Loss: 0.6633949875831604. Accuracy: 606606.0728744939\n",
      "\n",
      "Iteration: 820. \n",
      "Test - Loss: 0.668624758720398. Accuracy: 58.58841386288894\n",
      "Train -  Loss: 0.6615747213363647. Accuracy: 606827.4291497975\n",
      "\n",
      "Iteration: 840. \n",
      "Test - Loss: 0.6669589877128601. Accuracy: 59.17025044270174\n",
      "Train -  Loss: 0.6599034667015076. Accuracy: 606827.4291497975\n",
      "\n",
      "Iteration: 860. \n",
      "Test - Loss: 0.665436327457428. Accuracy: 59.625600809511766\n",
      "Train -  Loss: 0.6583696603775024. Accuracy: 606737.2469635628\n",
      "\n",
      "Iteration: 880. \n",
      "Test - Loss: 0.6640455722808838. Accuracy: 59.954464963319\n",
      "Train -  Loss: 0.6569628715515137. Accuracy: 606597.8744939271\n",
      "\n",
      "Iteration: 900. \n",
      "Test - Loss: 0.6627766489982605. Accuracy: 60.18214014672401\n",
      "Train -  Loss: 0.6556732654571533. Accuracy: 606573.2793522268\n",
      "\n",
      "Iteration: 920. \n",
      "Test - Loss: 0.6616198420524597. Accuracy: 60.43511257272957\n",
      "Train -  Loss: 0.6544917821884155. Accuracy: 606647.0647773279\n",
      "\n",
      "Iteration: 940. \n",
      "Test - Loss: 0.6605668067932129. Accuracy: 60.51100430053124\n",
      "Train -  Loss: 0.6534099578857422. Accuracy: 606581.4777327935\n",
      "\n",
      "Iteration: 960. \n",
      "Test - Loss: 0.6596091389656067. Accuracy: 60.58689602833291\n",
      "Train -  Loss: 0.6524198651313782. Accuracy: 606573.2793522268\n",
      "\n",
      "Iteration: 980. \n",
      "Test - Loss: 0.6587390899658203. Accuracy: 60.81457121173792\n",
      "Train -  Loss: 0.6515141725540161. Accuracy: 606532.2874493927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_l_adam = LinearClassifier([len(X_train_np[0]), 1])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_l_adam.parameters(), lr=0.001)\n",
    "\n",
    "nn_l_adam = train_nn(nn_l_adam, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. \n",
      "Test - Loss: 10.508774757385254. Accuracy: 45.88919807740956\n",
      "Train -  Loss: 13.149428367614746. Accuracy: 544568.9271255061\n",
      "\n",
      "Iteration: 20. \n",
      "Test - Loss: 1.6060242652893066. Accuracy: 48.41892233746522\n",
      "Train -  Loss: 1.7154561281204224. Accuracy: 626634.7165991903\n",
      "\n",
      "Iteration: 40. \n",
      "Test - Loss: 1.2922370433807373. Accuracy: 47.50822160384518\n",
      "Train -  Loss: 1.2886101007461548. Accuracy: 613328.7449392712\n",
      "\n",
      "Iteration: 60. \n",
      "Test - Loss: 0.9924436807632446. Accuracy: 46.77460156842904\n",
      "Train -  Loss: 0.9961879849433899. Accuracy: 609024.5951417004\n",
      "\n",
      "Iteration: 80. \n",
      "Test - Loss: 0.7921686768531799. Accuracy: 47.280546420440174\n",
      "Train -  Loss: 0.796733021736145. Accuracy: 609696.8623481782\n",
      "\n",
      "Iteration: 100. \n",
      "Test - Loss: 0.7049551010131836. Accuracy: 53.57955982797875\n",
      "Train -  Loss: 0.7025899887084961. Accuracy: 620231.7813765182\n",
      "\n",
      "Iteration: 120. \n",
      "Test - Loss: 0.6815851926803589. Accuracy: 56.539337212243865\n",
      "Train -  Loss: 0.6749976873397827. Accuracy: 616288.3603238866\n",
      "\n",
      "Iteration: 140. \n",
      "Test - Loss: 0.6752434372901917. Accuracy: 58.68960283329117\n",
      "Train -  Loss: 0.6673697829246521. Accuracy: 609541.0931174089\n",
      "\n",
      "Iteration: 160. \n",
      "Test - Loss: 0.6708995699882507. Accuracy: 59.069061472299516\n",
      "Train -  Loss: 0.6630652546882629. Accuracy: 607827.6315789474\n",
      "\n",
      "Iteration: 180. \n",
      "Test - Loss: 0.6671050190925598. Accuracy: 59.069061472299516\n",
      "Train -  Loss: 0.6595423817634583. Accuracy: 607704.6558704453\n",
      "\n",
      "Iteration: 200. \n",
      "Test - Loss: 0.663925290107727. Accuracy: 59.676195294712876\n",
      "Train -  Loss: 0.656578540802002. Accuracy: 607885.020242915\n",
      "\n",
      "Iteration: 220. \n",
      "Test - Loss: 0.6613315939903259. Accuracy: 60.51100430053124\n",
      "Train -  Loss: 0.6540917158126831. Accuracy: 607737.4493927126\n",
      "\n",
      "Iteration: 240. \n",
      "Test - Loss: 0.6592303514480591. Accuracy: 60.83986845433848\n",
      "Train -  Loss: 0.6520086526870728. Accuracy: 607401.3157894737\n",
      "\n",
      "Iteration: 260. \n",
      "Test - Loss: 0.6575318574905396. Accuracy: 61.14343536554516\n",
      "Train -  Loss: 0.6502697467803955. Accuracy: 607311.1336032388\n",
      "\n",
      "Iteration: 280. \n",
      "Test - Loss: 0.6561650633811951. Accuracy: 61.16873260814571\n",
      "Train -  Loss: 0.648824155330658. Accuracy: 607073.3805668016\n",
      "\n",
      "Iteration: 300. \n",
      "Test - Loss: 0.6550722122192383. Accuracy: 61.54819124715406\n",
      "Train -  Loss: 0.6476272344589233. Accuracy: 607007.7935222673\n",
      "\n",
      "Iteration: 320. \n",
      "Test - Loss: 0.6542057394981384. Accuracy: 61.82646091576018\n",
      "Train -  Loss: 0.6466390490531921. Accuracy: 606942.2064777327\n",
      "\n",
      "Iteration: 340. \n",
      "Test - Loss: 0.6535241603851318. Accuracy: 62.05413609916519\n",
      "Train -  Loss: 0.6458243727684021. Accuracy: 607007.7935222673\n",
      "\n",
      "Iteration: 360. \n",
      "Test - Loss: 0.6529929041862488. Accuracy: 62.05413609916519\n",
      "Train -  Loss: 0.6451525688171387. Accuracy: 606745.4453441296\n",
      "\n",
      "Iteration: 380. \n",
      "Test - Loss: 0.6525824666023254. Accuracy: 62.02883885656463\n",
      "Train -  Loss: 0.6445974111557007. Accuracy: 606556.8825910931\n",
      "\n",
      "Iteration: 400. \n",
      "Test - Loss: 0.652268648147583. Accuracy: 62.13002782696686\n",
      "Train -  Loss: 0.6441371440887451. Accuracy: 606384.7165991903\n",
      "\n",
      "Iteration: 420. \n",
      "Test - Loss: 0.6520313024520874. Accuracy: 62.1047305843663\n",
      "Train -  Loss: 0.6437534689903259. Accuracy: 606245.3441295547\n",
      "\n",
      "Iteration: 440. \n",
      "Test - Loss: 0.6518541574478149. Accuracy: 62.02883885656463\n",
      "Train -  Loss: 0.6434316039085388. Accuracy: 606228.947368421\n",
      "\n",
      "Iteration: 460. \n",
      "Test - Loss: 0.6517241597175598. Accuracy: 62.13002782696686\n",
      "Train -  Loss: 0.6431597471237183. Accuracy: 606073.1781376519\n",
      "\n",
      "Iteration: 480. \n",
      "Test - Loss: 0.6516305208206177. Accuracy: 62.307108525170754\n",
      "Train -  Loss: 0.6429282426834106. Accuracy: 605958.4008097165\n",
      "\n",
      "Iteration: 500. \n",
      "Test - Loss: 0.6515651941299438. Accuracy: 62.231216797369086\n",
      "Train -  Loss: 0.6427294611930847. Accuracy: 605827.2267206478\n",
      "\n",
      "Iteration: 520. \n",
      "Test - Loss: 0.6515212655067444. Accuracy: 62.256514039969645\n",
      "Train -  Loss: 0.6425575613975525. Accuracy: 605720.6477732793\n",
      "\n",
      "Iteration: 540. \n",
      "Test - Loss: 0.6514936685562134. Accuracy: 62.231216797369086\n",
      "Train -  Loss: 0.6424077153205872. Accuracy: 605622.2672064777\n",
      "\n",
      "Iteration: 560. \n",
      "Test - Loss: 0.6514784693717957. Accuracy: 62.13002782696686\n",
      "Train -  Loss: 0.6422761082649231. Accuracy: 605515.6882591093\n",
      "\n",
      "Iteration: 580. \n",
      "Test - Loss: 0.6514723896980286. Accuracy: 62.02883885656463\n",
      "Train -  Loss: 0.6421598792076111. Accuracy: 605450.1012145749\n",
      "\n",
      "Iteration: 600. \n",
      "Test - Loss: 0.6514732837677002. Accuracy: 61.952947128762965\n",
      "Train -  Loss: 0.6420565843582153. Accuracy: 605376.3157894737\n",
      "\n",
      "Iteration: 620. \n",
      "Test - Loss: 0.651479184627533. Accuracy: 61.902352643561855\n",
      "Train -  Loss: 0.6419643759727478. Accuracy: 605359.91902834\n",
      "\n",
      "Iteration: 640. \n",
      "Test - Loss: 0.6514886617660522. Accuracy: 62.02883885656463\n",
      "Train -  Loss: 0.6418817639350891. Accuracy: 605294.3319838056\n",
      "\n",
      "Iteration: 660. \n",
      "Test - Loss: 0.6515007019042969. Accuracy: 62.2818112825702\n",
      "Train -  Loss: 0.6418073177337646. Accuracy: 605154.9595141701\n",
      "\n",
      "Iteration: 680. \n",
      "Test - Loss: 0.6515146493911743. Accuracy: 62.307108525170754\n",
      "Train -  Loss: 0.6417402029037476. Accuracy: 605097.5708502025\n",
      "\n",
      "Iteration: 700. \n",
      "Test - Loss: 0.6515299081802368. Accuracy: 62.33240576777131\n",
      "Train -  Loss: 0.6416793465614319. Accuracy: 605064.7773279352\n",
      "\n",
      "Iteration: 720. \n",
      "Test - Loss: 0.6515459418296814. Accuracy: 62.307108525170754\n",
      "Train -  Loss: 0.6416241526603699. Accuracy: 605089.3724696357\n",
      "\n",
      "Iteration: 740. \n",
      "Test - Loss: 0.651562511920929. Accuracy: 62.20591955476853\n",
      "Train -  Loss: 0.6415739059448242. Accuracy: 605056.5789473684\n",
      "\n",
      "Iteration: 760. \n",
      "Test - Loss: 0.6515793800354004. Accuracy: 62.13002782696686\n",
      "Train -  Loss: 0.6415281891822815. Accuracy: 605040.1821862349\n",
      "\n",
      "Iteration: 780. \n",
      "Test - Loss: 0.6515964269638062. Accuracy: 62.15532506956742\n",
      "Train -  Loss: 0.6414864659309387. Accuracy: 604974.5951417004\n",
      "\n",
      "Iteration: 800. \n",
      "Test - Loss: 0.6516135931015015. Accuracy: 62.13002782696686\n",
      "Train -  Loss: 0.6414483785629272. Accuracy: 604909.008097166\n",
      "\n",
      "Iteration: 820. \n",
      "Test - Loss: 0.6516306400299072. Accuracy: 62.07943334176575\n",
      "Train -  Loss: 0.6414135098457336. Accuracy: 604925.4048582996\n",
      "\n",
      "Iteration: 840. \n",
      "Test - Loss: 0.651647686958313. Accuracy: 62.02883885656463\n",
      "Train -  Loss: 0.641381561756134. Accuracy: 604884.4129554656\n",
      "\n",
      "Iteration: 860. \n",
      "Test - Loss: 0.6516644954681396. Accuracy: 62.13002782696686\n",
      "Train -  Loss: 0.6413521766662598. Accuracy: 604835.2226720648\n",
      "\n",
      "Iteration: 880. \n",
      "Test - Loss: 0.6516812443733215. Accuracy: 62.13002782696686\n",
      "Train -  Loss: 0.6413252353668213. Accuracy: 604884.4129554656\n",
      "\n",
      "Iteration: 900. \n",
      "Test - Loss: 0.6516978144645691. Accuracy: 62.15532506956742\n",
      "Train -  Loss: 0.6413004398345947. Accuracy: 604851.6194331984\n",
      "\n",
      "Iteration: 920. \n",
      "Test - Loss: 0.6517140865325928. Accuracy: 62.15532506956742\n",
      "Train -  Loss: 0.6412776708602905. Accuracy: 604835.2226720648\n",
      "\n",
      "Iteration: 940. \n",
      "Test - Loss: 0.6517301797866821. Accuracy: 62.180622312167976\n",
      "Train -  Loss: 0.6412566304206848. Accuracy: 604868.016194332\n",
      "\n",
      "Iteration: 960. \n",
      "Test - Loss: 0.6517460942268372. Accuracy: 62.15532506956742\n",
      "Train -  Loss: 0.6412371397018433. Accuracy: 604851.6194331984\n",
      "\n",
      "Iteration: 980. \n",
      "Test - Loss: 0.6517617702484131. Accuracy: 62.15532506956742\n",
      "Train -  Loss: 0.6412191987037659. Accuracy: 604802.4291497975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_l_multi = LinearClassifier([len(X_train_np[0]), 30, 1])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_l_multi.parameters(), lr=0.001)\n",
    "\n",
    "nn_l_multi = train_nn(nn_l_multi, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. \n",
      "Test - Loss: 4.5518479347229. Accuracy: 46.34454844421958\n",
      "Train -  Loss: 4.74475622177124. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 20. \n",
      "Test - Loss: 0.8826178312301636. Accuracy: 58.133063496078925\n",
      "Train -  Loss: 0.8271356821060181. Accuracy: 624675.3036437246\n",
      "\n",
      "Iteration: 40. \n",
      "Test - Loss: 0.7352921366691589. Accuracy: 54.945610928408804\n",
      "Train -  Loss: 0.7206556797027588. Accuracy: 584208.0971659919\n",
      "\n",
      "Iteration: 60. \n",
      "Test - Loss: 0.7082777619361877. Accuracy: 57.32355173286112\n",
      "Train -  Loss: 0.6907952427864075. Accuracy: 598522.4696356276\n",
      "\n",
      "Iteration: 80. \n",
      "Test - Loss: 0.6982366442680359. Accuracy: 57.4500379458639\n",
      "Train -  Loss: 0.6831768155097961. Accuracy: 597678.036437247\n",
      "\n",
      "Iteration: 100. \n",
      "Test - Loss: 0.6910252571105957. Accuracy: 57.90538831267392\n",
      "Train -  Loss: 0.6776135563850403. Accuracy: 600129.3522267207\n",
      "\n",
      "Iteration: 120. \n",
      "Test - Loss: 0.6865088939666748. Accuracy: 58.158360738679484\n",
      "Train -  Loss: 0.6732087731361389. Accuracy: 599768.6234817813\n",
      "\n",
      "Iteration: 140. \n",
      "Test - Loss: 0.6824054718017578. Accuracy: 58.436630407285605\n",
      "Train -  Loss: 0.6696746945381165. Accuracy: 600153.947368421\n",
      "\n",
      "Iteration: 160. \n",
      "Test - Loss: 0.678574800491333. Accuracy: 59.01846698709841\n",
      "Train -  Loss: 0.6663296222686768. Accuracy: 600842.6113360324\n",
      "\n",
      "Iteration: 180. \n",
      "Test - Loss: 0.6747574806213379. Accuracy: 60.106248418922334\n",
      "Train -  Loss: 0.6631446480751038. Accuracy: 603777.6315789474\n",
      "\n",
      "Iteration: 200. \n",
      "Test - Loss: 0.6729236841201782. Accuracy: 59.65089805211232\n",
      "Train -  Loss: 0.6606124043464661. Accuracy: 605769.8380566802\n",
      "\n",
      "Iteration: 220. \n",
      "Test - Loss: 0.6701142191886902. Accuracy: 59.979762205919556\n",
      "Train -  Loss: 0.65787672996521. Accuracy: 608549.0890688259\n",
      "\n",
      "Iteration: 240. \n",
      "Test - Loss: 0.6672148704528809. Accuracy: 59.726789779913986\n",
      "Train -  Loss: 0.6544548869132996. Accuracy: 608319.5344129555\n",
      "\n",
      "Iteration: 260. \n",
      "Test - Loss: 0.6639822125434875. Accuracy: 60.30862635972679\n",
      "Train -  Loss: 0.6505188941955566. Accuracy: 607688.2591093117\n",
      "\n",
      "Iteration: 280. \n",
      "Test - Loss: 0.6616728901863098. Accuracy: 60.283329117126236\n",
      "Train -  Loss: 0.6479507684707642. Accuracy: 607384.91902834\n",
      "\n",
      "Iteration: 300. \n",
      "Test - Loss: 0.6599460244178772. Accuracy: 60.283329117126236\n",
      "Train -  Loss: 0.6459049582481384. Accuracy: 606868.4210526316\n",
      "\n",
      "Iteration: 320. \n",
      "Test - Loss: 0.6586150527000427. Accuracy: 60.86516569693903\n",
      "Train -  Loss: 0.6441704034805298. Accuracy: 606573.2793522268\n",
      "\n",
      "Iteration: 340. \n",
      "Test - Loss: 0.6575744152069092. Accuracy: 61.19402985074627\n",
      "Train -  Loss: 0.6427837014198303. Accuracy: 606466.7004048583\n",
      "\n",
      "Iteration: 360. \n",
      "Test - Loss: 0.6568989157676697. Accuracy: 61.54819124715406\n",
      "Train -  Loss: 0.6417338252067566. Accuracy: 606196.1538461539\n",
      "\n",
      "Iteration: 380. \n",
      "Test - Loss: 0.6564638614654541. Accuracy: 61.47229951935239\n",
      "Train -  Loss: 0.6409514546394348. Accuracy: 605843.6234817813\n",
      "\n",
      "Iteration: 400. \n",
      "Test - Loss: 0.6560704708099365. Accuracy: 61.44700227675183\n",
      "Train -  Loss: 0.6403346657752991. Accuracy: 605663.2591093117\n",
      "\n",
      "Iteration: 420. \n",
      "Test - Loss: 0.6557822227478027. Accuracy: 61.42170503415128\n",
      "Train -  Loss: 0.639818549156189. Accuracy: 605605.8704453441\n",
      "\n",
      "Iteration: 440. \n",
      "Test - Loss: 0.6555336713790894. Accuracy: 61.21932709334683\n",
      "Train -  Loss: 0.6393739581108093. Accuracy: 605655.060728745\n",
      "\n",
      "Iteration: 460. \n",
      "Test - Loss: 0.6553462147712708. Accuracy: 61.16873260814571\n",
      "Train -  Loss: 0.6389780640602112. Accuracy: 605556.6801619433\n",
      "\n",
      "Iteration: 480. \n",
      "Test - Loss: 0.6552547216415405. Accuracy: 61.14343536554516\n",
      "Train -  Loss: 0.6386333107948303. Accuracy: 605269.7368421053\n",
      "\n",
      "Iteration: 500. \n",
      "Test - Loss: 0.6550961136817932. Accuracy: 60.991651909941815\n",
      "Train -  Loss: 0.6383218765258789. Accuracy: 605655.060728745\n",
      "\n",
      "Iteration: 520. \n",
      "Test - Loss: 0.6550790667533875. Accuracy: 61.016949152542374\n",
      "Train -  Loss: 0.638037383556366. Accuracy: 605310.7287449393\n",
      "\n",
      "Iteration: 540. \n",
      "Test - Loss: 0.6551476120948792. Accuracy: 61.320516063749054\n",
      "Train -  Loss: 0.6377424001693726. Accuracy: 605146.7611336033\n",
      "\n",
      "Iteration: 560. \n",
      "Test - Loss: 0.6552254557609558. Accuracy: 61.295218821148495\n",
      "Train -  Loss: 0.6373838186264038. Accuracy: 605466.4979757085\n",
      "\n",
      "Iteration: 580. \n",
      "Test - Loss: 0.655547022819519. Accuracy: 61.42170503415128\n",
      "Train -  Loss: 0.6371045112609863. Accuracy: 604982.7935222673\n",
      "\n",
      "Iteration: 600. \n",
      "Test - Loss: 0.6555224657058716. Accuracy: 61.21932709334683\n",
      "Train -  Loss: 0.6368253231048584. Accuracy: 605376.3157894737\n",
      "\n",
      "Iteration: 620. \n",
      "Test - Loss: 0.6555380821228027. Accuracy: 61.14343536554516\n",
      "Train -  Loss: 0.636591374874115. Accuracy: 605441.9028340081\n",
      "\n",
      "Iteration: 640. \n",
      "Test - Loss: 0.6556500792503357. Accuracy: 61.14343536554516\n",
      "Train -  Loss: 0.6363716125488281. Accuracy: 605515.6882591093\n",
      "\n",
      "Iteration: 660. \n",
      "Test - Loss: 0.6558347344398499. Accuracy: 61.06754363774348\n",
      "Train -  Loss: 0.6361472010612488. Accuracy: 605482.8947368421\n",
      "\n",
      "Iteration: 680. \n",
      "Test - Loss: 0.6560189127922058. Accuracy: 61.09284088034404\n",
      "Train -  Loss: 0.6359089612960815. Accuracy: 605179.5546558704\n",
      "\n",
      "Iteration: 700. \n",
      "Test - Loss: 0.656251847743988. Accuracy: 61.06754363774348\n",
      "Train -  Loss: 0.6356738209724426. Accuracy: 605548.4817813765\n",
      "\n",
      "Iteration: 720. \n",
      "Test - Loss: 0.6564303636550903. Accuracy: 60.96635466734126\n",
      "Train -  Loss: 0.6354671716690063. Accuracy: 604982.7935222673\n",
      "\n",
      "Iteration: 740. \n",
      "Test - Loss: 0.6566406488418579. Accuracy: 61.14343536554516\n",
      "Train -  Loss: 0.6352800726890564. Accuracy: 605425.5060728745\n",
      "\n",
      "Iteration: 760. \n",
      "Test - Loss: 0.6568132638931274. Accuracy: 61.09284088034404\n",
      "Train -  Loss: 0.6351024508476257. Accuracy: 605368.1174089069\n",
      "\n",
      "Iteration: 780. \n",
      "Test - Loss: 0.6569653153419495. Accuracy: 61.14343536554516\n",
      "Train -  Loss: 0.6349378824234009. Accuracy: 605359.91902834\n",
      "\n",
      "Iteration: 800. \n",
      "Test - Loss: 0.6570925712585449. Accuracy: 61.21932709334683\n",
      "Train -  Loss: 0.6347669363021851. Accuracy: 605154.9595141701\n",
      "\n",
      "Iteration: 820. \n",
      "Test - Loss: 0.6572659611701965. Accuracy: 61.24462433594738\n",
      "Train -  Loss: 0.6345659494400024. Accuracy: 605228.7449392712\n",
      "\n",
      "Iteration: 840. \n",
      "Test - Loss: 0.657374382019043. Accuracy: 61.09284088034404\n",
      "Train -  Loss: 0.6343902349472046. Accuracy: 604941.8016194332\n",
      "\n",
      "Iteration: 860. \n",
      "Test - Loss: 0.6574521064758301. Accuracy: 61.09284088034404\n",
      "Train -  Loss: 0.6342385411262512. Accuracy: 605753.4412955466\n",
      "\n",
      "Iteration: 880. \n",
      "Test - Loss: 0.657568097114563. Accuracy: 60.91576018214015\n",
      "Train -  Loss: 0.634080708026886. Accuracy: 605392.7125506073\n",
      "\n",
      "Iteration: 900. \n",
      "Test - Loss: 0.6577636003494263. Accuracy: 60.91576018214015\n",
      "Train -  Loss: 0.633926510810852. Accuracy: 605113.967611336\n",
      "\n",
      "Iteration: 920. \n",
      "Test - Loss: 0.6579249501228333. Accuracy: 60.991651909941815\n",
      "Train -  Loss: 0.6337677836418152. Accuracy: 605302.5303643724\n",
      "\n",
      "Iteration: 940. \n",
      "Test - Loss: 0.6580087542533875. Accuracy: 61.06754363774348\n",
      "Train -  Loss: 0.6336103677749634. Accuracy: 605343.5222672065\n",
      "\n",
      "Iteration: 960. \n",
      "Test - Loss: 0.658263087272644. Accuracy: 61.1181381229446\n",
      "Train -  Loss: 0.6334791779518127. Accuracy: 605507.4898785425\n",
      "\n",
      "Iteration: 980. \n",
      "Test - Loss: 0.658404529094696. Accuracy: 60.991651909941815\n",
      "Train -  Loss: 0.633342981338501. Accuracy: 605335.3238866397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_l_multi_act = LinearClassifier([len(X_train_np[0]), 30, 1], act=True)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_l_multi_act.parameters(), lr=0.001)\n",
    "\n",
    "nn_l_multi_act = train_nn(nn_l_multi_act, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bayesian Neural Network Classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "PI = 0.5\n",
    "SIGMA_1 = torch.FloatTensor([math.exp(-0)])\n",
    "SIGMA_2 = torch.FloatTensor([math.exp(-6)])\n",
    "\n",
    "\n",
    "class Gaussian(object):\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log(torch.exp(self.rho))\n",
    "\n",
    "    def sample(self):\n",
    "        epsilon = self.normal.sample(self.rho.size())\n",
    "        return self.mu + self.sigma * epsilon\n",
    "\n",
    "    def log_prob(self, input):\n",
    "        print(\"log prob\",input)\n",
    "        return (-math.log(math.sqrt(2 * math.pi))\n",
    "                - torch.log(self.sigma)\n",
    "                - ((input - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()\n",
    "\n",
    "\n",
    "class ScaleMixtureGaussian(object):\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.gaussian1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prob(self, input):\n",
    "        prob1 = torch.exp(self.gaussian1.log_prob(input))\n",
    "        prob2 = torch.exp(self.gaussian2.log_prob(input))\n",
    "        return (torch.log(self.pi * prob1 + (1 - self.pi) * prob2)).sum()\n",
    "\n",
    "\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        # Weight parameters\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(2e-1, 2e-1))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-2e-2, 0))\n",
    "        self.weight = Gaussian(self.weight_mu, self.weight_rho)\n",
    "        # Bias parameters\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(2e-1, 2e-1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-2e-2, 0))\n",
    "        self.bias = Gaussian(self.bias_mu, self.bias_rho)\n",
    "        # Prior distributions\n",
    "        self.weight_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
    "        self.bias_prior = ScaleMixtureGaussian(PI, SIGMA_1, SIGMA_2)\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, input, sample=False, calculate_log_probs=False):\n",
    "        if self.training or sample:\n",
    "            weight = self.weight.sample()\n",
    "            bias = self.bias.sample()\n",
    "        else:\n",
    "            weight = self.weight.mu\n",
    "            bias = self.bias.mu\n",
    "        if self.training or calculate_log_probs:\n",
    "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
    "            self.log_variational_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
    "        else:\n",
    "            self.log_prior, self.log_variational_posterior = 0, 0\n",
    "\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, dims, act=False):\n",
    "        super().__init__()\n",
    "        self.linears = torch.nn.ModuleList()\n",
    "        self.act = act\n",
    "        for idx in range(len(dims) - 1):\n",
    "            self.linears.append(BayesianLinear(dims[idx], dims[idx + 1]))\n",
    "\n",
    "    def forward(self, x, sample=False):\n",
    "        outputs = x\n",
    "        for ly in self.linears:\n",
    "            outputs = ly(outputs, sample)\n",
    "            if self.act:\n",
    "                outputs = F.relu(outputs)\n",
    "\n",
    "        return torch.sigmoid(outputs)\n",
    "\n",
    "    def log_prior(self):\n",
    "        su = 0\n",
    "        for ly in self.linears:\n",
    "            su += ly.log_prior\n",
    "        return su\n",
    "\n",
    "    def log_variational_posterior(self):\n",
    "        su = 0\n",
    "        for ly in self.linears:\n",
    "            su += ly.log_variational_posterior\n",
    "        return su\n",
    "\n",
    "    def convert(self, ts):\n",
    "        ts = np.array(ts).reshape(-1)\n",
    "        return torch.LongTensor(ts)\n",
    "\n",
    "    def sample_elbo(self, input, target, samples=1):\n",
    "        outputs = torch.zeros(samples, input.size()[0], 1)\n",
    "        for i in range(1):\n",
    "            outputs[i] = self(input, sample=True)\n",
    "        log_prior = self.log_prior()\n",
    "        log_variational_posterior = self.log_variational_posterior()\n",
    "        negative_log_likelihood = F.nll_loss(outputs.mean(0).reshape(-1), target.reshape(-1).type(torch.LongTensor),\n",
    "                                             size_average=True)\n",
    "        print(1,log_variational_posterior)\n",
    "\n",
    "        loss = (log_variational_posterior - log_prior) + negative_log_likelihood\n",
    "        return loss, log_prior, log_variational_posterior, negative_log_likelihood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "def train_bnn(nn_k, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000, sample=False):\n",
    "    nn_k.train()\n",
    "    losses = []\n",
    "    losses_test = []\n",
    "    Iterations = []\n",
    "    iter = -1\n",
    "    for epoch in range(epochs):\n",
    "        x = X_train_ts\n",
    "        labels = y_train_ts\n",
    "\n",
    "        nn_k.zero_grad()\n",
    "        loss, log_prior, log_variational_posterior, negative_log_likelihood = nn_k.sample_elbo(x, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs = nn_k(x.detach())\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                # Calculating the loss and accuracy for the test dataset\n",
    "                correct_test = 0\n",
    "                total_test = 0\n",
    "\n",
    "                loss_test, log_prior_test, log_variational_posterior_test, negative_log_likelihood_test = nn_k.sample_elbo(\n",
    "                    X_val_ts, y_val_ts)\n",
    "                outputs_test = nn_k(X_val_ts.detach())\n",
    "\n",
    "                predicted_test = outputs_test.round().detach().numpy()\n",
    "                total_test += y_val_ts.size(0)\n",
    "                correct_test += np.sum(predicted_test == y_val_ts.detach().numpy())\n",
    "                accuracy_test = 100 * correct_test / total_test\n",
    "                losses_test.append(loss_test.item())\n",
    "\n",
    "                # Calculating the loss and accuracy for the train dataset\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                total += y_train_ts.size(0)\n",
    "                correct += np.sum(torch.squeeze(outputs).round().detach().numpy() == y_train_ts.detach().numpy())\n",
    "                accuracy = 100 * correct / total\n",
    "                losses.append(loss.item())\n",
    "                Iterations.append(iter)\n",
    "\n",
    "                print(f\"Iteration: {iter}. \\nTest - Loss: {loss_test.item()}. Accuracy: {accuracy_test}\")\n",
    "                print(f\"Train -  Loss: {loss.item()}. Accuracy: {accuracy}\\n\")\n",
    "\n",
    "    return bnn_l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob tensor([[0.1862, 0.2025, 0.2002, 0.2192, 0.2059, 0.1935, 0.2082, 0.2107, 0.1981,\n",
      "         0.1987, 0.1953, 0.1989, 0.1960, 0.2093, 0.2195, 0.2120, 0.1945, 0.2022,\n",
      "         0.2070, 0.1992]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2009], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1923, 0.1722, 0.1826, 0.2179, 0.2265, 0.2253, 0.2218, 0.2046, 0.2045,\n",
      "         0.2187, 0.2236, 0.2147, 0.1850, 0.1840, 0.2163, 0.2130, 0.1870, 0.1777,\n",
      "         0.1973, 0.2167]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2118], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2132, 0.1671, 0.1387, 0.2046, 0.2418, 0.2036, 0.1892, 0.1959, 0.2352,\n",
      "         0.2109, 0.1987, 0.1863, 0.1362, 0.2067, 0.1874, 0.1977, 0.2096, 0.1855,\n",
      "         0.1836, 0.2256]])\n",
      "log prob tensor([0.2202])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[0.2263, 0.1478, 0.2162, 0.1741, 0.1480, 0.1928, 0.2149, 0.2131, 0.1987,\n",
      "         0.1989, 0.2157, 0.1522, 0.2253, 0.2208, 0.1896, 0.2407, 0.2006, 0.2071,\n",
      "         0.2058, 0.2316]])\n",
      "log prob tensor([0.2143])\n",
      "Iteration: 0. \n",
      "Test - Loss: nan. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "log prob tensor([[0.1999, 0.2172, 0.2293, 0.2221, 0.1607, 0.2000, 0.2099, 0.2105, 0.1994,\n",
      "         0.1894, 0.2110, 0.2161, 0.2254, 0.1586, 0.1871, 0.2154, 0.2255, 0.2257,\n",
      "         0.2120, 0.2079]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1952], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1805, 0.1409, 0.1807, 0.2127, 0.2231, 0.2083, 0.1683, 0.2105, 0.1474,\n",
      "         0.2079, 0.1363, 0.2222, 0.2007, 0.2053, 0.1403, 0.2084, 0.2043, 0.2281,\n",
      "         0.1872, 0.2001]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1833], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2178, 0.2175, 0.1860, 0.1331, 0.2127, 0.2410, 0.1960, 0.2043, 0.1924,\n",
      "         0.2039, 0.1984, 0.1874, 0.1831, 0.2151, 0.2321, 0.1977, 0.2267, 0.1913,\n",
      "         0.1816, 0.2124]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2260], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1903, 0.1975, 0.2627, 0.1661, 0.1767, 0.2109, 0.2104, 0.1788, 0.1653,\n",
      "         0.2449, 0.1969, 0.2133, 0.1761, 0.2553, 0.2157, 0.1844, 0.1993, 0.2373,\n",
      "         0.2441, 0.2287]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1889], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1758, 0.2201, 0.2040, 0.2142, 0.2172, 0.2344, 0.1599, 0.1829, 0.1369,\n",
      "         0.2559, 0.2135, 0.1621, 0.2098, 0.2500, 0.2145, 0.2133, 0.1929, 0.1950,\n",
      "         0.2419, 0.1713]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2256], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2251, 0.0797, 0.1834, 0.1851, 0.2243, 0.1760, 0.1659, 0.1848, 0.1781,\n",
      "         0.1741, 0.2241, 0.1701, 0.2002, 0.1853, 0.2036, 0.1770, 0.1906, 0.1732,\n",
      "         0.2425, 0.2427]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2485], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2018, 0.1301, 0.1756, 0.2299, 0.1720, 0.2177, 0.2669, 0.1703, 0.1396,\n",
      "         0.1862, 0.1922, 0.2061, 0.2028, 0.2132, 0.2021, 0.2279, 0.2425, 0.2124,\n",
      "         0.1669, 0.2572]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2648], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2520, 0.1269, 0.2111, 0.1867, 0.1979, 0.2530, 0.1602, 0.1706, 0.1662,\n",
      "         0.1352, 0.1935, 0.2533, 0.1056, 0.2491, 0.2088, 0.2236, 0.2345, 0.2298,\n",
      "         0.1599, 0.1318]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1904], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1976, 0.1754, 0.2115, 0.2439, 0.1945, 0.1673, 0.1936, 0.1481, 0.1646,\n",
      "         0.2605, 0.2026, 0.2245, 0.1615, 0.2080, 0.2227, 0.2290, 0.1759, 0.1939,\n",
      "         0.1755, 0.2264]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1977], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2582, 0.2122, 0.2650, 0.1579, 0.1926, 0.1109, 0.1869, 0.1931, 0.2217,\n",
      "         0.2029, 0.2147, 0.1980, 0.1702, 0.2732, 0.1970, 0.2027, 0.2194, 0.1250,\n",
      "         0.2060, 0.2183]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1846], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2356, 0.1965, 0.0964, 0.2344, 0.2967, 0.1995, 0.2364, 0.1720, 0.2536,\n",
      "         0.2061, 0.1979, 0.2275, 0.2243, 0.2446, 0.2295, 0.2571, 0.1980, 0.2174,\n",
      "         0.1409, 0.1728]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2513], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1627, 0.2028, 0.2334, 0.2260, 0.1968, 0.1699, 0.2619, 0.2007, 0.2330,\n",
      "         0.2416, 0.1837, 0.2093, 0.1285, 0.2185, 0.1684, 0.1585, 0.3088, 0.2179,\n",
      "         0.1990, 0.1308]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2310], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2205, 0.2095, 0.0851, 0.2362, 0.1925, 0.2503, 0.2089, 0.1680, 0.2105,\n",
      "         0.2583, 0.2721, 0.2509, 0.1706, 0.2167, 0.1562, 0.1496, 0.2542, 0.1362,\n",
      "         0.2322, 0.2512]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1455], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1956, 0.1739, 0.2306, 0.2220, 0.1722, 0.2311, 0.1789, 0.2326, 0.2561,\n",
      "         0.1280, 0.3288, 0.2051, 0.1917, 0.1391, 0.2000, 0.1325, 0.1305, 0.2037,\n",
      "         0.1380, 0.2040]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1610], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1834, 0.1207, 0.1798, 0.2410, 0.2612, 0.1987, 0.2388, 0.2614, 0.2566,\n",
      "         0.2060, 0.1752, 0.2590, 0.1194, 0.1509, 0.1503, 0.2016, 0.2744, 0.1843,\n",
      "         0.1861, 0.1908]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2053], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2766, 0.2354, 0.1524, 0.2087, 0.2951, 0.1988, 0.2100, 0.2015, 0.2223,\n",
      "         0.1971, 0.2047, 0.2085, 0.1838, 0.1117, 0.2586, 0.1379, 0.2011, 0.1798,\n",
      "         0.2039, 0.1917]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2901], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2025, 0.1951, 0.2315, 0.1948, 0.2645, 0.1737, 0.2140, 0.2274, 0.3331,\n",
      "         0.2303, 0.2400, 0.2220, 0.1893, 0.2125, 0.1396, 0.2190, 0.2200, 0.1679,\n",
      "         0.1772, 0.1138]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2559], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.0841, 0.2564, 0.2749, 0.2636, 0.1419, 0.1577, 0.1903, 0.2470, 0.1627,\n",
      "         0.2235, 0.2154, 0.2036, 0.0533, 0.2085, 0.2206, 0.2431, 0.1792, 0.2719,\n",
      "         0.1730, 0.1780]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2686], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2413, 0.1714, 0.2593, 0.2094, 0.2948, 0.1792, 0.2774, 0.2212, 0.0853,\n",
      "         0.1476, 0.2636, 0.2600, 0.3028, 0.2063, 0.1443, 0.2000, 0.2424, 0.2066,\n",
      "         0.1405, 0.2639]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2630], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1715, 0.2939, 0.1621, 0.2580, 0.1967, 0.0991, 0.2209, 0.1896, 0.1952,\n",
      "         0.2318, 0.1486, 0.2461, 0.2558, 0.1969, 0.1440, 0.1042, 0.1728, 0.2358,\n",
      "         0.2384, 0.0844]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1879], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2125, 0.1831, 0.1811, 0.1777, 0.1793, 0.1232, 0.1065, 0.2823, 0.2345,\n",
      "         0.1627, 0.1002, 0.1564, 0.2621, 0.2227, 0.1632, 0.3166, 0.1513, 0.1880,\n",
      "         0.3231, 0.3126]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1683], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2467, 0.2240, 0.2699, 0.2189, 0.1530, 0.1654, 0.2127, 0.2692, 0.2191,\n",
      "         0.2258, 0.2258, 0.2462, 0.3502, 0.1827, 0.2374, 0.1544, 0.1329, 0.1948,\n",
      "         0.1500, 0.2447]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1722], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1556, 0.1439, 0.1610, 0.2719, 0.3002, 0.1841, 0.2485, 0.2239, 0.2631,\n",
      "         0.1051, 0.2129, 0.2248, 0.2053, 0.2548, 0.1730, 0.2749, 0.1155, 0.1428,\n",
      "         0.2340, 0.1842]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3002], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.0667, 0.2752, 0.1461, 0.2004, 0.1847, 0.2491, 0.2302, 0.2112, 0.2264,\n",
      "         0.1950, 0.1816, 0.1356, 0.1767, 0.2376, 0.1777, 0.1602, 0.2112, 0.1188,\n",
      "         0.2567, 0.1517]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0247], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2708, 0.2708, 0.1047, 0.0617, 0.1553, 0.1817, 0.1470, 0.2396, 0.1106,\n",
      "         0.2071, 0.2383, 0.2755, 0.2102, 0.1709, 0.2028, 0.3091, 0.2795, 0.1959,\n",
      "         0.1284, 0.2795]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1984], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2229, 0.2379, 0.1611, 0.1013, 0.2395, 0.2233, 0.2121, 0.2014, 0.2475,\n",
      "         0.1623, 0.1556, 0.1871, 0.3046, 0.2227, 0.2146, 0.1831, 0.2522, 0.2443,\n",
      "         0.2011, 0.2407]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1598], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1555, 0.2279, 0.2115, 0.2326, 0.2070, 0.1654, 0.2413, 0.2652, 0.1728,\n",
      "         0.1789, 0.1535, 0.2722, 0.1683, 0.1943, 0.1251, 0.2387, 0.2172, 0.2045,\n",
      "         0.1269, 0.2407]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3243], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2654, 0.1967, 0.2569, 0.2179, 0.2154, 0.1741, 0.2195, 0.1609, 0.1511,\n",
      "         0.2327, 0.1781, 0.2249, 0.1255, 0.2486, 0.1982, 0.1832, 0.2277, 0.1469,\n",
      "         0.3012, 0.2607]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1559], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2998, 0.1880, 0.0649, 0.1539, 0.2041, 0.1842, 0.1852, 0.1895, 0.1706,\n",
      "         0.1451, 0.2101, 0.1158, 0.1115, 0.1657, 0.0301, 0.2833, 0.1992, 0.2882,\n",
      "         0.2189, 0.2039]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2403], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2156, 0.2275, 0.1078, 0.2318, 0.1824, 0.0995, 0.0750, 0.1837, 0.1408,\n",
      "         0.1480, 0.2380, 0.2057, 0.1824, 0.2583, 0.2855, 0.2080, 0.1711, 0.1668,\n",
      "         0.1294, 0.2325]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1030], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3555, 0.3867, 0.2274, 0.1774, 0.1275, 0.2097, 0.3482, 0.1839, 0.1876,\n",
      "         0.2351, 0.1734, 0.1075, 0.2733, 0.2728, 0.1405, 0.3067, 0.2585, 0.2360,\n",
      "         0.2072, 0.2667]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1809], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3080, 0.1880, 0.1546, 0.2655, 0.0229, 0.1583, 0.2260, 0.2739, 0.1494,\n",
      "         0.1795, 0.1105, 0.2382, 0.1998, 0.2472, 0.2937, 0.2491, 0.2691, 0.0757,\n",
      "         0.2290, 0.3620]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0559], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2499, 0.2757, 0.1670, 0.0958, 0.1921, 0.2274, 0.2520, 0.2105, 0.1584,\n",
      "         0.2963, 0.1093, 0.3753, 0.1278, 0.1834, 0.1349, 0.2093, 0.2803, 0.1916,\n",
      "         0.1572, 0.1705]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2373], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyu/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob tensor([[0.2582, 0.0593, 0.0806, 0.1442, 0.3125, 0.2880, 0.1850, 0.2231, 0.1861,\n",
      "         0.1456, 0.2074, 0.0857, 0.1176, 0.1224, 0.1936, 0.1351, 0.2530, 0.2284,\n",
      "         0.1748, 0.0617]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2554], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1942, 0.1541, 0.2001, 0.2417, 0.1873, 0.0952, 0.2001, 0.1736, 0.1302,\n",
      "         0.3008, 0.1642, 0.1613, 0.2460, 0.2793, 0.0917, 0.2006, 0.1503, 0.2756,\n",
      "         0.2511, 0.1940]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1863], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1351, 0.1066, 0.1469, 0.1742, 0.2595, 0.1981, 0.1433, 0.2182, 0.1672,\n",
      "         0.2618, 0.1086, 0.1886, 0.2661, 0.1326, 0.2367, 0.2048, 0.1980, 0.1972,\n",
      "         0.2226, 0.2345]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2712], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1736, 0.1987, 0.1696, 0.1246, 0.1700, 0.2570, 0.1334, 0.2420, 0.1166,\n",
      "         0.1724, 0.1316, 0.2412, 0.2813, 0.2600, 0.2750, 0.2354, 0.1676, 0.1357,\n",
      "         0.1319, 0.2008]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2478], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2225, 0.1833, 0.3262, 0.2168, 0.1447, 0.0626, 0.1449, 0.1623, 0.2567,\n",
      "         0.2007, 0.0773, 0.2860, 0.1438, 0.1729, 0.2786, 0.2059, 0.2669, 0.1569,\n",
      "         0.2517, 0.2180]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1803], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1431, 0.3013, 0.2136, 0.2099, 0.0614, 0.2626, 0.2378, 0.1997, 0.1693,\n",
      "         0.1333, 0.2250, 0.2693, 0.1710, 0.1184, 0.1453, 0.2248, 0.2595, 0.2490,\n",
      "         0.0708, 0.0739]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2324], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2016, 0.1465, 0.3494, 0.2108, 0.1686, 0.1699, 0.1938, 0.2078, 0.1720,\n",
      "         0.1717, 0.2265, 0.2049, 0.1214, 0.2280, 0.1534, 0.3025, 0.2750, 0.1823,\n",
      "         0.2405, 0.0090]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3480], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2509, 0.2851, 0.2189, 0.2669, 0.1043, 0.2608, 0.2160, 0.2399, 0.1034,\n",
      "         0.2263, 0.1070, 0.1664, 0.2633, 0.2773, 0.2237, 0.0617, 0.2298, 0.1918,\n",
      "         0.2956, 0.2194]])\n",
      "log prob tensor([0.2425])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[0.1300, 0.1713, 0.2654, 0.2577, 0.1519, 0.0595, 0.1214, 0.1607, 0.1964,\n",
      "         0.2320, 0.2931, 0.1612, 0.1333, 0.1922, 0.2218, 0.2217, 0.2671, 0.1245,\n",
      "         0.1357, 0.1109]])\n",
      "log prob tensor([0.1865])\n",
      "Iteration: 20. \n",
      "Test - Loss: nan. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "log prob tensor([[0.1977, 0.2541, 0.3009, 0.1067, 0.1376, 0.2990, 0.3431, 0.2572, 0.3318,\n",
      "         0.2358, 0.1221, 0.2322, 0.1486, 0.1153, 0.1875, 0.1604, 0.0716, 0.1972,\n",
      "         0.1724, 0.1072]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1152], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1816, 0.1918, 0.1313, 0.1649, 0.2775, 0.1259, 0.2492, 0.2542, 0.1458,\n",
      "         0.1599, 0.2357, 0.1465, 0.2315, 0.1804, 0.1973, 0.2217, 0.1619, 0.1732,\n",
      "         0.0262, 0.1796]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.4097], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2469, 0.2678, 0.1309, 0.2550, 0.2187, 0.0652, 0.1766, 0.1860, 0.2119,\n",
      "         0.2642, 0.2600, 0.1494, 0.1920, 0.3410, 0.2106, 0.0907, 0.1953, 0.1905,\n",
      "         0.2669, 0.1828]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2024], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2161,  0.2527,  0.2307,  0.2409,  0.3284,  0.2016,  0.1906,  0.2086,\n",
      "          0.2582,  0.1316,  0.2327,  0.1385,  0.1717, -0.0164,  0.1405,  0.1455,\n",
      "          0.2302,  0.1018,  0.2928,  0.2672]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0338], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1204, 0.1398, 0.2706, 0.2836, 0.2020, 0.1436, 0.0678, 0.1089, 0.2673,\n",
      "         0.2968, 0.1261, 0.1948, 0.1574, 0.1364, 0.2736, 0.2776, 0.0442, 0.2331,\n",
      "         0.2830, 0.0867]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2136], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1975, 0.3151, 0.2416, 0.2328, 0.1046, 0.2303, 0.1954, 0.1474, 0.3052,\n",
      "         0.0971, 0.1042, 0.3242, 0.2173, 0.3294, 0.3530, 0.2917, 0.2910, 0.1683,\n",
      "         0.3088, 0.1967]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0696], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2319,  0.2209,  0.2784,  0.1493,  0.2129,  0.2054,  0.1086,  0.1274,\n",
      "          0.2917,  0.2373,  0.2057,  0.2555,  0.2830,  0.3592,  0.2669,  0.1856,\n",
      "          0.1929,  0.1558,  0.1519, -0.0239]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2515], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1802, 0.1502, 0.1736, 0.1798, 0.1833, 0.2230, 0.1611, 0.2638, 0.1310,\n",
      "         0.1496, 0.1800, 0.2006, 0.1158, 0.2169, 0.1878, 0.1218, 0.1358, 0.1516,\n",
      "         0.1839, 0.1481]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0922], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2335, 0.1526, 0.2672, 0.1742, 0.1684, 0.1843, 0.1549, 0.2396, 0.2371,\n",
      "         0.0640, 0.1929, 0.1885, 0.1830, 0.2291, 0.0565, 0.0857, 0.2501, 0.2267,\n",
      "         0.2207, 0.2135]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1897], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1535, 0.0749, 0.1788, 0.2229, 0.2617, 0.0481, 0.2559, 0.1348, 0.1764,\n",
      "         0.1928, 0.1803, 0.1575, 0.1873, 0.4286, 0.0847, 0.1989, 0.1862, 0.2300,\n",
      "         0.2155, 0.2034]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1894], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2636, 0.2562, 0.1935, 0.1085, 0.1866, 0.1823, 0.2448, 0.1817, 0.2955,\n",
      "         0.1707, 0.1919, 0.2257, 0.2159, 0.1361, 0.2330, 0.3415, 0.1883, 0.2570,\n",
      "         0.2064, 0.1901]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1916], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1950, 0.1251, 0.1251, 0.0897, 0.2083, 0.2939, 0.1716, 0.1617, 0.1424,\n",
      "         0.2469, 0.3925, 0.1772, 0.1276, 0.1186, 0.1428, 0.1846, 0.2390, 0.1541,\n",
      "         0.1520, 0.0204]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3000], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3546, 0.0358, 0.1140, 0.1259, 0.2823, 0.1770, 0.2846, 0.1799, 0.0691,\n",
      "         0.2715, 0.0697, 0.1648, 0.1793, 0.2649, 0.0849, 0.2301, 0.2571, 0.0776,\n",
      "         0.1959, 0.3362]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3547], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2033, 0.2358, 0.2042, 0.2008, 0.1685, 0.2309, 0.2315, 0.0595, 0.1335,\n",
      "         0.2148, 0.2729, 0.2793, 0.3352, 0.1426, 0.1410, 0.2381, 0.2467, 0.1391,\n",
      "         0.1750, 0.2100]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1902], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1843, 0.2359, 0.1980, 0.2165, 0.2094, 0.2090, 0.2315, 0.2151, 0.1810,\n",
      "         0.2047, 0.2547, 0.1266, 0.2205, 0.1853, 0.1397, 0.2240, 0.2094, 0.1645,\n",
      "         0.1595, 0.2393]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1328], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.0550, 0.2020, 0.1812, 0.3440, 0.1957, 0.1646, 0.2780, 0.1604, 0.2070,\n",
      "         0.2174, 0.0627, 0.1774, 0.1589, 0.2255, 0.2444, 0.1774, 0.1746, 0.2085,\n",
      "         0.2092, 0.2361]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2353], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1444, 0.2278, 0.2425, 0.0748, 0.2192, 0.3117, 0.1888, 0.0843, 0.2186,\n",
      "         0.2831, 0.2270, 0.1893, 0.0779, 0.1095, 0.1450, 0.2443, 0.2943, 0.2261,\n",
      "         0.1831, 0.2974]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1939], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1359, 0.3049, 0.2317, 0.1139, 0.2473, 0.2173, 0.1605, 0.1240, 0.2266,\n",
      "         0.2159, 0.2952, 0.2044, 0.2743, 0.1705, 0.2346, 0.2591, 0.1640, 0.1352,\n",
      "         0.2768, 0.1644]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2310], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2731, 0.1024, 0.2151, 0.4287, 0.1814, 0.1442, 0.2134, 0.0781, 0.2441,\n",
      "         0.2026, 0.2990, 0.1539, 0.1097, 0.0991, 0.1589, 0.2219, 0.1858, 0.1234,\n",
      "         0.1373, 0.2629]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.4333], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1307, 0.0331, 0.2937, 0.1826, 0.1054, 0.2386, 0.2564, 0.1443, 0.0712,\n",
      "         0.2728, 0.1591, 0.0594, 0.1716, 0.2475, 0.2057, 0.1739, 0.3594, 0.2163,\n",
      "         0.1583, 0.3558]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1763], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3431, 0.2889, 0.1917, 0.3189, 0.0674, 0.1911, 0.2335, 0.3164, 0.2504,\n",
      "         0.0568, 0.2513, 0.2542, 0.2546, 0.0201, 0.1759, 0.1711, 0.1464, 0.2820,\n",
      "         0.1426, 0.1163]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2280], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1157, 0.0548, 0.1894, 0.2958, 0.2157, 0.1390, 0.1522, 0.1468, 0.4487,\n",
      "         0.1754, 0.0709, 0.2447, 0.2743, 0.1074, 0.2367, 0.2450, 0.2963, 0.2002,\n",
      "         0.2488, 0.2938]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3275], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1410, 0.1614, 0.3704, 0.1804, 0.0891, 0.2247, 0.1231, 0.1317, 0.1648,\n",
      "         0.2004, 0.1921, 0.2575, 0.1965, 0.2086, 0.1442, 0.1997, 0.2749, 0.1668,\n",
      "         0.2629, 0.2840]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3627], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3121, 0.2353, 0.2034, 0.2955, 0.1431, 0.3061, 0.2612, 0.2633, 0.2131,\n",
      "         0.1378, 0.2621, 0.1928, 0.1836, 0.2228, 0.1535, 0.1865, 0.1718, 0.0991,\n",
      "         0.2483, 0.2167]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2705], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1515, 0.2432, 0.2164, 0.1823, 0.1571, 0.2955, 0.1083, 0.1966, 0.0895,\n",
      "         0.3409, 0.0595, 0.2312, 0.1412, 0.0771, 0.2225, 0.1484, 0.0985, 0.2017,\n",
      "         0.1467, 0.1333]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1977], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2257, 0.3268, 0.2983, 0.1819, 0.2704, 0.3143, 0.0953, 0.2741, 0.2254,\n",
      "         0.1439, 0.1519, 0.2376, 0.1002, 0.1669, 0.1521, 0.1592, 0.1554, 0.1221,\n",
      "         0.2290, 0.1754]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0742], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1481, 0.1342, 0.2168, 0.1925, 0.1491, 0.0998, 0.1416, 0.1960, 0.2413,\n",
      "         0.3360, 0.1563, 0.1295, 0.1898, 0.3731, 0.1824, 0.1733, 0.3818, 0.2248,\n",
      "         0.1264, 0.2570]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0802], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.0750, 0.2635, 0.2557, 0.3724, 0.1797, 0.3773, 0.1718, 0.2388, 0.1966,\n",
      "         0.0721, 0.1383, 0.3460, 0.0905, 0.3123, 0.1327, 0.3017, 0.3273, 0.1589,\n",
      "         0.2524, 0.2658]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2608], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1307, 0.2755, 0.3323, 0.2054, 0.1101, 0.1318, 0.1409, 0.0906, 0.2528,\n",
      "         0.2251, 0.3337, 0.2524, 0.1953, 0.1932, 0.2686, 0.1719, 0.3342, 0.2479,\n",
      "         0.1601, 0.1903]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2557], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2250, 0.2263, 0.1658, 0.2999, 0.1357, 0.2651, 0.2187, 0.1477, 0.3666,\n",
      "         0.2853, 0.3624, 0.1740, 0.0843, 0.2045, 0.2394, 0.4452, 0.3922, 0.2970,\n",
      "         0.2455, 0.0448]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1113], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2599, 0.2237, 0.2102, 0.1149, 0.2595, 0.2289, 0.3744, 0.1827, 0.2610,\n",
      "         0.3621, 0.2379, 0.1468, 0.2680, 0.1298, 0.3092, 0.4110, 0.2438, 0.1990,\n",
      "         0.2340, 0.2535]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2503], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3055, 0.1458, 0.3092, 0.1287, 0.1593, 0.2756, 0.1630, 0.1258, 0.0649,\n",
      "         0.2139, 0.1832, 0.2926, 0.2955, 0.1870, 0.1299, 0.2889, 0.1372, 0.2207,\n",
      "         0.1497, 0.2939]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2827], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3178, 0.1980, 0.1525, 0.1628, 0.3746, 0.1156, 0.1788, 0.1596, 0.1416,\n",
      "         0.1090, 0.0927, 0.2693, 0.3515, 0.1108, 0.2397, 0.2847, 0.2960, 0.3112,\n",
      "         0.1575, 0.3325]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3406], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2788, 0.2321, 0.3337, 0.1614, 0.2498, 0.0686, 0.1342, 0.2808, 0.2884,\n",
      "         0.2456, 0.2949, 0.1757, 0.0785, 0.2372, 0.1116, 0.3211, 0.2944, 0.2433,\n",
      "         0.2726, 0.3319]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2129], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1676, 0.2023, 0.2142, 0.3648, 0.0362, 0.2865, 0.1601, 0.2536, 0.1469,\n",
      "         0.1778, 0.2854, 0.1489, 0.2518, 0.1385, 0.2478, 0.2356, 0.2222, 0.1824,\n",
      "         0.1624, 0.1801]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2755], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3181, 0.1516, 0.2488, 0.1689, 0.1644, 0.0868, 0.1300, 0.3630, 0.2289,\n",
      "         0.1988, 0.3007, 0.3262, 0.0099, 0.1722, 0.2855, 0.0337, 0.1647, 0.1264,\n",
      "         0.3310, 0.3129]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2560], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1588, 0.2717, 0.0502, 0.2636, 0.1186, 0.2348, 0.1503, 0.1349, 0.3457,\n",
      "         0.1103, 0.1840, 0.2821, 0.2776, 0.0880, 0.3074, 0.4054, 0.1173, 0.2863,\n",
      "         0.1906, 0.2508]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1364], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.0478, 0.2481, 0.3048, 0.2013, 0.1342, 0.2887, 0.2335, 0.2558, 0.2329,\n",
      "         0.1353, 0.1219, 0.2422, 0.1315, 0.1190, 0.0973, 0.1538, 0.2410, 0.2220,\n",
      "         0.2171, 0.2255]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0823], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0357,  0.0689,  0.2541,  0.3042, -0.0214,  0.3169,  0.0514,  0.2712,\n",
      "          0.3624,  0.2163,  0.1334,  0.3196,  0.0650,  0.1774,  0.3119,  0.3993,\n",
      "          0.3483,  0.1334,  0.2638,  0.1723]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2492], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2388, 0.1066, 0.2863, 0.2513, 0.2294, 0.2506, 0.2532, 0.1845, 0.2284,\n",
      "         0.0757, 0.2018, 0.1111, 0.2974, 0.1487, 0.1777, 0.2048, 0.1105, 0.1546,\n",
      "         0.0894, 0.0863]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1962], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2655, 0.1399, 0.2540, 0.3196, 0.1616, 0.2671, 0.0383, 0.2036, 0.1123,\n",
      "         0.1291, 0.2383, 0.1144, 0.2499, 0.1616, 0.1216, 0.2272, 0.0952, 0.1708,\n",
      "         0.0563, 0.1395]])\n",
      "log prob tensor([0.1076])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[0.0608, 0.2533, 0.1266, 0.1680, 0.1316, 0.3098, 0.1776, 0.1873, 0.2092,\n",
      "         0.0656, 0.1954, 0.1288, 0.2845, 0.2362, 0.1982, 0.1485, 0.0781, 0.1449,\n",
      "         0.2439, 0.1749]])\n",
      "log prob tensor([0.1206])\n",
      "Iteration: 40. \n",
      "Test - Loss: nan. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "log prob tensor([[0.4119, 0.0927, 0.1841, 0.3006, 0.1559, 0.3091, 0.0267, 0.2623, 0.2843,\n",
      "         0.1228, 0.1051, 0.1228, 0.2098, 0.2128, 0.1546, 0.2200, 0.0328, 0.2208,\n",
      "         0.2501, 0.1856]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2746], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.4138, 0.4182, 0.1758, 0.1955, 0.1417, 0.1460, 0.1562, 0.2050, 0.3575,\n",
      "         0.1320, 0.3284, 0.1341, 0.0596, 0.2219, 0.2815, 0.1704, 0.2352, 0.2110,\n",
      "         0.2101, 0.2530]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1583], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2555, 0.2925, 0.2461, 0.3643, 0.1668, 0.0900, 0.3458, 0.2315, 0.0961,\n",
      "         0.2213, 0.1148, 0.3196, 0.0624, 0.1275, 0.1657, 0.2557, 0.1712, 0.2927,\n",
      "         0.2408, 0.1670]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1380], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3413, 0.3111, 0.2750, 0.4076, 0.0942, 0.2435, 0.2692, 0.2456, 0.1937,\n",
      "         0.1127, 0.2331, 0.3127, 0.3034, 0.2046, 0.1615, 0.2491, 0.3184, 0.1321,\n",
      "         0.3309, 0.0795]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2288], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1200,  0.2482,  0.0037,  0.1057,  0.2030,  0.1832,  0.0312,  0.2015,\n",
      "          0.0639, -0.0253,  0.1703,  0.1965,  0.3132,  0.1778,  0.2129,  0.1238,\n",
      "          0.2585,  0.2475,  0.2396,  0.2463]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0162], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2509,  0.1734,  0.4292,  0.1573,  0.3919,  0.1983,  0.1800,  0.1974,\n",
      "          0.1383,  0.1217, -0.0073,  0.3001,  0.0671,  0.1194,  0.1773,  0.0685,\n",
      "          0.1569,  0.2875,  0.1058,  0.3673]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1582], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1934, 0.3692, 0.1782, 0.2402, 0.3067, 0.2087, 0.2192, 0.2199, 0.0879,\n",
      "         0.2133, 0.1662, 0.3700, 0.3393, 0.2743, 0.2214, 0.0861, 0.0549, 0.1278,\n",
      "         0.2509, 0.2092]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2095], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2536,  0.3260, -0.0837,  0.1707,  0.2862,  0.1566, -0.0113,  0.0519,\n",
      "          0.4243,  0.1221,  0.2493,  0.1807,  0.3197,  0.1871,  0.2199,  0.2727,\n",
      "          0.1466,  0.1911,  0.1221,  0.0950]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2688], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1665, 0.2195, 0.2183, 0.1969, 0.3415, 0.3181, 0.2511, 0.1653, 0.2556,\n",
      "         0.3671, 0.1437, 0.3922, 0.0837, 0.3329, 0.2459, 0.0685, 0.2714, 0.2733,\n",
      "         0.1785, 0.2041]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3951], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2838,  0.4326,  0.0835,  0.2511,  0.2683,  0.0884,  0.0515,  0.3354,\n",
      "         -0.0532,  0.2380,  0.2121,  0.1973,  0.3111,  0.2959,  0.0598,  0.2656,\n",
      "          0.3760,  0.1508,  0.1428,  0.0128]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2097], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3447, 0.3149, 0.3206, 0.2741, 0.1200, 0.1889, 0.1919, 0.1695, 0.2446,\n",
      "         0.2439, 0.1950, 0.1786, 0.3250, 0.0660, 0.1227, 0.3137, 0.2686, 0.3683,\n",
      "         0.1165, 0.1176]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0792], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2254, 0.1186, 0.2959, 0.1069, 0.2035, 0.2598, 0.1900, 0.4204, 0.2020,\n",
      "         0.2445, 0.0594, 0.1555, 0.2747, 0.1291, 0.2244, 0.1692, 0.2765, 0.3918,\n",
      "         0.2691, 0.2819]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2324], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2305, 0.2570, 0.1826, 0.2239, 0.2298, 0.3536, 0.1220, 0.2911, 0.3207,\n",
      "         0.2574, 0.1856, 0.1726, 0.2946, 0.3287, 0.1879, 0.1984, 0.1910, 0.2481,\n",
      "         0.1750, 0.2193]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2973], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.0765, 0.1087, 0.0925, 0.1097, 0.2661, 0.2555, 0.1809, 0.1523, 0.1289,\n",
      "         0.3214, 0.1001, 0.1020, 0.0624, 0.3197, 0.2505, 0.2239, 0.2682, 0.0444,\n",
      "         0.1772, 0.3125]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2184], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1316, 0.2088, 0.4083, 0.2025, 0.2090, 0.1869, 0.3295, 0.1975, 0.2078,\n",
      "         0.1670, 0.1722, 0.1423, 0.2556, 0.2150, 0.1152, 0.1885, 0.0656, 0.1387,\n",
      "         0.2400, 0.1701]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2293], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2674,  0.1228, -0.0581,  0.3432,  0.2164,  0.3613,  0.1648,  0.2591,\n",
      "          0.2925,  0.1313,  0.1400,  0.0630,  0.1045,  0.2122,  0.1422,  0.1816,\n",
      "          0.1693,  0.1361,  0.1972,  0.3083]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2497], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1887,  0.2935, -0.0751,  0.2304,  0.3404,  0.2588,  0.0129,  0.1326,\n",
      "          0.4417,  0.1738,  0.2241,  0.1057,  0.2108,  0.1892,  0.2602,  0.1884,\n",
      "          0.1887,  0.3318,  0.1660,  0.2588]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2154], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0443,  0.0263,  0.1540,  0.1996,  0.3528,  0.0266,  0.3000,  0.1697,\n",
      "          0.2143,  0.2251,  0.1779,  0.1202,  0.3678,  0.3842,  0.3986,  0.3193,\n",
      "          0.2445,  0.1853,  0.3531,  0.2029]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2364], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.0664, 0.0283, 0.0186, 0.2415, 0.0295, 0.1742, 0.1732, 0.4212, 0.2605,\n",
      "         0.1699, 0.2628, 0.2318, 0.2461, 0.2959, 0.1674, 0.0739, 0.2499, 0.3842,\n",
      "         0.2064, 0.3207]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1279], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3684,  0.2555,  0.1485,  0.3005,  0.1674,  0.4593,  0.0545,  0.1853,\n",
      "          0.0632,  0.0320, -0.0880,  0.0794,  0.2743,  0.1986,  0.2083,  0.3068,\n",
      "          0.3229,  0.1837,  0.1242,  0.2273]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1667], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1665,  0.0931,  0.0310,  0.1098,  0.1408,  0.1335,  0.2853,  0.1576,\n",
      "          0.0763,  0.2125,  0.2250,  0.1925,  0.2131, -0.0317,  0.0654, -0.0659,\n",
      "          0.0056,  0.2018,  0.1171,  0.3496]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1770], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3719,  0.0099,  0.4301,  0.0658,  0.1910,  0.2387,  0.3739,  0.2345,\n",
      "          0.1984,  0.3270,  0.2248,  0.4152,  0.1860,  0.1170,  0.1863,  0.0399,\n",
      "         -0.2853,  0.0624,  0.1575,  0.1492]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1160], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3812,  0.3408,  0.1992,  0.3504,  0.1664,  0.3533,  0.2286, -0.0362,\n",
      "          0.2182,  0.3584,  0.2297,  0.2463,  0.3383,  0.0942,  0.2595,  0.1834,\n",
      "         -0.1309,  0.1255,  0.2020,  0.4894]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2546], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2196,  0.1763, -0.1278,  0.1012,  0.1722,  0.2576,  0.2643,  0.1662,\n",
      "          0.3228,  0.1059,  0.2177,  0.0805,  0.1886,  0.3327,  0.1000,  0.2891,\n",
      "         -0.0503,  0.2559,  0.0509,  0.2424]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1816], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.0195, 0.4086, 0.3165, 0.4689, 0.0961, 0.2514, 0.2958, 0.1916, 0.2383,\n",
      "         0.1428, 0.1395, 0.2532, 0.3436, 0.0967, 0.0449, 0.1701, 0.0883, 0.2298,\n",
      "         0.1542, 0.2893]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2499], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2273, -0.0187,  0.3093,  0.1368,  0.2035,  0.2980,  0.2447,  0.3716,\n",
      "          0.2525,  0.0936,  0.1916,  0.3491,  0.1432,  0.1981,  0.3923,  0.1203,\n",
      "          0.2399,  0.3236,  0.1458,  0.1412]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0778], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2860,  0.1550,  0.0828,  0.1046,  0.1755,  0.1682,  0.2805,  0.1721,\n",
      "          0.3482,  0.0219,  0.3561, -0.0269,  0.0861,  0.2880,  0.4450,  0.2675,\n",
      "          0.3061,  0.2647,  0.4042,  0.1908]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1131], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1311, 0.2557, 0.5613, 0.1047, 0.1291, 0.1077, 0.2951, 0.1363, 0.1787,\n",
      "         0.3078, 0.3042, 0.2090, 0.2637, 0.1501, 0.4843, 0.1394, 0.1694, 0.1466,\n",
      "         0.0309, 0.0984]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2096], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4934,  0.1868,  0.0271,  0.1185,  0.1906,  0.0875,  0.0264,  0.3343,\n",
      "          0.0472,  0.1565,  0.3020,  0.3104,  0.2047,  0.1086,  0.1725,  0.1619,\n",
      "         -0.5044,  0.0687,  0.3491,  0.0978]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2099], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1017,  0.1796,  0.0635,  0.1868,  0.1115,  0.0761,  0.0359,  0.1333,\n",
      "         -0.0172,  0.0628,  0.2370,  0.0684,  0.1818, -0.0170,  0.2524,  0.1006,\n",
      "          0.2032,  0.2680,  0.2817,  0.4121]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2829], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1588,  0.1476,  0.5624,  0.1809,  0.1369,  0.2731,  0.2645,  0.0124,\n",
      "          0.2375,  0.1634,  0.1853,  0.1830,  0.3550,  0.1961,  0.2839,  0.3034,\n",
      "         -0.0881,  0.2093,  0.3505,  0.1623]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0027], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1610,  0.2390, -0.0240,  0.1833,  0.1271,  0.1243, -0.0068,  0.2154,\n",
      "          0.3969,  0.2378,  0.2413,  0.0786,  0.3170,  0.3310,  0.3742,  0.0978,\n",
      "          0.0511,  0.1131,  0.0986,  0.0400]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2676], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1673, 0.0709, 0.3549, 0.2564, 0.1964, 0.0102, 0.3291, 0.1834, 0.1183,\n",
      "         0.2315, 0.2190, 0.1901, 0.3547, 0.1869, 0.1054, 0.1092, 0.7616, 0.3090,\n",
      "         0.3430, 0.2279]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2044], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1966, 0.2241, 0.4759, 0.2103, 0.1800, 0.2993, 0.1748, 0.4650, 0.3001,\n",
      "         0.1114, 0.0112, 0.1399, 0.2105, 0.2690, 0.0967, 0.1729, 0.3076, 0.1256,\n",
      "         0.2366, 0.3298]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1994], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3488,  0.3701,  0.3143,  0.3680,  0.1345,  0.1252,  0.2430,  0.1781,\n",
      "          0.3868,  0.2661,  0.3966,  0.3463,  0.3532,  0.1017,  0.2801,  0.3544,\n",
      "          0.0331,  0.1049, -0.1457,  0.2056]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2681], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1994, 0.1664, 0.1291, 0.1110, 0.1295, 0.4174, 0.1232, 0.2110, 0.1324,\n",
      "         0.2955, 0.1822, 0.2875, 0.1762, 0.1675, 0.3245, 0.3280, 0.6093, 0.0445,\n",
      "         0.2661, 0.3188]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2515], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2593,  0.3086, -0.0940,  0.0879,  0.1391,  0.1694,  0.1788,  0.1234,\n",
      "          0.2362,  0.0845,  0.3387,  0.3397,  0.1376,  0.0563,  0.0971,  0.2172,\n",
      "          0.1692,  0.1696,  0.1993,  0.0411]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2190], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0819,  0.1726, -0.0165,  0.1380,  0.2326,  0.4184,  0.1428,  0.3171,\n",
      "          0.2279,  0.1539, -0.0892,  0.1852,  0.3346,  0.2020,  0.1699,  0.3148,\n",
      "          0.2943,  0.1397,  0.3217,  0.3557]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2171], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2002,  0.1482, -0.0934,  0.2554,  0.1157,  0.3695,  0.2144,  0.2829,\n",
      "          0.0892,  0.2648,  0.1834,  0.0495,  0.2116,  0.3279,  0.1634, -0.0546,\n",
      "         -0.1765,  0.1832,  0.1906,  0.2983]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2550], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1638,  0.3420,  0.6012,  0.0355,  0.1298,  0.0050,  0.1878,  0.2324,\n",
      "          0.1719,  0.2596,  0.2793,  0.2591,  0.2822,  0.2166,  0.2059, -0.0127,\n",
      "          0.7461,  0.2294,  0.4050,  0.4435]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2369], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.1124,  0.0269,  0.1213,  0.2193,  0.2431,  0.2755,  0.2265, -0.0143,\n",
      "          0.0620,  0.2683,  0.1107,  0.1209,  0.1942,  0.2169,  0.0546,  0.3452,\n",
      "          0.1301,  0.0384,  0.1204,  0.3651]])\n",
      "log prob tensor([0.3220])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[ 0.2851,  0.2312,  0.3637,  0.2323,  0.3317,  0.2236,  0.1113,  0.0700,\n",
      "          0.2307,  0.3057,  0.1559,  0.3643,  0.1507,  0.2232,  0.3291,  0.1260,\n",
      "         -0.0441,  0.1652,  0.1959,  0.3119]])\n",
      "log prob tensor([0.1853])\n",
      "Iteration: 60. \n",
      "Test - Loss: nan. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "log prob tensor([[ 0.1606,  0.1084,  0.1684,  0.2261,  0.0296, -0.0745, -0.0017,  0.1675,\n",
      "          0.3260,  0.1629,  0.3018,  0.2426,  0.3015,  0.2011,  0.2040,  0.3065,\n",
      "          0.0088,  0.2130,  0.2091,  0.1860]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2143], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2957,  0.1546,  0.2536,  0.1248,  0.2309,  0.3532,  0.1552, -0.0646,\n",
      "          0.3360,  0.1975,  0.3973,  0.1742,  0.1867,  0.1944,  0.2376,  0.3331,\n",
      "          0.0670,  0.2666,  0.2215,  0.0218]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2011], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1433,  0.3258,  0.6054,  0.3019,  0.3692,  0.0191,  0.2404,  0.2963,\n",
      "          0.1987,  0.0461,  0.2156,  0.2527,  0.2021,  0.2308,  0.2123,  0.2561,\n",
      "          0.0948, -0.0852,  0.0709,  0.2723]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2812], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.2118, 0.0191, 0.0637, 0.1817, 0.3004, 0.0691, 0.3131, 0.2680, 0.0941,\n",
      "         0.1454, 0.0686, 0.1115, 0.2979, 0.1865, 0.1557, 0.3348, 0.1146, 0.1287,\n",
      "         0.3719, 0.3540]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2779], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1464,  0.1188, -0.2329,  0.3422,  0.1293,  0.1773,  0.2637,  0.0882,\n",
      "          0.1921,  0.3695,  0.1209,  0.2370,  0.2114,  0.2246,  0.3479,  0.2725,\n",
      "         -0.6653,  0.1990,  0.2346,  0.2922]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2265], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0788,  0.0258,  0.2372,  0.4244,  0.3838,  0.4616,  0.2222,  0.1362,\n",
      "          0.0896,  0.0923,  0.2605,  0.0972, -0.0993,  0.3365,  0.0869,  0.2481,\n",
      "          0.1806,  0.2857,  0.1102,  0.2965]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2576], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1542,  0.2494, -0.1639,  0.0629,  0.2264,  0.0865,  0.1725,  0.2574,\n",
      "          0.2775,  0.1070,  0.3493,  0.0891,  0.1832,  0.0809,  0.2378,  0.3159,\n",
      "         -0.3113,  0.1844,  0.0939,  0.3011]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3457], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2110,  0.0825,  0.2371,  0.2693,  0.2204,  0.2327,  0.2843,  0.3227,\n",
      "          0.0960,  0.1594, -0.0360,  0.3336,  0.1892,  0.1163,  0.2155,  0.3151,\n",
      "          0.1152,  0.1849,  0.2553,  0.2285]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2320], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.3996, 0.3658, 0.3265, 0.3103, 0.1996, 0.3224, 0.1456, 0.1949, 0.0447,\n",
      "         0.1634, 0.0673, 0.1750, 0.1614, 0.2898, 0.1616, 0.2554, 0.4233, 0.2658,\n",
      "         0.2151, 0.1897]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2062], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1076,  0.1555,  0.4366,  0.0541,  0.2760,  0.2076,  0.1993,  0.3688,\n",
      "          0.0227,  0.3431,  0.1751, -0.0062,  0.0845,  0.3053,  0.0845,  0.3376,\n",
      "         -0.2038,  0.3474,  0.0519,  0.0979]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2651], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2560,  0.0966, -0.0198,  0.1578,  0.0225,  0.2676,  0.2852,  0.2486,\n",
      "          0.0414,  0.2762,  0.1675, -0.0971,  0.2356,  0.3535,  0.0308,  0.0987,\n",
      "          0.2896,  0.4513,  0.1932, -0.0074]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1581], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1711,  0.1341,  0.1105,  0.1697,  0.4344,  0.2644,  0.2539,  0.4513,\n",
      "          0.3809,  0.0140,  0.1384,  0.1153,  0.2409,  0.2832,  0.3941,  0.0198,\n",
      "         -0.3331,  0.4051,  0.1756,  0.3514]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2659], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.1620,  0.2396, -0.1234,  0.2446,  0.1868,  0.2540,  0.1898,  0.3754,\n",
      "          0.0895,  0.0485,  0.0256,  0.0535,  0.1871,  0.2801, -0.0035,  0.1781,\n",
      "         -0.0330,  0.3676,  0.0684,  0.2919]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2179], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 6.6850e-02,  3.7240e-01,  4.6270e-02,  1.8089e-01,  1.8928e-01,\n",
      "         -3.4859e-02,  3.1927e-01,  3.5391e-01,  3.3478e-01,  2.5487e-01,\n",
      "          2.8308e-01,  1.1355e-01,  2.3447e-01,  2.3029e-01,  2.7474e-01,\n",
      "          2.9673e-01, -3.4664e-04,  4.0459e-01,  3.4803e-01,  2.9259e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2483], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0239,  0.0051,  0.2516,  0.2599,  0.2092,  0.1695,  0.2187,  0.1086,\n",
      "          0.4074,  0.1026,  0.1514, -0.0403,  0.0419,  0.2217,  0.2554,  0.0633,\n",
      "         -0.2622,  0.2977,  0.3489,  0.3701]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3704], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2160,  0.0109,  0.2574,  0.1652, -0.0455,  0.1187,  0.1670, -0.1057,\n",
      "          0.3149,  0.0440,  0.5248,  0.2412,  0.1843,  0.3227,  0.2889,  0.2781,\n",
      "         -0.0507,  0.1934,  0.3183,  0.2419]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3163], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3399,  0.5712,  0.1948,  0.1038,  0.3361,  0.1473,  0.2932,  0.2301,\n",
      "          0.1027,  0.0752,  0.1957,  0.1883,  0.3534,  0.2242,  0.2862, -0.0309,\n",
      "          0.1262,  0.0435,  0.2054,  0.3046]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3016], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3446,  0.1546, -0.0341,  0.0667,  0.2637,  0.4757,  0.2909,  0.1278,\n",
      "          0.2622,  0.3868,  0.3144,  0.1406,  0.2718,  0.4026,  0.2445,  0.3350,\n",
      "         -0.6586,  0.2635,  0.2080,  0.3287]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1924], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2215,  0.3315,  0.0290,  0.3162,  0.3199,  0.2349,  0.3210,  0.2656,\n",
      "          0.3138,  0.1704,  0.1517,  0.2086,  0.2402,  0.0860,  0.2712,  0.1793,\n",
      "         -0.1201,  0.0936,  0.0929,  0.2416]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1904], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1435,  0.3467,  0.0966,  0.3576,  0.1689,  0.0938,  0.2415,  0.0268,\n",
      "          0.2810,  0.3579, -0.0376,  0.3134,  0.3881,  0.2402,  0.2807, -0.0444,\n",
      "         -0.1074,  0.2723,  0.1127,  0.3007]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3077], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4616, -0.1107,  0.2588,  0.1209,  0.2462,  0.1756,  0.1740,  0.1883,\n",
      "          0.1863,  0.2627,  0.0165,  0.3450,  0.1762,  0.0306,  0.2385,  0.2288,\n",
      "          0.1741,  0.0981,  0.1110,  0.2984]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2609], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0324, -0.2120,  0.1169,  0.2655,  0.1751,  0.0233,  0.3043,  0.2851,\n",
      "          0.4601,  0.2637,  0.0777,  0.1960,  0.3358, -0.1169,  0.2650,  0.3631,\n",
      "         -0.1058,  0.4779,  0.2686,  0.2361]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2138], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2094, -0.0899,  0.4083,  0.1357,  0.2576,  0.2187,  0.2823,  0.2257,\n",
      "          0.2918,  0.1894,  0.2799,  0.3316,  0.1632,  0.1103,  0.3048,  0.1247,\n",
      "         -0.0788,  0.1428,  0.1459,  0.3345]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1545], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2986,  0.1609,  0.3808,  0.2663,  0.1615,  0.2987,  0.2064,  0.0724,\n",
      "          0.0223,  0.3617,  0.1679,  0.0382,  0.2709,  0.2462,  0.2875,  0.2435,\n",
      "         -0.1322,  0.1321,  0.1653,  0.4390]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1532], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2474, -0.0383, -0.5300,  0.2349, -0.0093,  0.1107,  0.2204,  0.2512,\n",
      "          0.3793,  0.3615,  0.2922,  0.2814,  0.1508,  0.3586,  0.2068,  0.2610,\n",
      "          0.0015,  0.1835,  0.1302,  0.2275]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2044], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4732,  0.3357,  0.4961,  0.2109,  0.2728,  0.1549,  0.2607,  0.2124,\n",
      "          0.3798,  0.0367,  0.1589,  0.1232,  0.2245,  0.0912,  0.2342,  0.4580,\n",
      "         -0.1779,  0.1011,  0.2357,  0.2617]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3506], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2112,  0.3412, -0.3871,  0.3592,  0.1611,  0.2101,  0.1669,  0.1489,\n",
      "          0.1966,  0.0784,  0.0365,  0.1605,  0.3811,  0.1297,  0.2738,  0.1769,\n",
      "         -0.4203, -0.0288,  0.1507,  0.2304]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2044], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3644,  0.0459,  0.1908,  0.1029,  0.1538,  0.1701,  0.1507,  0.0808,\n",
      "          0.2738,  0.2872,  0.1540,  0.2230,  0.0898,  0.1714,  0.2859,  0.1983,\n",
      "         -0.4717,  0.0509,  0.1883,  0.2960]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2086], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1301,  0.3262, -0.0278,  0.1444,  0.3724,  0.1746,  0.2141,  0.1497,\n",
      "          0.0011,  0.3597,  0.2172,  0.3157,  0.3438,  0.2472,  0.1996,  0.2544,\n",
      "          0.4933,  0.0051,  0.3408,  0.4186]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1938], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1847, 0.2924, 0.0419, 0.0911, 0.1704, 0.1936, 0.1174, 0.2415, 0.2387,\n",
      "         0.0520, 0.1615, 0.2509, 0.1189, 0.0885, 0.2756, 0.3646, 0.7382, 0.2611,\n",
      "         0.2073, 0.0888]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2815], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0198,  0.2548,  0.5441,  0.1299,  0.0611,  0.0943,  0.2307,  0.1127,\n",
      "          0.2814,  0.5640,  0.1855,  0.2802,  0.3517, -0.0897,  0.3013,  0.1277,\n",
      "          0.3549, -0.1377,  0.1951,  0.2258]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1708], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1559,  0.1215, -0.0085,  0.1596,  0.2181, -0.0574,  0.3586, -0.0082,\n",
      "         -0.0759,  0.2755,  0.1128,  0.2928,  0.0145,  0.0626,  0.3031,  0.1451,\n",
      "          0.3391,  0.2832,  0.2369,  0.3764]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0976], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3986, -0.1413,  0.3265,  0.1424,  0.3039,  0.0612,  0.2743,  0.3605,\n",
      "          0.2229,  0.2625,  0.0966,  0.2651,  0.2443,  0.1435,  0.2475,  0.2129,\n",
      "          0.3910,  0.1085,  0.2122,  0.2779]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3094], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4000, -0.1969,  0.5261,  0.2117,  0.2710,  0.2065,  0.0621,  0.4209,\n",
      "          0.1290,  0.2349,  0.1629,  0.1740,  0.0928,  0.5032,  0.2506,  0.0666,\n",
      "         -0.2448, -0.2799,  0.0853,  0.3126]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2496], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3520,  0.1137,  0.0564,  0.2854,  0.2685,  0.3251,  0.2429,  0.1848,\n",
      "         -0.0206, -0.1297,  0.0353,  0.3597,  0.4811,  0.3909,  0.2208,  0.3736,\n",
      "          0.0608, -0.0811,  0.1914,  0.2320]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2522], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1954,  0.1751,  0.0409,  0.0402,  0.3115,  0.1026,  0.3226,  0.2271,\n",
      "          0.3618,  0.2996,  0.1191,  0.0807,  0.1279,  0.0993,  0.2205, -0.0169,\n",
      "         -0.3293,  0.1754,  0.3532,  0.2959]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2215], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1638,  0.4270,  0.6107,  0.1404,  0.1468,  0.3400,  0.1451,  0.2637,\n",
      "          0.2588,  0.1900,  0.2944,  0.2691,  0.0963,  0.1730,  0.2603,  0.2229,\n",
      "         -0.2766,  0.0155,  0.0989,  0.2197]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2244], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1288, -0.0699,  0.2255,  0.3289,  0.2718,  0.1471,  0.2744,  0.0430,\n",
      "          0.2691,  0.2457,  0.2801,  0.4540,  0.2031,  0.1501,  0.3050,  0.1757,\n",
      "         -0.4890, -0.3050,  0.1878,  0.3642]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2519], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1975, -0.0236,  0.1987,  0.2264,  0.0480,  0.0908,  0.3240,  0.2934,\n",
      "          0.4130,  0.1358,  0.1173,  0.1730,  0.2292,  0.2592,  0.2323,  0.4900,\n",
      "          0.1175,  0.0962,  0.2012,  0.2426]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2105], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1947,  0.4273,  0.1995,  0.1566,  0.2970,  0.0247,  0.1030,  0.0472,\n",
      "          0.0482,  0.2662,  0.2031,  0.2385,  0.0383,  0.2536,  0.1674,  0.1707,\n",
      "         -0.1540, -0.1522,  0.4834,  0.2676]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1285], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2383,  0.0661, -0.1284,  0.2049,  0.3769,  0.3155,  0.2192,  0.2914,\n",
      "          0.2297,  0.2527,  0.2280,  0.2285,  0.2470,  0.3336,  0.2417,  0.2545,\n",
      "         -0.4159,  0.3345,  0.1353,  0.2990]])\n",
      "log prob tensor([0.3106])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[ 0.2397, -0.1842,  0.4531,  0.3408,  0.1988,  0.4005,  0.4158,  0.0072,\n",
      "          0.1672,  0.2980,  0.3591,  0.5196,  0.1301,  0.3597,  0.1991,  0.1051,\n",
      "         -0.5920,  0.0052,  0.3367,  0.3099]])\n",
      "log prob tensor([0.1893])\n",
      "Iteration: 80. \n",
      "Test - Loss: nan. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "log prob tensor([[ 0.1402, -0.1391,  0.0223,  0.1181,  0.1682,  0.2622,  0.3316,  0.2421,\n",
      "          0.2409,  0.3739,  0.0205,  0.2500,  0.0673,  0.2151,  0.2748,  0.3919,\n",
      "          0.2470, -0.0419,  0.2743,  0.3635]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2419], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.1529,  0.5403,  0.0401,  0.0705,  0.2417,  0.1527,  0.3252,  0.0902,\n",
      "          0.1422,  0.1346,  0.1253,  0.0633,  0.2856,  0.1152,  0.1892,  0.1720,\n",
      "          0.0468,  0.1136,  0.1269,  0.3140]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2832], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2770,  0.2468,  0.2010,  0.3124,  0.1452,  0.3853,  0.2014,  0.2295,\n",
      "          0.3135,  0.2411,  0.2332,  0.3387, -0.0708,  0.1116,  0.1969,  0.4242,\n",
      "          0.0277,  0.5417,  0.2469,  0.3632]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1476], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3131,  0.0148, -0.0524,  0.0209,  0.2160,  0.3155,  0.3773,  0.2221,\n",
      "          0.2212,  0.1355,  0.1372,  0.2028,  0.2258,  0.5259,  0.2918,  0.1318,\n",
      "         -0.0969,  0.2741,  0.3441,  0.1060]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2103], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2624,  0.2408,  0.2222,  0.1926,  0.2303,  0.2637,  0.2156,  0.2490,\n",
      "          0.2902, -0.0046,  0.2134,  0.2433,  0.2579,  0.3830,  0.2533,  0.0256,\n",
      "         -0.1505,  0.4587,  0.3506,  0.3204]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2481], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2877,  0.0394,  0.2056,  0.2868,  0.3023,  0.0364,  0.3674,  0.0463,\n",
      "          0.2165,  0.2621,  0.1992,  0.2148,  0.2684,  0.5390,  0.2695,  0.2041,\n",
      "         -0.7066, -0.2735,  0.2264,  0.3026]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2697], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0998,  0.0956,  0.0447,  0.2977,  0.2416,  0.2601,  0.1015,  0.0290,\n",
      "          0.2538,  0.2564,  0.1939,  0.0805, -0.0968,  0.3804,  0.2173,  0.0824,\n",
      "         -0.4688,  0.4667,  0.2585,  0.2915]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2759], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2907,  0.2008, -0.0437, -0.0086,  0.2149,  0.2066,  0.1624,  0.2469,\n",
      "         -0.0113,  0.2660,  0.3798,  0.4267,  0.2998,  0.3103,  0.2820,  0.0087,\n",
      "          0.1087,  0.0731,  0.2177,  0.5003]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2154], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3260, -0.0373,  0.1928, -0.0798,  0.2768,  0.2069,  0.2692,  0.1348,\n",
      "          0.3093,  0.2979,  0.1965,  0.1632,  0.1101,  0.2218,  0.2579,  0.3193,\n",
      "         -0.9733, -0.2903,  0.3967,  0.1933]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3198], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2676,  0.0708,  0.1754,  0.0997,  0.1654,  0.1856,  0.1357,  0.0393,\n",
      "          0.0406,  0.2392,  0.3602, -0.0231,  0.1789,  0.3199,  0.2972,  0.2120,\n",
      "          0.2594, -0.1438,  0.4320,  0.3613]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1812], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2634,  0.4177,  0.2106,  0.0077,  0.3405,  0.1532,  0.2700,  0.1594,\n",
      "         -0.0246,  0.2711, -0.0896,  0.2035,  0.0669,  0.0854,  0.1908,  0.1795,\n",
      "         -0.2319,  0.1526,  0.2508,  0.1933]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3144], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4186,  0.6137,  0.5739, -0.1419,  0.2307,  0.0552,  0.0496,  0.1997,\n",
      "          0.0626,  0.2626,  0.3520,  0.0762,  0.2387,  0.4129,  0.2740,  0.3089,\n",
      "         -0.0082,  0.0457,  0.0443,  0.3997]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1535], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1095, -0.2816, -0.0716,  0.4110,  0.3516,  0.0900,  0.1620,  0.2107,\n",
      "          0.0444,  0.2543,  0.1784,  0.1475,  0.1765,  0.2915,  0.3392,  0.1529,\n",
      "          0.2414, -0.0523,  0.3666,  0.4219]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1964], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[0.1756, 0.1327, 0.0665, 0.2973, 0.3154, 0.0484, 0.2602, 0.1294, 0.1020,\n",
      "         0.2610, 0.3109, 0.1240, 0.2601, 0.3599, 0.3037, 0.1642, 0.0034, 0.2613,\n",
      "         0.0264, 0.3177]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1895], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2279,  0.0884,  0.1788,  0.3856,  0.1579,  0.3554,  0.3889,  0.1366,\n",
      "          0.0123,  0.3113, -0.0879,  0.0123,  0.4902,  0.3089,  0.2712,  0.2170,\n",
      "         -0.2530,  0.7128,  0.3168,  0.1918]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3071], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2902,  0.5561,  0.5076,  0.4447,  0.1490,  0.3656,  0.1720,  0.0741,\n",
      "          0.1923,  0.2993,  0.1385,  0.2182,  0.2578,  0.0196,  0.2598,  0.1246,\n",
      "          0.0940, -0.2590,  0.0948,  0.0658]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3893], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0655,  0.0989,  0.4181, -0.3945,  0.1593,  0.0313,  0.2620,  0.3132,\n",
      "          0.2229,  0.2492,  0.0530,  0.0463,  0.3058,  0.1841,  0.1029,  0.0745,\n",
      "          0.2856, -0.0331,  0.2708,  0.2124]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1603], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1377,  0.3218,  0.3722,  0.2159,  0.2780,  0.2366,  0.3462,  0.1180,\n",
      "          0.0801,  0.2953,  0.3660,  0.1334,  0.2583,  0.3667,  0.2287,  0.1349,\n",
      "         -0.2872, -0.3529,  0.1819,  0.4008]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2334], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0631,  0.1133,  0.3148, -0.2230,  0.1897,  0.2178,  0.2479,  0.3735,\n",
      "          0.3230,  0.2994, -0.0196,  0.1979,  0.0049,  0.2250,  0.3289,  0.2137,\n",
      "          0.0034, -0.1348, -0.1140,  0.3322]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1643], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2170, -0.1196, -0.0375, -0.0171,  0.0534,  0.4922,  0.1892,  0.2684,\n",
      "         -0.0227,  0.2822,  0.2499,  0.2406,  0.5070,  0.2385,  0.2347,  0.3009,\n",
      "          0.2303,  0.1178,  0.0587,  0.2720]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1632], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3134,  0.1484,  0.1775, -0.0818,  0.1334,  0.2945,  0.1270,  0.3247,\n",
      "          0.3048,  0.2215,  0.3440,  0.1623, -0.0236,  0.2305,  0.3373,  0.3356,\n",
      "          0.0119, -0.0640,  0.3961,  0.1978]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3429], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0861,  0.1732,  0.0773,  0.3422,  0.2748,  0.5673,  0.0044,  0.2638,\n",
      "         -0.0381,  0.3325,  0.2463,  0.2726,  0.6230,  0.1061,  0.2148,  0.1681,\n",
      "          0.2418,  0.0721,  0.3247,  0.2995]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2457], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3937, -0.2058,  0.1581, -0.4003,  0.3416,  0.4397,  0.1656,  0.4110,\n",
      "          0.0658,  0.2841,  0.1180,  0.1100,  0.2237,  0.3508,  0.2725,  0.1276,\n",
      "         -0.1489,  0.2447,  0.0112,  0.4862]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2194], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1709,  0.9406, -0.0676,  0.2638,  0.2287,  0.2783,  0.1039,  0.2753,\n",
      "          0.1696,  0.2010, -0.0185,  0.3818,  0.1842,  0.2494,  0.3188,  0.2468,\n",
      "          0.0664, -0.1599,  0.0288,  0.3004]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1962], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2465,  0.3072, -0.0716, -0.1157,  0.2225,  0.2189,  0.1633,  0.2035,\n",
      "          0.0649,  0.2397,  0.0341,  0.1169, -0.1556,  0.0943,  0.4557,  0.3670,\n",
      "          0.0028, -0.5737,  0.1586,  0.2386]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1247], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3841,  0.2461, -0.0823,  0.3231,  0.2672,  0.2426,  0.2676,  0.3413,\n",
      "          0.0631,  0.1797,  0.2468,  0.2703, -0.0081,  0.1022,  0.1573,  0.0293,\n",
      "         -0.4477,  0.4346,  0.2076,  0.1909]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2320], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0932, -0.1581, -0.3836,  0.1581,  0.1900,  0.1375,  0.3593,  0.2214,\n",
      "          0.1275,  0.2499,  0.1483,  0.3636,  0.2277,  0.1623,  0.2602, -0.0252,\n",
      "         -0.4185,  0.4269,  0.3531,  0.1979]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1225], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0098, -0.0024,  0.1591,  0.2013,  0.3753,  0.1293,  0.3124,  0.1655,\n",
      "          0.3395,  0.2108,  0.0621, -0.0509,  0.2059,  0.1388,  0.2701,  0.2505,\n",
      "          0.0114, -0.2191,  0.2063,  0.2574]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1945], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1757, -0.0823,  0.1227, -0.0479,  0.2425,  0.3691,  0.1678,  0.1468,\n",
      "          0.3132,  0.2392,  0.4644,  0.3500,  0.1240,  0.1962,  0.3237,  0.3024,\n",
      "         -0.0057, -0.2975,  0.0707,  0.2602]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1891], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0343,  0.2458,  0.5150, -0.0531,  0.2775,  0.0853,  0.2368,  0.0968,\n",
      "          0.4232,  0.3530,  0.1220,  0.1831, -0.1641,  0.3379,  0.1876,  0.3047,\n",
      "         -0.1562,  0.3693, -0.0136,  0.3399]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2459], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4183,  0.1502,  0.3040,  0.1049,  0.4132,  0.3646, -0.0745,  0.2561,\n",
      "          0.1498,  0.3924,  0.2144,  0.2740,  0.3212,  0.2539,  0.2345,  0.4698,\n",
      "         -0.1215,  0.0748,  0.2972,  0.3293]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3009], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2017,  0.2384, -0.0550,  0.6021,  0.3962,  0.2353,  0.2709, -0.0188,\n",
      "          0.3499,  0.2839,  0.3063,  0.1305, -0.2428,  0.4448,  0.1923,  0.0745,\n",
      "         -0.2979, -0.3287,  0.2280,  0.3319]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1797], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1506,  0.0726,  0.1877,  0.2370,  0.2958,  0.1934,  0.3531,  0.1311,\n",
      "          0.0918,  0.2172,  0.1159, -0.0122,  0.1963,  0.3564,  0.3123,  0.2725,\n",
      "         -0.1622, -0.1469,  0.1948,  0.2599]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3400], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2125,  0.3890,  0.1062, -0.3002,  0.1157, -0.0597,  0.3289,  0.3654,\n",
      "          0.0297,  0.2842,  0.2399,  0.0602,  0.0958,  0.2457,  0.4001,  0.1670,\n",
      "          0.1657,  0.0620,  0.0465,  0.2592]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3090], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1676, -0.2777,  0.4689,  0.1930,  0.2125,  0.1166,  0.1514,  0.1826,\n",
      "          0.2418,  0.1567,  0.3451,  0.1516,  0.4117,  0.4418,  0.2242,  0.3388,\n",
      "          1.0195, -0.1974,  0.2962,  0.4256]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2639], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0159,  0.4871,  0.0301,  0.0087,  0.2179,  0.0771,  0.0969,  0.2070,\n",
      "          0.1847,  0.2766,  0.3298, -0.0098,  0.2022,  0.3633,  0.3167,  0.4078,\n",
      "          0.6457, -0.1433,  0.2245,  0.2740]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2582], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2303, -0.0099,  0.0386,  0.7755,  0.2897,  0.3023,  0.3709,  0.1615,\n",
      "          0.3356,  0.3145,  0.1476,  0.2896, -0.5590, -0.0955,  0.4073,  0.1708,\n",
      "          0.3934,  0.2717,  0.1830,  0.0877]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3862], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1399,  0.1964,  0.0013,  0.1093,  0.0846,  0.4005,  0.1304,  0.4681,\n",
      "          0.0463,  0.3008,  0.2147,  0.0634, -0.0453,  0.3612,  0.2868,  0.2867,\n",
      "          0.0030, -0.0232,  0.2972,  0.1604]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1583], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1511, -0.0450,  0.1531,  0.2410,  0.2359,  0.1709,  0.3928, -0.1603,\n",
      "          0.0063,  0.2026,  0.2735,  0.2574, -0.0767, -0.1494,  0.2859,  0.2522,\n",
      "          0.1204,  0.3068,  0.4569,  0.3540]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2780], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1637,  0.3198,  0.4356, -0.1075,  0.1322,  0.0330,  0.1478,  0.3345,\n",
      "          0.0804,  0.2413, -0.0251,  0.3985, -0.1818,  0.2409,  0.2785,  0.0587,\n",
      "         -0.5974, -0.1918,  0.3773,  0.1776]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1475], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0791,  0.2957,  0.0798, -0.1825,  0.2923,  0.5665,  0.1748,  0.2298,\n",
      "          0.1693,  0.2714,  0.2824,  0.3268,  0.0154, -0.0124,  0.1956,  0.0650,\n",
      "         -0.0509, -0.2819,  0.2922,  0.3112]])\n",
      "log prob tensor([0.1398])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[ 0.1113, -0.2362,  0.3361,  0.4584,  0.2019,  0.0466,  0.2624,  0.2890,\n",
      "          0.0246,  0.2982,  0.1043,  0.3327,  0.1247,  0.2443,  0.3849,  0.2450,\n",
      "         -0.0433, -0.1663,  0.0024,  0.1911]])\n",
      "log prob tensor([0.2994])\n",
      "Iteration: 100. \n",
      "Test - Loss: nan. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "log prob tensor([[ 0.0707,  0.1371, -0.0638,  0.0637,  0.3003,  0.2969,  0.0380, -0.1498,\n",
      "         -0.3503,  0.1321,  0.0733,  0.2767,  0.3629,  0.1708,  0.3914,  0.2423,\n",
      "         -0.3167,  0.1714,  0.1415,  0.2934]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2093], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1165,  0.5163, -0.0140,  0.1326,  0.1874,  0.3317,  0.1199,  0.4356,\n",
      "         -0.4638,  0.3011,  0.2286,  0.2726, -0.3904, -0.0430,  0.2664,  0.0935,\n",
      "          0.5320, -0.0224,  0.0842,  0.4643]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1902], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0692,  0.0863,  0.0403,  0.2940,  0.3701,  0.3071,  0.2337,  0.3023,\n",
      "          0.4675,  0.3772,  0.3093,  0.0472,  0.7986, -0.0414,  0.1782,  0.4460,\n",
      "          0.7109, -0.1872,  0.0962,  0.2942]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2202], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4736, -0.0234,  0.3295,  0.2628,  0.2985,  0.3044,  0.2162,  0.2165,\n",
      "         -0.0774,  0.2682,  0.5604,  0.3906, -0.1115,  0.3367,  0.2475,  0.2562,\n",
      "          0.3931,  0.9534,  0.2227,  0.0957]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2365], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1573,  0.2757, -0.0299, -0.1717,  0.2488, -0.0861,  0.2853,  0.2499,\n",
      "          0.0086,  0.2104,  0.2318,  0.0795, -0.1027,  0.3647,  0.0968,  0.0493,\n",
      "         -0.5748,  0.7550,  0.4037,  0.3438]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2860], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2377, -0.2525, -0.0811, -0.2831,  0.2812,  0.1403,  0.1020,  0.3088,\n",
      "          0.0112,  0.2957,  0.1356,  0.0735, -0.2199,  0.4326,  0.2359,  0.1522,\n",
      "          0.4244,  0.0170,  0.1636,  0.4332]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2965], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3208, -0.1952,  0.2879,  0.0920,  0.3061,  0.3675,  0.2498,  0.3120,\n",
      "         -0.2620,  0.2028,  0.1765,  0.0948, -0.0548,  0.3651,  0.3245,  0.2409,\n",
      "         -0.8292,  0.0849,  0.4053,  0.2721]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2219], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3465,  0.0638,  0.1523,  0.0839,  0.2141,  0.0046,  0.1293,  0.1747,\n",
      "         -0.2183,  0.3530,  0.2278,  0.1310, -0.2243,  0.2381,  0.3352,  0.1961,\n",
      "         -0.3844,  0.2798,  0.1734,  0.3339]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3323], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.5134,  0.3542,  0.1470, -0.0589,  0.2042, -0.2824,  0.1893,  0.0361,\n",
      "          0.3290,  0.3319,  0.4194,  0.4610,  0.5198,  0.3183,  0.3259,  0.1894,\n",
      "         -0.5585,  0.0398,  0.3164,  0.1132]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2305], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0544, -0.3558,  0.2814,  0.2298,  0.3354,  0.1725,  0.2167,  0.4081,\n",
      "         -0.2917,  0.3012,  0.0416,  0.1407,  0.7648,  0.1012,  0.4375,  0.1862,\n",
      "         -0.4906,  0.3134,  0.0827,  0.2810]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2753], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0657,  0.3126,  0.9062,  0.0512,  0.2642,  0.0868,  0.2828,  0.0692,\n",
      "          0.0849,  0.2178,  0.0482,  0.1403,  0.2827,  0.3563,  0.2568,  0.4338,\n",
      "          0.1627, -0.1705,  0.4184,  0.2418]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3295], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0481, -0.1080,  0.0588,  0.3738,  0.2199,  0.2216, -0.0275,  0.2067,\n",
      "         -0.0436,  0.3403,  0.2748,  0.5247, -0.0165,  0.3569,  0.3828, -0.0426,\n",
      "         -0.2308, -0.0417,  0.2418,  0.1934]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2828], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1320,  0.1164, -0.1508, -0.1064,  0.1905,  0.0025,  0.3583,  0.4249,\n",
      "         -0.2158,  0.3717,  0.2877,  0.4805,  0.1826,  0.2631,  0.2946,  0.2450,\n",
      "          0.2234,  0.1782,  0.2509,  0.3517]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1818], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.5502,  0.3404,  0.2477, -0.0628,  0.2861,  0.2037,  0.1840,  0.5042,\n",
      "         -0.0091,  0.3277,  0.2857,  0.0565,  0.0787,  0.2994,  0.2857,  0.4530,\n",
      "         -0.0723,  0.2960,  0.0871,  0.1473]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2570], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4485,  0.3674, -0.0686, -0.0291,  0.3182, -0.0836,  0.1749,  0.1414,\n",
      "          0.4959,  0.2462,  0.3822,  0.1576,  0.4262,  0.2799,  0.1128,  0.1376,\n",
      "         -0.0532,  0.2477,  0.3015,  0.3155]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2351], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1542,  0.0664, -0.2924, -0.1408,  0.2347,  0.1713,  0.1555,  0.1248,\n",
      "          0.0112,  0.4412,  0.3094,  0.2272,  0.5572,  0.1192,  0.3176,  0.0869,\n",
      "          0.0050,  0.3031,  0.1180,  0.1886]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1214], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2683, -0.2044,  0.0951,  0.4995,  0.1675, -0.1018,  0.3401,  0.1454,\n",
      "          0.1848,  0.3006,  0.2432,  0.2315,  0.1181,  0.3642,  0.1635,  0.3530,\n",
      "         -0.0331,  0.2971,  0.1938,  0.2652]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2175], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 9.8670e-02,  3.8794e-01,  1.0368e-01, -1.8713e-01,  1.9019e-01,\n",
      "          1.5731e-01,  3.3887e-01,  4.4967e-01,  5.7936e-03,  2.9538e-01,\n",
      "          1.4776e-01,  1.5248e-01,  1.4287e-01,  3.2202e-01,  1.8076e-01,\n",
      "          2.1902e-01, -5.8709e-01, -1.8160e-01, -4.6179e-05,  4.7219e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2514], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0080, -0.2905,  0.0860,  0.1795,  0.1830,  0.5422,  0.2308, -0.0361,\n",
      "         -0.6153,  0.2760,  0.2180,  0.3458,  0.2713, -0.0438,  0.2353,  0.1899,\n",
      "          0.2332,  0.1005, -0.0349,  0.3910]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3564], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3755,  0.1818,  0.4003,  0.1851,  0.3875,  0.0042,  0.1742,  0.1836,\n",
      "          0.4926,  0.1672, -0.1467,  0.3808,  0.0403,  0.0719,  0.1366,  0.2200,\n",
      "          0.3019,  0.5275,  0.3257,  0.4599]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3978], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2972,  0.0623,  0.0326,  0.7357,  0.2998,  0.4437,  0.3757,  0.1147,\n",
      "         -0.2364,  0.2272, -0.0073,  0.1201,  0.0784,  0.1433,  0.2491,  0.1927,\n",
      "          0.5033,  0.0662,  0.1304,  0.2041]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2147], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0348,  0.5640, -0.3647,  0.0623,  0.1131,  0.1490,  0.4646,  0.5107,\n",
      "         -0.2406,  0.2044,  0.3128,  0.3975,  0.8132,  0.2376,  0.3249,  0.2105,\n",
      "         -0.1327, -0.0914,  0.1491,  0.1529]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2818], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0920,  0.2984,  0.5430,  0.2854,  0.2261,  0.4802,  0.2760, -0.0232,\n",
      "         -0.4991,  0.3119,  0.2912,  0.2744,  0.3808,  0.1247,  0.2525,  0.3527,\n",
      "          0.2446,  0.2308,  0.3422,  0.4582]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2231], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3454,  0.3508, -0.3130, -0.6390,  0.2624, -0.2891,  0.2818,  0.0705,\n",
      "         -0.1804,  0.0730,  0.3000,  0.1781,  0.1052,  0.1377,  0.3232,  0.2198,\n",
      "         -0.1587,  0.1610,  0.2845,  0.1902]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1836], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1394, -0.3520,  0.0344,  0.3554,  0.1168,  0.4972,  0.2702,  0.0334,\n",
      "         -0.0058,  0.1374,  0.2937,  0.3288, -0.1419, -0.0125,  0.0687,  0.2084,\n",
      "          0.0532,  0.2391,  0.3474,  0.3802]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1349], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-1.3116e-01, -1.7713e-02,  2.6485e-01,  3.5983e-04,  2.2803e-02,\n",
      "         -2.3224e-01,  1.7918e-01, -8.4479e-03, -2.4280e-01,  2.8469e-01,\n",
      "          3.2719e-01,  2.1245e-01,  3.6663e-01,  2.8411e-01,  3.0955e-01,\n",
      "          2.3080e-01,  3.1307e-01, -3.4900e-01,  5.2156e-01,  2.3471e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1951], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0572,  0.0309, -0.0582, -0.2211,  0.2779, -0.3429,  0.1960,  0.3877,\n",
      "          0.2982,  0.2267,  0.3060,  0.0818,  0.4073,  0.0998,  0.3131,  0.2172,\n",
      "          0.2134,  0.1192,  0.0984,  0.1700]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1963], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2780,  0.6718, -0.0855, -0.3102,  0.4109,  0.2697,  0.1394,  0.1783,\n",
      "          0.0284,  0.4215,  0.2846,  0.2539, -0.0246,  0.4283,  0.3835,  0.1762,\n",
      "          0.5549,  0.1339,  0.5736,  0.3870]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3678], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0803, -0.5370,  0.3965, -0.2812,  0.1599,  0.0811,  0.1137,  0.0808,\n",
      "          0.1476,  0.2422,  0.3218,  0.3319,  0.1165,  0.3125,  0.2068,  0.2177,\n",
      "          0.3459, -0.0847,  0.0611,  0.2268]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1185], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.3656,  0.1678,  0.3462,  0.0605,  0.1878,  0.1637,  0.2292,  0.2411,\n",
      "          0.3864,  0.1682,  0.2299, -0.0821,  0.3785, -0.0641,  0.3572, -0.0974,\n",
      "         -0.0223, -0.1798,  0.2419,  0.3195]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0918], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0854, -0.0505,  0.4466, -0.3573,  0.0743, -0.0628,  0.1501,  0.1489,\n",
      "         -0.3768,  0.1964,  0.3432,  0.2371,  0.1645,  0.4596,  0.4646,  0.1960,\n",
      "          0.3306, -0.0019,  0.2002,  0.1485]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2322], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.7766,  0.2552,  0.2875,  0.1992,  0.2097,  0.3147,  0.2831,  0.0471,\n",
      "          0.1133,  0.1939,  0.2547,  0.0305,  0.0193, -0.0417,  0.3445,  0.1720,\n",
      "         -0.1996,  0.1070,  0.1421,  0.3373]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1902], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1273,  0.0892,  0.5345, -0.2779,  0.2552,  0.4891,  0.1479,  0.1972,\n",
      "          0.4090,  0.4129,  0.2693,  0.3529,  0.7392, -0.0123,  0.2551,  0.3727,\n",
      "         -0.0404,  0.0951, -0.0317,  0.1852]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0966], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1420,  0.1024,  0.1944, -0.7195,  0.2875, -0.1049,  0.1657,  0.3926,\n",
      "          0.0472,  0.2999,  0.2736,  0.3294,  0.1350,  0.2291,  0.1703,  0.2073,\n",
      "          0.7514,  0.2229,  0.1621,  0.2941]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2912], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3167, -0.1596,  0.2374,  0.1645,  0.2731, -0.0362,  0.1453,  0.2324,\n",
      "          0.0756,  0.2206,  0.2996,  0.0614,  0.0127,  0.2309,  0.3273,  0.2210,\n",
      "          0.0998,  0.7279,  0.2788,  0.4665]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2615], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4055,  0.6455,  0.1618,  0.0526,  0.1044,  0.1170,  0.2280,  0.2349,\n",
      "          0.1984,  0.3771,  0.3223,  0.0442,  0.1473,  0.1843,  0.2136,  0.1448,\n",
      "         -0.1019,  0.0900,  0.1325,  0.2387]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2337], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.4986, -0.1154,  0.5373,  0.3243,  0.3952, -0.0850,  0.0243,  0.3270,\n",
      "          0.3328,  0.3032,  0.3306,  0.3399,  0.0905,  0.4212,  0.2448,  0.1116,\n",
      "          0.4693,  0.1782,  0.2938,  0.2683]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3304], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2080,  0.3243, -0.1732,  0.1732,  0.2823,  0.4557,  0.0931,  0.2875,\n",
      "          0.7558,  0.3729,  0.2388,  0.2933,  0.4104, -0.0082,  0.3909,  0.3803,\n",
      "          0.2885,  0.0998, -0.1374,  0.2297]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1981], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0767, -0.2545, -0.0989,  0.0238,  0.4018,  0.2280, -0.0645,  0.0944,\n",
      "         -0.1554,  0.4965,  0.3358,  0.4642, -0.2001,  0.1529,  0.1186, -0.1320,\n",
      "          0.0388, -0.1057,  0.1071,  0.4463]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1718], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.1857,  0.1476,  0.8028,  0.5414,  0.3649,  0.1036,  0.3704,  0.2858,\n",
      "         -0.0436,  0.3235,  0.2735,  0.2532,  0.4454,  0.3514,  0.1695,  0.3768,\n",
      "         -0.3652, -0.0041, -0.1758,  0.2801]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1009], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2153,  0.3572, -0.0638,  0.3955,  0.4891,  0.4873,  0.3614,  0.3252,\n",
      "          0.4207,  0.1796,  0.2105,  0.0187, -0.3434, -0.0448,  0.4188,  0.5459,\n",
      "         -0.2106,  0.0779,  0.2909,  0.2594]])\n",
      "log prob tensor([0.2849])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[-0.3739, -0.1833, -0.0216, -0.0298,  0.1165,  0.3652,  0.1917, -0.0609,\n",
      "          0.2222,  0.2335,  0.2689,  0.2745,  0.0907, -0.0626,  0.1988, -0.1156,\n",
      "         -0.1869, -0.0556,  0.4208,  0.2468]])\n",
      "log prob tensor([0.5735])\n",
      "Iteration: 120. \n",
      "Test - Loss: nan. Accuracy: 51.90994181634202\n",
      "Train -  Loss: nan. Accuracy: 634382.1862348178\n",
      "\n",
      "log prob tensor([[-0.2347, -0.2614,  0.3688, -0.1068,  0.1303,  0.1147, -0.0551,  0.2201,\n",
      "         -0.0858,  0.3512,  0.2519,  0.2311,  0.3804,  0.5087,  0.2054,  0.3724,\n",
      "         -0.5971,  0.3357, -0.0543,  0.4886]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1238], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1499,  0.2428,  0.0456, -0.1150,  0.3527,  0.2926,  0.2843,  0.2867,\n",
      "          0.0916,  0.3211,  0.2798,  0.0838,  0.3386,  0.0455,  0.3709,  0.2306,\n",
      "          0.2816,  0.0740,  0.3356,  0.2494]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2029], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0718,  0.3125,  0.0296,  0.3524,  0.1719, -0.0776,  0.2743,  0.2535,\n",
      "         -0.2682,  0.3057,  0.3059,  0.2146,  0.4828,  0.3799,  0.4166,  0.0676,\n",
      "         -0.1645,  0.1568,  0.0804,  0.3890]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2828], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.9212,  0.2339,  0.0393,  0.6055,  0.1716,  0.3116,  0.2106,  0.3295,\n",
      "          0.9199,  0.4600,  0.3227,  0.0961,  0.0766,  0.4311,  0.2172,  0.5006,\n",
      "          0.2940, -0.1218,  0.1653,  0.2971]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3775], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3558,  0.6352,  0.0338, -0.1717,  0.3923,  0.5283,  0.1961,  0.3766,\n",
      "         -0.3998,  0.4341,  0.2750,  0.3210, -0.1213,  0.2062,  0.1639, -0.0493,\n",
      "          0.4843, -0.0012,  0.2679,  0.3912]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2047], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3002, -0.0645,  0.3588,  0.1421,  0.0847,  0.1392,  0.5661,  0.1325,\n",
      "         -0.1643,  0.2460,  0.3438,  0.2049,  0.6691,  0.2170,  0.3610,  0.2069,\n",
      "         -0.0196,  0.0892,  0.2677,  0.4176]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1801], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3440, -0.4478,  0.0343, -0.0360,  0.2802,  0.2939,  0.2042,  0.1720,\n",
      "          0.7598,  0.1408,  0.3288,  0.3957, -0.3481, -0.0338,  0.2633,  0.0206,\n",
      "         -0.2581, -0.1107,  0.1300,  0.1596]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3729], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.6793,  0.3363,  0.1237,  0.3209,  0.2583, -0.0149,  0.3862,  0.3276,\n",
      "          0.3594,  0.1467,  0.2455,  0.2149, -0.5269,  0.1317,  0.1544,  0.3843,\n",
      "          0.2103,  0.2392,  0.1716,  0.4140]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3250], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2998,  0.1733,  0.0161,  0.2786,  0.3222,  0.2280,  0.2869,  0.0090,\n",
      "          0.4234,  0.3883,  0.2495,  0.2189,  0.2615,  0.0743,  0.4045,  0.1360,\n",
      "         -0.3090,  0.0729,  0.1927,  0.3730]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2330], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2882,  0.1636,  0.2994, -0.1438,  0.2312,  0.3943,  0.1119,  0.3008,\n",
      "         -0.0181,  0.2801,  0.4654, -0.1834, -0.2630, -0.0279,  0.1705,  0.4769,\n",
      "         -0.4303, -0.2611,  0.3772,  0.3369]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1986], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0328, -0.1752,  0.2877, -0.0966,  0.3397, -0.0444,  0.2356,  0.1533,\n",
      "          0.2450,  0.3878,  0.3300, -0.2569,  0.0331,  0.2132,  0.1313,  0.4481,\n",
      "          0.0869,  0.2635, -0.2366,  0.2084]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2862], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0678,  0.4564,  0.1383, -0.2247,  0.3702, -0.0241,  0.2684,  0.4462,\n",
      "          0.2678,  0.3495,  0.3382, -0.0128, -0.1532,  0.0837,  0.2534,  0.0334,\n",
      "         -0.2077,  0.2536,  0.1593,  0.3832]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2669], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2928,  0.2751,  0.4377, -0.0433,  0.2147,  0.3830,  0.2708, -0.1869,\n",
      "         -0.2337,  0.3975,  0.2658,  0.5476,  0.2622,  0.2675,  0.3166,  0.0897,\n",
      "          0.0874,  0.3344,  0.2306,  0.1873]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2411], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3269,  0.4618, -0.0642, -0.2082,  0.1640,  0.3410,  0.2191,  0.1463,\n",
      "          0.5101,  0.1280,  0.2824,  0.2673, -0.0039,  0.2334,  0.3168,  0.5636,\n",
      "         -1.0849,  0.2515,  0.0324,  0.6131]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2842], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1344,  0.3113,  0.0744,  0.2352,  0.3110,  0.0505,  0.1644,  0.0527,\n",
      "          0.6017,  0.4404,  0.3233,  0.3836,  0.2705,  0.2579,  0.2618,  0.1743,\n",
      "          0.5044, -0.0841,  0.3987,  0.3664]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3425], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1907,  0.3952,  0.1302,  0.1656,  0.4071, -0.0543, -0.0085,  0.3515,\n",
      "          0.3825,  0.1591,  0.2290,  0.5122, -0.1572,  0.0761,  0.3904, -0.0681,\n",
      "          0.1107,  0.1085, -0.0762,  0.3002]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1815], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 4.9267e-01, -8.7837e-02,  4.1048e-01, -6.9560e-02,  2.7870e-01,\n",
      "          1.7861e-01,  3.7665e-01, -1.7271e-01,  6.6920e-01,  3.8948e-01,\n",
      "          2.3695e-01,  3.0678e-01, -4.5184e-02,  4.4886e-01,  2.0972e-01,\n",
      "          3.6552e-01,  2.9592e-01, -6.5341e-02, -5.0201e-04,  1.8032e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1455], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3068, -0.0900,  0.3919, -0.2641,  0.3912,  0.2907,  0.2728,  0.2103,\n",
      "          0.0806,  0.2019,  0.2324,  0.2080, -0.0015,  0.3119,  0.1412,  0.3280,\n",
      "          0.0502,  0.2258,  0.2814,  0.4374]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3204], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0916, -0.1854,  0.1310, -0.2506,  0.0754, -0.2035,  0.0893, -0.0139,\n",
      "         -0.1996,  0.1792,  0.3730,  0.1400, -0.1673,  0.2968,  0.4008,  0.3898,\n",
      "         -0.5001,  0.2554,  0.6235,  0.4678]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2863], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0628,  0.2466, -0.0867, -0.0680,  0.1828,  0.5454,  0.2550, -0.2985,\n",
      "         -0.5461,  0.1443,  0.3637,  0.3829, -0.0604,  0.3289,  0.2241,  0.0367,\n",
      "          0.6043,  0.3683,  0.2656,  0.3993]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2895], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4573,  0.8812,  0.2577,  0.1106,  0.0881,  0.6250,  0.3517,  0.3694,\n",
      "         -0.0141,  0.2285,  0.3078,  0.2572,  0.4340,  0.1948,  0.3101,  0.2355,\n",
      "          0.1944,  0.4465,  0.2329,  0.3822]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0041], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1722,  0.0149,  0.0432,  0.5168,  0.2712,  0.5303,  0.3110,  0.2561,\n",
      "         -0.4178,  0.3835,  0.2826,  0.4285, -0.1015,  0.1279,  0.3122,  0.1726,\n",
      "          0.1927, -0.0899,  0.1895,  0.0200]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0607], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.2159,  0.2963,  0.2252, -0.1853,  0.1614, -0.1295,  0.3826,  0.3245,\n",
      "          0.2970,  0.4485,  0.2354,  0.2847,  0.1734,  0.3223,  0.2733, -0.1474,\n",
      "         -0.1283, -0.3877,  0.2534,  0.3840]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.2751], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0781,  0.8503,  0.3968, -0.2129,  0.1659,  0.2826,  0.3193,  0.0985,\n",
      "         -0.0642,  0.1160,  0.3255,  0.2305,  0.0687,  0.3257,  0.5063,  0.2846,\n",
      "         -0.1334,  0.5028,  0.0517,  0.2179]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0413], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1686,  0.4573,  0.0209, -0.0445,  0.0305,  0.4736,  0.2589, -0.0563,\n",
      "         -0.1838,  0.2370,  0.2700,  0.0552, -0.1401,  0.3208,  0.0962,  0.3167,\n",
      "         -0.2314,  0.3303,  0.1984,  0.3962]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1879], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4932,  0.4951, -0.1251,  0.4365,  0.3891, -0.0183,  0.3123,  0.2901,\n",
      "          0.6171,  0.1147,  0.2917,  0.2165, -0.3967,  0.5429,  0.3339,  0.3651,\n",
      "         -0.2316,  0.6489,  0.4902,  0.0899]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1794], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.3445,  0.5040,  0.3764,  0.3713,  0.2703,  0.1876,  0.0119,  0.0256,\n",
      "         -0.0794,  0.2926,  0.2435,  0.5136, -0.0476,  0.2135,  0.2570,  0.3026,\n",
      "          0.2405,  0.3141,  0.2010,  0.1898]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1175], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4299,  0.4199, -0.1889,  0.1890,  0.0340,  0.2733,  0.0550, -0.0867,\n",
      "         -0.1779,  0.2880,  0.3180,  0.2287,  0.4104, -0.2541,  0.0973,  0.1882,\n",
      "          0.1816,  0.1396,  0.3758,  0.3209]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.5273], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1571,  0.1154,  0.2991, -0.0335,  0.2345,  0.2772,  0.2359,  0.2156,\n",
      "          0.2115,  0.3738,  0.3147,  0.3041,  0.0532,  0.3029,  0.2697,  0.4458,\n",
      "          0.0865,  0.4161, -0.0278,  0.2089]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.2733], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3985,  0.0711, -0.3333,  0.1893,  0.3638, -0.0232,  0.2604,  0.6407,\n",
      "          0.1726,  0.0967,  0.2825,  0.0650,  0.0857,  0.0903,  0.1980,  0.1234,\n",
      "         -0.2147, -0.2927,  0.2697,  0.3991]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0219], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4865,  0.3034,  0.1693,  0.2962,  0.2537, -0.1689,  0.3030,  0.1141,\n",
      "         -0.0830,  0.2089,  0.3159, -0.0027, -0.4647, -0.1131,  0.2360,  0.2695,\n",
      "          0.4281,  0.1214,  0.2013,  0.2123]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.4771], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3292,  0.2121, -0.0966, -0.1230,  0.0768,  0.0927,  0.2108,  0.2784,\n",
      "         -0.0736,  0.1321,  0.3332,  0.1847, -0.0807,  0.1995,  0.3926,  0.2442,\n",
      "         -0.2035, -0.2185, -0.2647,  0.4251]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.4968], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2514, -0.1493,  0.3900, -0.3670,  0.1695,  0.0047,  0.2097,  0.2780,\n",
      "         -0.0292,  0.1055,  0.2872,  0.5092,  0.2675,  0.3207,  0.3505,  0.3170,\n",
      "          0.8115, -0.1836,  0.4588,  0.3155]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0897], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1427, -0.0145,  0.4973, -0.0144,  0.4763, -0.3191,  0.2604,  0.3108,\n",
      "          0.2082,  0.2398,  0.3613,  0.0495,  0.0305,  0.2895,  0.1428,  0.1367,\n",
      "         -0.4204, -0.0561,  0.0223,  0.3842]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1253], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1442,  0.6722,  0.0275, -0.1340,  0.3245, -0.2351,  0.2677,  0.1386,\n",
      "          0.3653,  0.3546,  0.3583,  0.4026,  0.5423,  0.5649,  0.0965,  0.2627,\n",
      "          0.0062,  0.2984, -0.0369,  0.2961]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3637], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1541, -0.1635, -0.1025,  0.2093,  0.1298,  0.0221,  0.3150, -0.2684,\n",
      "          0.4468,  0.3486,  0.3758,  0.2625, -0.4729,  0.0601,  0.5561,  0.1649,\n",
      "         -0.4414,  0.1027,  0.5041,  0.4956]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.6288], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.4332,  0.1025, -0.0411, -0.0731,  0.0942,  0.1115,  0.1919,  0.5710,\n",
      "         -0.1045,  0.3557,  0.3110,  0.3108,  0.2822,  0.1583,  0.2849,  0.1926,\n",
      "          0.0021,  0.1647,  0.1195,  0.2174]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.4136], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0704,  0.0157,  0.5114,  0.0008,  0.1252,  0.1166,  0.3660,  0.0685,\n",
      "          0.0278,  0.2210,  0.2859,  0.2854, -0.1547,  0.4400,  0.2889,  0.3556,\n",
      "          0.4625,  0.1373,  0.4561,  0.2422]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.2569], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0652,  0.0312,  0.9238, -0.3395,  0.2843,  0.5685,  0.1279,  0.0105,\n",
      "         -0.1990,  0.2120,  0.1471,  0.0392, -0.0164,  0.2890,  0.3003,  0.3055,\n",
      "          0.5825,  0.1990, -0.0248,  0.3570]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.1789], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3886,  0.3801,  0.3208, -0.3700,  0.3883, -0.1347,  0.1573,  0.1108,\n",
      "          0.1554,  0.2940,  0.1874,  0.4588,  0.4468,  0.2988,  0.4255,  0.1036,\n",
      "          0.1212,  0.2091,  0.1390,  0.1911]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2855], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0067,  0.3018,  0.4149,  0.4059,  0.3238,  0.2159,  0.3708, -0.2009,\n",
      "          0.2270,  0.3505,  0.2899,  0.3085,  0.2558,  0.2064,  0.2050,  0.2153,\n",
      "         -0.1091,  0.6343, -0.0299,  0.4587]])\n",
      "log prob tensor([-0.1616])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[ 0.0058, -0.2767,  0.0810,  0.1033,  0.1647,  0.2575,  0.4553,  0.1375,\n",
      "         -0.2375,  0.2098,  0.4157,  0.2295, -0.0972,  0.0200,  0.1272, -0.0195,\n",
      "         -0.2091,  0.3185,  0.2051,  0.4236]])\n",
      "log prob tensor([-0.1672])\n",
      "Iteration: 140. \n",
      "Test - Loss: nan. Accuracy: 47.609410574247406\n",
      "Train -  Loss: nan. Accuracy: 544970.6477732793\n",
      "\n",
      "log prob tensor([[ 0.1009, -0.4602,  0.1175, -0.5211,  0.1793, -0.2454, -0.0498,  0.1790,\n",
      "         -0.1293,  0.5765,  0.2643,  0.3254, -0.1064,  0.0249,  0.3072,  0.4517,\n",
      "         -0.0634,  0.3667,  0.1154,  0.2998]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([1.0872], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1765,  0.8058,  0.2051,  0.3866,  0.1364, -0.0088,  0.1492, -0.1792,\n",
      "          0.0388,  0.4472,  0.2269,  0.2098,  0.2346, -0.0272,  0.2704,  0.2977,\n",
      "         -0.3205,  0.3197, -0.0658,  0.0944]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2267], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.1961,  0.0103,  0.0645,  0.0703,  0.2095,  0.2723,  0.1617,  0.3211,\n",
      "          0.1008,  0.1688,  0.4588,  0.0615,  0.3058,  0.5479,  0.3801,  0.0735,\n",
      "         -0.4622,  0.0885,  0.3106,  0.2120]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3051], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2898,  0.3507, -0.1119,  0.6469,  0.5686,  0.1376,  0.5131, -0.2645,\n",
      "         -0.1164,  0.3016,  0.3259,  0.2312,  0.3927,  0.1791,  0.1392,  0.2388,\n",
      "         -0.2280,  0.0392,  0.1110,  0.4524]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3059], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0132, -0.4052, -0.0985, -0.1326,  0.0127, -0.3782,  0.3718,  0.2001,\n",
      "          0.4586,  0.3292,  0.2854,  0.0733,  0.0059,  0.5141,  0.0947,  0.2583,\n",
      "         -0.2312,  0.2740,  0.2127,  0.2255]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0523], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4800,  0.1777,  0.3021, -0.3332,  0.2544,  0.2187,  0.3564,  0.6012,\n",
      "         -0.3177,  0.2736,  0.2650,  0.2688, -0.1305,  0.1911,  0.3236,  0.4685,\n",
      "         -0.1180,  0.5161,  0.3116,  0.3079]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.1440], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0671, -0.1914,  0.3018, -0.0848,  0.0929,  0.0306,  0.4394,  0.4547,\n",
      "         -0.1041,  0.2624,  0.2991,  0.2147, -0.0572,  0.2967,  0.2249,  0.0559,\n",
      "         -0.1470,  0.0527, -0.0632,  0.2868]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1512], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0082,  0.0348,  0.1202, -0.2521,  0.0889, -0.1062,  0.0172,  0.3855,\n",
      "          0.2170,  0.2104,  0.1502,  0.1498,  0.0807,  0.1984,  0.1564,  0.3102,\n",
      "         -0.2559,  0.1118,  0.0512,  0.2964]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.1107], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0839,  0.3828,  0.5189, -0.3251,  0.3430,  0.4842,  0.1184, -0.0561,\n",
      "         -0.1486,  0.3146,  0.3042,  0.2865,  0.0080,  0.1692,  0.2559,  0.0630,\n",
      "          0.2449, -0.4754,  0.2241,  0.2936]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0033], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.5036,  0.0627, -0.0262, -0.0734,  0.3472,  0.1448,  0.0552, -0.0825,\n",
      "          0.3624,  0.1495,  0.0527,  0.1252, -0.1675,  0.1012,  0.4247,  0.1013,\n",
      "          0.1232, -0.5469,  0.0914,  0.4772]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.1974], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3196,  0.4919, -0.0401,  0.1877,  0.0278, -0.0805,  0.1132, -0.0433,\n",
      "         -0.1272,  0.3474,  0.2640,  0.2295,  0.0212,  0.2663,  0.2483,  0.1928,\n",
      "          0.1333, -0.0688,  0.0746,  0.4355]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0748], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1669,  0.0262,  0.2826,  0.5525,  0.1397,  0.0251,  0.0587,  0.0880,\n",
      "          0.2758,  0.3709,  0.2099,  0.4272, -0.5326,  0.1037,  0.1888,  0.2184,\n",
      "          0.2142,  0.1994,  0.3771,  0.4036]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.4631], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1253,  0.2184,  0.3538,  0.3306,  0.3831, -0.0817,  0.3337,  0.6155,\n",
      "          0.2327,  0.4100,  0.4418,  0.3208, -0.0548,  0.2722,  0.2326,  0.4360,\n",
      "         -0.0285,  0.1897,  0.3709,  0.1350]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1318], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1349,  0.2190, -0.2619,  0.0719, -0.0189,  0.4515,  0.3351,  0.0945,\n",
      "          0.0917,  0.3846,  0.2757,  0.0126,  0.5216,  0.3928,  0.4276,  0.4322,\n",
      "         -0.4547,  0.0161,  0.1556,  0.4119]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.2451], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.2919, -0.2613,  0.4504, -0.0512,  0.1576,  0.2422,  0.3135,  0.0652,\n",
      "          0.1167,  0.3962,  0.2627,  0.4061, -0.3989,  0.3175,  0.2615,  0.4766,\n",
      "         -0.0440,  0.0125,  0.1505,  0.2695]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.4522], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.0893,  0.2905,  0.3493,  0.0406,  0.1686,  0.1755,  0.0225,  0.1770,\n",
      "          0.2074,  0.3322,  0.3748,  0.3113, -0.0427,  0.1365,  0.1194,  0.3147,\n",
      "         -0.0766,  0.3391,  0.0556,  0.4564]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1907], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.2466,  0.0035, -0.1366,  0.1531,  0.1969,  0.1616,  0.2754, -0.0131,\n",
      "          0.1793,  0.2042,  0.2924,  0.1691, -0.3712,  0.1463,  0.3024,  0.0372,\n",
      "         -0.2015,  0.4494,  0.2151,  0.3586]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.8859], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0060,  0.0443,  0.0393,  0.3493,  0.2936,  0.1554,  0.1696,  0.3634,\n",
      "          0.0735,  0.3981,  0.1642,  0.4211, -0.1051,  0.0827,  0.4113,  0.4633,\n",
      "          0.1520,  0.0610,  0.0841,  0.1150]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3119], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2416,  0.0622,  0.2533, -0.0504,  0.4465,  0.3069,  0.1710,  0.0624,\n",
      "          0.1591,  0.3048,  0.2342,  0.3582,  0.0300,  0.2906,  0.3337,  0.2606,\n",
      "         -0.4862,  0.0214,  0.3193,  0.1584]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1676], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.2247, -0.5300,  0.1252, -0.7218,  0.3330, -0.5055,  0.0787, -0.1769,\n",
      "          0.0032,  0.1927,  0.2965,  0.2011,  0.0792,  0.3861,  0.1414,  0.4097,\n",
      "          0.5165, -0.3992, -0.0117,  0.3272]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1904], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4748,  0.2971,  0.2086, -0.0054,  0.0776,  0.2585,  0.1448,  0.2100,\n",
      "         -0.2413,  0.2433,  0.3844,  0.1797, -0.1610,  0.0837,  0.2236,  0.5467,\n",
      "         -0.1046,  0.2482,  0.2345,  0.0722]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.9128], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.3247,  0.2926, -0.3912,  0.0851,  0.1209,  0.1514,  0.1456, -0.1094,\n",
      "          0.0590,  0.2270,  0.3590,  0.3107,  0.1103, -0.0410,  0.1246,  0.2879,\n",
      "         -0.2600, -0.3459,  0.2046,  0.4921]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1305], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.4889,  0.1358, -0.3382,  0.3256,  0.2647,  0.5876,  0.2935, -0.0109,\n",
      "          0.1419,  0.2503,  0.3384,  0.4382, -0.2326,  0.3625,  0.3951,  0.3061,\n",
      "          0.3783,  0.1305,  0.2341,  0.2108]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0563], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.1477,  0.3145,  0.0439, -0.0208,  0.1581,  0.2951,  0.3579, -0.0005,\n",
      "          0.3215,  0.2266,  0.2934,  0.0865, -0.3563,  0.3292,  0.1991,  0.1882,\n",
      "         -0.1207, -0.4269,  0.3900,  0.0998]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0318], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0173,  0.0757,  0.2048,  0.2869,  0.2074, -0.1983,  0.0140, -0.0199,\n",
      "         -0.0979,  0.2146,  0.4005,  0.3020,  0.2607,  0.1996,  0.3359,  0.2999,\n",
      "         -0.2973,  0.7102,  0.2743,  0.3039]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.5143], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0555,  0.3166, -0.2037,  0.3417,  0.0858,  0.1278,  0.2182,  0.3155,\n",
      "          0.0153,  0.4474,  0.2640,  0.5403,  0.1563,  0.2870,  0.3837,  0.2458,\n",
      "         -0.0309, -0.0837,  0.3032,  0.4255]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1316], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 3.8894e-01, -3.9377e-04,  4.3794e-01, -1.2095e-01,  8.1587e-02,\n",
      "          1.4839e-01,  2.3689e-01,  4.9367e-01,  1.8066e-01,  1.8148e-01,\n",
      "          3.4462e-01,  2.2511e-01,  1.4710e-01, -1.1609e-01,  1.2852e-01,\n",
      "         -1.7579e-02, -2.0322e-01,  1.4070e-02, -3.9944e-02,  3.9226e-01]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.2797], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1108, -0.2916, -0.0804, -0.0450,  0.3116,  0.5082,  0.1056,  0.0733,\n",
      "          0.2142,  0.0794,  0.0686,  0.0294, -0.1391,  0.1991,  0.3347,  0.2945,\n",
      "          0.0079, -0.3080,  0.3960,  0.3972]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3473], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3840, -0.2091, -0.1751, -0.0999,  0.2687,  0.0680,  0.0301, -0.2328,\n",
      "         -0.3675,  0.2894,  0.2688,  0.1685,  0.0220,  0.6899,  0.4953,  0.2779,\n",
      "         -0.0550,  0.0524,  0.1596,  0.2377]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1036], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.2796, -0.2286,  0.0272,  0.2786,  0.0426,  0.2305,  0.3905, -0.0574,\n",
      "          0.0812,  0.3692,  0.2771,  0.2246, -0.0174,  0.2060,  0.0574,  0.2726,\n",
      "         -0.6050,  0.1429,  0.2081,  0.0974]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.0307], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.5687,  0.1906,  0.2412,  0.3467,  0.2895,  0.0433,  0.4148, -0.2348,\n",
      "          0.0595,  0.2850,  0.4728,  0.3581, -0.3535,  0.2710,  0.3374,  0.2268,\n",
      "         -0.6391, -0.0430,  0.3477,  0.1193]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3435], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0232,  0.5654, -0.2985, -0.0928,  0.3285, -0.1405,  0.0823,  0.1855,\n",
      "         -0.2496,  0.1846,  0.3145,  0.2994, -0.6238,  0.5365,  0.3694,  0.2459,\n",
      "          0.0785,  0.2188,  0.1352,  0.1319]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1261], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3483,  0.3603,  0.6173,  0.0469, -0.0094,  0.1540,  0.1063, -0.0222,\n",
      "          0.2814,  0.4603,  0.3470,  0.3091,  0.2358, -0.0283,  0.2153,  0.3318,\n",
      "         -0.1476,  0.2785, -0.0107,  0.3870]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2258], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.1609, -0.0840, -0.0700,  0.3362,  0.3042, -0.2348,  0.1959,  0.2265,\n",
      "          0.3447,  0.3370,  0.4251,  0.2923, -0.2912, -0.0508,  0.0555,  0.4901,\n",
      "         -0.4516,  0.2235,  0.2437,  0.2795]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.1145], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3472, -0.0069,  0.5275,  0.2323,  0.2527, -0.2379,  0.0816,  0.4370,\n",
      "          0.5185,  0.2567,  0.1675,  0.3182,  0.1394,  0.2578,  0.1697,  0.1617,\n",
      "          0.2836,  0.0095,  0.1750,  0.2766]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3012], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.0118,  0.2135, -0.1508,  0.7717,  0.2420, -0.2426,  0.0872,  0.2714,\n",
      "          0.2998,  0.3451,  0.2769,  0.1454,  0.1798, -0.0276,  0.2961,  0.1936,\n",
      "          0.2980,  0.3784,  0.0290,  0.4039]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2460], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[-0.2060,  0.3158,  0.0159,  0.4333,  0.1848,  0.1800,  0.2879,  0.2736,\n",
      "          0.0830,  0.1277,  0.4007, -0.0143, -0.2922,  0.2475,  0.1078,  0.3761,\n",
      "          0.3702,  0.3415,  0.4031,  0.5583]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2952], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.5354,  0.0124,  0.1476, -0.0190,  0.1099, -0.0330,  0.3667,  0.3164,\n",
      "         -0.2631,  0.3249,  0.2014,  0.3739,  0.2560,  0.2291,  0.2542,  0.3345,\n",
      "         -0.5479,  0.5028,  0.3755,  0.3052]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.2550], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.2546,  0.1993,  0.0323, -0.0899,  0.3259,  0.0260,  0.2240, -0.0249,\n",
      "         -0.0226,  0.3074,  0.4503,  0.4010, -0.0606,  0.1531,  0.4528,  0.2165,\n",
      "          0.0336,  0.1694,  0.1087,  0.2753]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([0.3640], grad_fn=<AddBackward0>)\n",
      "1 tensor(nan, grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.4268,  0.2619, -0.0012,  0.0297,  0.3530,  0.4347,  0.0383,  0.2696,\n",
      "          0.1172,  0.4931,  0.2302,  0.2983, -0.1168,  0.4406,  0.2801,  0.0792,\n",
      "         -0.2565,  0.0721,  0.1635,  0.2732]], grad_fn=<AddBackward0>)\n",
      "log prob tensor([-0.0840], grad_fn=<AddBackward0>)\n",
      "log prob tensor([[ 0.3685,  0.5363, -0.3346,  0.2106,  0.2001, -0.1758,  0.3124, -0.1478,\n",
      "          0.2295,  0.2757,  0.3605,  0.3134, -0.4783,  0.3332,  0.3225,  0.1771,\n",
      "         -0.1805, -0.2878,  0.4448,  0.2546]])\n",
      "log prob tensor([0.4345])\n",
      "1 tensor(nan)\n",
      "log prob tensor([[ 0.6973,  0.0434,  0.0344,  0.0767,  0.3182,  0.0132,  0.1501,  0.3511,\n",
      "         -0.0429,  0.5265,  0.0690,  0.2043, -0.2938,  0.2315,  0.1653,  0.2969,\n",
      "         -0.4008,  0.1576,  0.0722,  0.2277]])\n",
      "log prob tensor([0.2662])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [362]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mBCELoss()\n\u001B[1;32m      3\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(bnn_l\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m bnn_l \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_bnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbnn_l\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_ts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_ts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val_ts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val_ts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [361]\u001B[0m, in \u001B[0;36mtrain_bnn\u001B[0;34m(nn_k, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs, sample)\u001B[0m\n\u001B[1;32m     36\u001B[0m correct \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     37\u001B[0m total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m y_train_ts\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 38\u001B[0m correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mround\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_train_ts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m*\u001B[39m correct \u001B[38;5;241m/\u001B[39m total\n\u001B[1;32m     40\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36msum\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2298\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2295\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2296\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2299\u001B[0m \u001B[43m                      \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Opt/anaconda3/envs/m1_torch/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "bnn_l = BayesianNetwork([len(X_train_np[0]), 1])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(bnn_l.parameters(), lr=1e-4)\n",
    "\n",
    "bnn_l = train_bnn(bnn_l, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. \n",
      "Test - Loss: 52.241458892822266. Accuracy: 47.00227675183405\n",
      "Train -  Loss: nan. Accuracy: 544281.983805668\n",
      "\n",
      "Iteration: 20. \n",
      "Test - Loss: 42.89405059814453. Accuracy: 53.52896534277764\n",
      "Train -  Loss: nan. Accuracy: 544306.5789473684\n",
      "\n",
      "Iteration: 40. \n",
      "Test - Loss: 32.00541687011719. Accuracy: 53.9590184669871\n",
      "Train -  Loss: nan. Accuracy: 624831.0728744939\n",
      "\n",
      "Iteration: 60. \n",
      "Test - Loss: 52.79511260986328. Accuracy: 46.976979509233495\n",
      "Train -  Loss: nan. Accuracy: 545733.0971659919\n",
      "\n",
      "Iteration: 80. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 548036.8421052631\n",
      "\n",
      "Iteration: 100. \n",
      "Test - Loss: 37.57537078857422. Accuracy: 50.89805211231976\n",
      "Train -  Loss: nan. Accuracy: 544216.3967611336\n",
      "\n",
      "Iteration: 120. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544634.5141700405\n",
      "\n",
      "Iteration: 140. \n",
      "Test - Loss: 12.926593780517578. Accuracy: 56.817606880849986\n",
      "Train -  Loss: nan. Accuracy: 554669.3319838056\n",
      "\n",
      "Iteration: 160. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 180. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544240.991902834\n",
      "\n",
      "Iteration: 200. \n",
      "Test - Loss: 47.30335235595703. Accuracy: 48.292436124462434\n",
      "Train -  Loss: nan. Accuracy: 554193.8259109312\n",
      "\n",
      "Iteration: 220. \n",
      "Test - Loss: 35.22761917114258. Accuracy: 52.719453579559826\n",
      "Train -  Loss: nan. Accuracy: 547561.3360323886\n",
      "\n",
      "Iteration: 240. \n",
      "Test - Loss: 52.732505798339844. Accuracy: 46.976979509233495\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 260. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 637030.2631578947\n",
      "\n",
      "Iteration: 280. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 300. \n",
      "Test - Loss: 47.32716369628906. Accuracy: 51.88464457374146\n",
      "Train -  Loss: nan. Accuracy: 641096.6599190283\n",
      "\n",
      "Iteration: 320. \n",
      "Test - Loss: 32.91128158569336. Accuracy: 47.027573994434604\n",
      "Train -  Loss: nan. Accuracy: 635644.7368421053\n",
      "\n",
      "Iteration: 340. \n",
      "Test - Loss: 36.75458526611328. Accuracy: 47.50822160384518\n",
      "Train -  Loss: nan. Accuracy: 630758.5020242915\n",
      "\n",
      "Iteration: 360. \n",
      "Test - Loss: 46.3403205871582. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 627077.4291497975\n",
      "\n",
      "Iteration: 380. \n",
      "Test - Loss: 52.478519439697266. Accuracy: 46.47103465722236\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 400. \n",
      "Test - Loss: 51.70404052734375. Accuracy: 47.88768024285353\n",
      "Train -  Loss: nan. Accuracy: 546421.7611336033\n",
      "\n",
      "Iteration: 420. \n",
      "Test - Loss: 30.574432373046875. Accuracy: 57.88009107007336\n",
      "Train -  Loss: nan. Accuracy: 641334.4129554656\n",
      "\n",
      "Iteration: 440. \n",
      "Test - Loss: 35.7252082824707. Accuracy: 51.732861118138125\n",
      "Train -  Loss: nan. Accuracy: 641022.8744939271\n",
      "\n",
      "Iteration: 460. \n",
      "Test - Loss: 53.65889358520508. Accuracy: 46.319251201619025\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 480. \n",
      "Test - Loss: 46.344547271728516. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 618993.8259109312\n",
      "\n",
      "Iteration: 500. \n",
      "Test - Loss: 34.63938522338867. Accuracy: 51.25221350872755\n",
      "Train -  Loss: nan. Accuracy: 557243.6234817813\n",
      "\n",
      "Iteration: 520. \n",
      "Test - Loss: 52.25616455078125. Accuracy: 47.22995193523906\n",
      "Train -  Loss: nan. Accuracy: 545544.5344129555\n",
      "\n",
      "Iteration: 540. \n",
      "Test - Loss: 50.503421783447266. Accuracy: 48.216544396660765\n",
      "Train -  Loss: nan. Accuracy: 552586.943319838\n",
      "\n",
      "Iteration: 560. \n",
      "Test - Loss: 52.2921028137207. Accuracy: 47.027573994434604\n",
      "Train -  Loss: nan. Accuracy: 600908.1983805668\n",
      "\n",
      "Iteration: 580. \n",
      "Test - Loss: 45.00444793701172. Accuracy: 52.28940045535037\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 600. \n",
      "Test - Loss: 53.037200927734375. Accuracy: 46.29395395901847\n",
      "Train -  Loss: nan. Accuracy: 555874.4939271255\n",
      "\n",
      "Iteration: 620. \n",
      "Test - Loss: 46.597007751464844. Accuracy: 53.14950670376929\n",
      "Train -  Loss: nan. Accuracy: 611771.052631579\n",
      "\n",
      "Iteration: 640. \n",
      "Test - Loss: 52.52177810668945. Accuracy: 47.12876296483683\n",
      "Train -  Loss: nan. Accuracy: 545872.4696356276\n",
      "\n",
      "Iteration: 660. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544298.3805668016\n",
      "\n",
      "Iteration: 680. \n",
      "Test - Loss: 46.282108306884766. Accuracy: 53.630154313179865\n",
      "Train -  Loss: nan. Accuracy: 545257.5910931174\n",
      "\n",
      "Iteration: 700. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 546323.3805668016\n",
      "\n",
      "Iteration: 720. \n",
      "Test - Loss: 34.54075622558594. Accuracy: 45.61092840880344\n",
      "Train -  Loss: nan. Accuracy: 638301.012145749\n",
      "\n",
      "Iteration: 740. \n",
      "Test - Loss: 52.543983459472656. Accuracy: 45.99038704781179\n",
      "Train -  Loss: nan. Accuracy: 589225.5060728745\n",
      "\n",
      "Iteration: 760. \n",
      "Test - Loss: 37.817840576171875. Accuracy: 52.33999494055148\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 780. \n",
      "Test - Loss: 38.66868591308594. Accuracy: 52.744750822160384\n",
      "Train -  Loss: nan. Accuracy: 544339.3724696357\n",
      "\n",
      "Iteration: 800. \n",
      "Test - Loss: 50.9611701965332. Accuracy: 43.66304072856059\n",
      "Train -  Loss: nan. Accuracy: 544232.7935222673\n",
      "\n",
      "Iteration: 820. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 631529.1497975709\n",
      "\n",
      "Iteration: 840. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544273.7854251012\n",
      "\n",
      "Iteration: 860. \n",
      "Test - Loss: 46.90253829956055. Accuracy: 52.77004806476094\n",
      "Train -  Loss: nan. Accuracy: 562236.4372469636\n",
      "\n",
      "Iteration: 880. \n",
      "Test - Loss: 48.1380500793457. Accuracy: 47.50822160384518\n",
      "Train -  Loss: nan. Accuracy: 544216.3967611336\n",
      "\n",
      "Iteration: 900. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 546897.2672064777\n",
      "\n",
      "Iteration: 920. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 940. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544273.7854251012\n",
      "\n",
      "Iteration: 960. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544322.975708502\n",
      "\n",
      "Iteration: 980. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 631061.8421052631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bnn_l_s = BayesianNetwork([len(X_train_np[0]), 1])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(bnn_l_s.parameters(), momentum=0.9, lr=1e-4)\n",
    "\n",
    "bnn_l_s = train_bnn(bnn_l_s, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. \n",
      "Test - Loss: 41.87934112548828. Accuracy: 51.68226663293701\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 20. \n",
      "Test - Loss: 44.831111907958984. Accuracy: 53.45307361497597\n",
      "Train -  Loss: nan. Accuracy: 544765.6882591093\n",
      "\n",
      "Iteration: 40. \n",
      "Test - Loss: 46.433048248291016. Accuracy: 53.402479129774854\n",
      "Train -  Loss: nan. Accuracy: 640629.3522267207\n",
      "\n",
      "Iteration: 60. \n",
      "Test - Loss: 20.195693969726562. Accuracy: 55.19858335441437\n",
      "Train -  Loss: nan. Accuracy: 544273.7854251012\n",
      "\n",
      "Iteration: 80. \n",
      "Test - Loss: 21.777568817138672. Accuracy: 52.87123703516317\n",
      "Train -  Loss: nan. Accuracy: 639735.7287449393\n",
      "\n",
      "Iteration: 100. \n",
      "Test - Loss: 7.027623176574707. Accuracy: 53.7819377687832\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 120. \n",
      "Test - Loss: 47.48462677001953. Accuracy: 48.34303060966354\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 140. \n",
      "Test - Loss: 45.96027374267578. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 639916.0931174089\n",
      "\n",
      "Iteration: 160. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544240.991902834\n",
      "\n",
      "Iteration: 180. \n",
      "Test - Loss: 17.04196548461914. Accuracy: 57.29825449026056\n",
      "Train -  Loss: nan. Accuracy: 640965.4858299595\n",
      "\n",
      "Iteration: 200. \n",
      "Test - Loss: 52.81313705444336. Accuracy: 46.87579053883127\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 220. \n",
      "Test - Loss: 53.39980697631836. Accuracy: 46.4457374146218\n",
      "Train -  Loss: nan. Accuracy: 638596.1538461539\n",
      "\n",
      "Iteration: 240. \n",
      "Test - Loss: 30.05698585510254. Accuracy: 52.26410321274981\n",
      "Train -  Loss: nan. Accuracy: 599555.4655870445\n",
      "\n",
      "Iteration: 260. \n",
      "Test - Loss: 40.80651092529297. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 555874.4939271255\n",
      "\n",
      "Iteration: 280. \n",
      "Test - Loss: 53.60762405395508. Accuracy: 46.369845686820135\n",
      "Train -  Loss: nan. Accuracy: 571344.8380566802\n",
      "\n",
      "Iteration: 300. \n",
      "Test - Loss: 52.97014236450195. Accuracy: 45.712117379205665\n",
      "Train -  Loss: nan. Accuracy: 640530.971659919\n",
      "\n",
      "Iteration: 320. \n",
      "Test - Loss: 22.042510986328125. Accuracy: 53.5542625853782\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 340. \n",
      "Test - Loss: 45.70928192138672. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 360. \n",
      "Test - Loss: 24.381938934326172. Accuracy: 47.2046546926385\n",
      "Train -  Loss: nan. Accuracy: 625372.1659919028\n",
      "\n",
      "Iteration: 380. \n",
      "Test - Loss: 46.2968635559082. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 400. \n",
      "Test - Loss: 37.92070007324219. Accuracy: 54.23728813559322\n",
      "Train -  Loss: nan. Accuracy: 544937.8542510121\n",
      "\n",
      "Iteration: 420. \n",
      "Test - Loss: 31.669832229614258. Accuracy: 54.31317986339489\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 440. \n",
      "Test - Loss: 32.414981842041016. Accuracy: 53.90842398178599\n",
      "Train -  Loss: nan. Accuracy: 562752.9352226721\n",
      "\n",
      "Iteration: 460. \n",
      "Test - Loss: 53.174163818359375. Accuracy: 46.52162914242348\n",
      "Train -  Loss: nan. Accuracy: 549397.7732793522\n",
      "\n",
      "Iteration: 480. \n",
      "Test - Loss: 23.501972198486328. Accuracy: 47.027573994434604\n",
      "Train -  Loss: nan. Accuracy: 595472.6720647773\n",
      "\n",
      "Iteration: 500. \n",
      "Test - Loss: 46.3294563293457. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 641318.016194332\n",
      "\n",
      "Iteration: 520. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 636390.7894736842\n",
      "\n",
      "Iteration: 540. \n",
      "Test - Loss: 35.477745056152344. Accuracy: 46.49633189982292\n",
      "Train -  Loss: nan. Accuracy: 544486.943319838\n",
      "\n",
      "Iteration: 560. \n",
      "Test - Loss: 38.48189926147461. Accuracy: 53.680748798380975\n",
      "Train -  Loss: nan. Accuracy: 544273.7854251012\n",
      "\n",
      "Iteration: 580. \n",
      "Test - Loss: 46.01531219482422. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 600. \n",
      "Test - Loss: 27.305763244628906. Accuracy: 50.467998988110295\n",
      "Train -  Loss: nan. Accuracy: 624158.8056680162\n",
      "\n",
      "Iteration: 620. \n",
      "Test - Loss: 28.21674156188965. Accuracy: 53.42777637237541\n",
      "Train -  Loss: nan. Accuracy: 544888.6639676114\n",
      "\n",
      "Iteration: 640. \n",
      "Test - Loss: 53.51007843017578. Accuracy: 46.4457374146218\n",
      "Train -  Loss: nan. Accuracy: 544290.1821862349\n",
      "\n",
      "Iteration: 660. \n",
      "Test - Loss: 20.782943725585938. Accuracy: 51.37869972173033\n",
      "Train -  Loss: nan. Accuracy: 551135.8299595142\n",
      "\n",
      "Iteration: 680. \n",
      "Test - Loss: 42.41776657104492. Accuracy: 52.49177839615482\n",
      "Train -  Loss: nan. Accuracy: 641318.016194332\n",
      "\n",
      "Iteration: 700. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 639784.91902834\n",
      "\n",
      "Iteration: 720. \n",
      "Test - Loss: 52.83646774291992. Accuracy: 46.74930432582848\n",
      "Train -  Loss: nan. Accuracy: 638251.8218623481\n",
      "\n",
      "Iteration: 740. \n",
      "Test - Loss: 53.26164627075195. Accuracy: 46.369845686820135\n",
      "Train -  Loss: nan. Accuracy: 545478.947368421\n",
      "\n",
      "Iteration: 760. \n",
      "Test - Loss: 53.632179260253906. Accuracy: 46.319251201619025\n",
      "Train -  Loss: nan. Accuracy: 641367.2064777327\n",
      "\n",
      "Iteration: 780. \n",
      "Test - Loss: 33.967529296875. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 641375.4048582996\n",
      "\n",
      "Iteration: 800. \n",
      "Test - Loss: 21.230772018432617. Accuracy: 52.56767012395649\n",
      "Train -  Loss: nan. Accuracy: 544216.3967611336\n",
      "\n",
      "Iteration: 820. \n",
      "Test - Loss: 44.78218460083008. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 840. \n",
      "Test - Loss: 53.43635559082031. Accuracy: 46.39514292942069\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 860. \n",
      "Test - Loss: 48.36271667480469. Accuracy: 45.93979256261068\n",
      "Train -  Loss: nan. Accuracy: 637636.943319838\n",
      "\n",
      "Iteration: 880. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544978.8461538461\n",
      "\n",
      "Iteration: 900. \n",
      "Test - Loss: 46.08940887451172. Accuracy: 53.5542625853782\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 920. \n",
      "Test - Loss: 35.618438720703125. Accuracy: 52.137616999747024\n",
      "Train -  Loss: nan. Accuracy: 549914.2712550607\n",
      "\n",
      "Iteration: 940. \n",
      "Test - Loss: 52.734920501708984. Accuracy: 46.92638502403238\n",
      "Train -  Loss: nan. Accuracy: 545060.8299595142\n",
      "\n",
      "Iteration: 960. \n",
      "Test - Loss: 5.209048271179199. Accuracy: 56.842904123450545\n",
      "Train -  Loss: nan. Accuracy: 544216.3967611336\n",
      "\n",
      "Iteration: 980. \n",
      "Test - Loss: 26.547687530517578. Accuracy: 48.570705793068555\n",
      "Train -  Loss: nan. Accuracy: 640834.3117408907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bnn_l_adam = BayesianNetwork([len(X_train_np[0]), 1])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(bnn_l_adam.parameters(), lr=0.001)\n",
    "\n",
    "bnn_l_adam = train_bnn(bnn_l_adam, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. \n",
      "Test - Loss: 46.344547271728516. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 637128.6437246964\n",
      "\n",
      "Iteration: 20. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544273.7854251012\n",
      "\n",
      "Iteration: 40. \n",
      "Test - Loss: 30.399696350097656. Accuracy: 53.680748798380975\n",
      "Train -  Loss: nan. Accuracy: 640629.3522267207\n",
      "\n",
      "Iteration: 60. \n",
      "Test - Loss: 46.52528762817383. Accuracy: 53.2001011889704\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 80. \n",
      "Test - Loss: 47.358028411865234. Accuracy: 47.88768024285353\n",
      "Train -  Loss: nan. Accuracy: 630717.5101214575\n",
      "\n",
      "Iteration: 100. \n",
      "Test - Loss: 50.50034713745117. Accuracy: 48.79838097647357\n",
      "Train -  Loss: nan. Accuracy: 551709.7165991903\n",
      "\n",
      "Iteration: 120. \n",
      "Test - Loss: 45.972965240478516. Accuracy: 43.5365545155578\n",
      "Train -  Loss: nan. Accuracy: 544700.1012145749\n",
      "\n",
      "Iteration: 140. \n",
      "Test - Loss: 53.589439392089844. Accuracy: 46.24335947381736\n",
      "Train -  Loss: nan. Accuracy: 546692.3076923077\n",
      "\n",
      "Iteration: 160. \n",
      "Test - Loss: 53.37498092651367. Accuracy: 45.737414621806224\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 180. \n",
      "Test - Loss: 41.419822692871094. Accuracy: 53.680748798380975\n",
      "Train -  Loss: nan. Accuracy: 626700.3036437246\n",
      "\n",
      "Iteration: 200. \n",
      "Test - Loss: 36.20817184448242. Accuracy: 47.71059954464963\n",
      "Train -  Loss: nan. Accuracy: 544208.1983805668\n",
      "\n",
      "Iteration: 220. \n",
      "Test - Loss: 39.22221755981445. Accuracy: 54.996205413609914\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 240. \n",
      "Test - Loss: 25.371374130249023. Accuracy: 54.00961295218821\n",
      "Train -  Loss: nan. Accuracy: 549176.4170040486\n",
      "\n",
      "Iteration: 260. \n",
      "Test - Loss: 45.899532318115234. Accuracy: 53.70604604098153\n",
      "Train -  Loss: nan. Accuracy: 641318.016194332\n",
      "\n",
      "Iteration: 280. \n",
      "Test - Loss: 47.54499435424805. Accuracy: 51.8593473311409\n",
      "Train -  Loss: nan. Accuracy: 611361.1336032388\n",
      "\n",
      "Iteration: 300. \n",
      "Test - Loss: 45.52535629272461. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 627766.0931174089\n",
      "\n",
      "Iteration: 320. \n",
      "Test - Loss: 46.344547271728516. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 340. \n",
      "Test - Loss: 42.87730026245117. Accuracy: 50.67037692891475\n",
      "Train -  Loss: nan. Accuracy: 640875.3036437246\n",
      "\n",
      "Iteration: 360. \n",
      "Test - Loss: 46.344547271728516. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 567745.7489878542\n",
      "\n",
      "Iteration: 380. \n",
      "Test - Loss: 39.83162307739258. Accuracy: 50.44270174550974\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 400. \n",
      "Test - Loss: 50.259639739990234. Accuracy: 49.177839615481915\n",
      "Train -  Loss: nan. Accuracy: 638751.9230769231\n",
      "\n",
      "Iteration: 420. \n",
      "Test - Loss: 46.31367492675781. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 440. \n",
      "Test - Loss: 41.58108139038086. Accuracy: 48.77308373387301\n",
      "Train -  Loss: nan. Accuracy: 636751.5182186235\n",
      "\n",
      "Iteration: 460. \n",
      "Test - Loss: 45.89609146118164. Accuracy: 53.5542625853782\n",
      "Train -  Loss: nan. Accuracy: 544503.3400809717\n",
      "\n",
      "Iteration: 480. \n",
      "Test - Loss: 51.29533767700195. Accuracy: 47.76119402985075\n",
      "Train -  Loss: nan. Accuracy: 547881.0728744939\n",
      "\n",
      "Iteration: 500. \n",
      "Test - Loss: 46.63372802734375. Accuracy: 53.326587401973185\n",
      "Train -  Loss: nan. Accuracy: 548799.2914979757\n",
      "\n",
      "Iteration: 520. \n",
      "Test - Loss: 53.0462760925293. Accuracy: 46.92638502403238\n",
      "Train -  Loss: nan. Accuracy: 640088.2591093117\n",
      "\n",
      "Iteration: 540. \n",
      "Test - Loss: 44.75197219848633. Accuracy: 53.7819377687832\n",
      "Train -  Loss: nan. Accuracy: 638399.3927125506\n",
      "\n",
      "Iteration: 560. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544200.0\n",
      "\n",
      "Iteration: 580. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 600. \n",
      "Test - Loss: 53.25992965698242. Accuracy: 46.622818112825705\n",
      "Train -  Loss: nan. Accuracy: 641080.2631578947\n",
      "\n",
      "Iteration: 620. \n",
      "Test - Loss: 46.331687927246094. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 640. \n",
      "Test - Loss: 52.86684036254883. Accuracy: 46.87579053883127\n",
      "Train -  Loss: nan. Accuracy: 626782.2874493927\n",
      "\n",
      "Iteration: 660. \n",
      "Test - Loss: 46.344547271728516. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 544814.8785425101\n",
      "\n",
      "Iteration: 680. \n",
      "Test - Loss: 46.344547271728516. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 641334.4129554656\n",
      "\n",
      "Iteration: 700. \n",
      "Test - Loss: 53.62273025512695. Accuracy: 46.2180622312168\n",
      "Train -  Loss: nan. Accuracy: 544240.991902834\n",
      "\n",
      "Iteration: 720. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 640301.4170040486\n",
      "\n",
      "Iteration: 740. \n",
      "Test - Loss: 42.755130767822266. Accuracy: 46.16746774601568\n",
      "Train -  Loss: nan. Accuracy: 545921.6599190283\n",
      "\n",
      "Iteration: 760. \n",
      "Test - Loss: 48.03489303588867. Accuracy: 47.280546420440174\n",
      "Train -  Loss: nan. Accuracy: 548217.2064777327\n",
      "\n",
      "Iteration: 780. \n",
      "Test - Loss: 53.258766174316406. Accuracy: 46.52162914242348\n",
      "Train -  Loss: nan. Accuracy: 634242.8137651822\n",
      "\n",
      "Iteration: 800. \n",
      "Test - Loss: 46.344547271728516. Accuracy: 53.65545155578042\n",
      "Train -  Loss: nan. Accuracy: 547758.0971659919\n",
      "\n",
      "Iteration: 820. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n",
      "Iteration: 840. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 544552.5303643724\n",
      "\n",
      "Iteration: 860. \n",
      "Test - Loss: 44.63161849975586. Accuracy: 53.680748798380975\n",
      "Train -  Loss: nan. Accuracy: 638399.3927125506\n",
      "\n",
      "Iteration: 880. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 639514.3724696357\n",
      "\n",
      "Iteration: 900. \n",
      "Test - Loss: 53.655452728271484. Accuracy: 46.34454844421958\n",
      "Train -  Loss: nan. Accuracy: 594357.6923076923\n",
      "\n",
      "Iteration: 920. \n",
      "Test - Loss: 33.72212219238281. Accuracy: 58.63900834809006\n",
      "Train -  Loss: nan. Accuracy: 549578.1376518218\n",
      "\n",
      "Iteration: 940. \n",
      "Test - Loss: 44.213890075683594. Accuracy: 51.783455603339235\n",
      "Train -  Loss: nan. Accuracy: 544273.7854251012\n",
      "\n",
      "Iteration: 960. \n",
      "Test - Loss: 53.6261100769043. Accuracy: 46.369845686820135\n",
      "Train -  Loss: nan. Accuracy: 545208.4008097165\n",
      "\n",
      "Iteration: 980. \n",
      "Test - Loss: 46.24074935913086. Accuracy: 53.07361497596762\n",
      "Train -  Loss: nan. Accuracy: 641400.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_l_multi = BayesianNetwork([len(X_train_np[0]), 30, 1])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_l_multi.parameters(), lr=0.001)\n",
    "\n",
    "nn_l_multi = train_bnn(nn_l_multi, criterion, optimizer, X_train_ts, y_train_ts, X_val_ts, y_val_ts, epochs=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MACS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5febda9ea3b33ffdc95c888fff59ec7615aa10294cac48f80c25517e6a63b54a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}